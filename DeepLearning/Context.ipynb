{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add context to models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, DistilBertTokenizer, BertModel, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, RobertaTokenizer, RobertaModel, XLMRobertaModel, AutoTokenizer, AutoModelForSequenceClassification, XLMRobertaForSequenceClassification, RobertaForSequenceClassification\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import string\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import time\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_embedding_to_list(list_embedding):\n",
    "    num_list = ast.literal_eval(list_embedding)\n",
    "    return num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name, embedding_name):\n",
    "    # OpenAI Embedding\n",
    "    if embedding_name == \"text-embedding-3-large\":\n",
    "        df = pd.read_csv('C:/Users/jorge/Desktop/UNI/4-CUARTO/4-2-TFG/CODE/Gender-Bias/OpenAI/seacabo_embeddings.csv')\n",
    "        df['embeddings'] = df['embeddings'].apply(lambda x : convert_embedding_to_list(x))\n",
    "    else:\n",
    "        df = pd.read_csv(dataset_name)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_lang(df):\n",
    "    langs = ['es', 'cy', 'ht', 'in', 'lt', 'qam', 'tl', 'und']\n",
    "    df = df[df['lang'].isin(langs)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexicon(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lexicon = {line.strip().lower() for line in file if line.strip()}\n",
    "    return lexicon\n",
    "\n",
    "def add_special_tokens(df, NEW_TOKENS):\n",
    "    # lexicons\n",
    "    misogyny_list = load_lexicon(\"../Lexicons/lexicons_train_misogyny_lexicon.txt\")\n",
    "    insults_list = load_lexicon(\"../Lexicons/lexicons_train_insults_lexicon.txt\")\n",
    "    victim_list = load_lexicon(\"../Lexicons/lexicons_victim_seacabo.txt\")\n",
    "    aggressor_list = load_lexicon(\"../Lexicons/lexicons_aggressor_seacabo.txt\")\n",
    "\n",
    "    def replace_words_with_tokens(text):\n",
    "        words = text.split()\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            word_lower = word.lower()\n",
    "            if word_lower in insults_list or word_lower in misogyny_list:\n",
    "                processed_words.append(NEW_TOKENS[0]) # [INSULT]\n",
    "            elif word_lower in victim_list:\n",
    "                processed_words.append(NEW_TOKENS[1]) # [VICTIM]\n",
    "            elif word_lower in aggressor_list:\n",
    "                processed_words.append(NEW_TOKENS[2]) # [AGGRESSOR]\n",
    "            else:\n",
    "                processed_words.append(word)\n",
    "        return ' '.join(processed_words)\n",
    "    \n",
    "    # Aplicar la función de procesamiento a la columna especificada\n",
    "    df['full_text'] = df['full_text'].apply(replace_words_with_tokens)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(df, context, tweet_original):\n",
    "\n",
    "    # Preprocesado de datos\n",
    "    df['tweet_respuesta'] = df['full_text'].apply(lambda x: preprocess_tweet(x, lang=\"es\"))\n",
    "\n",
    "    # Añadir columnas para diferentes contextos:\n",
    "    df['Sin_contexto'] = df['tweet_respuesta'] # cambio full_text por tweet_respuesta\n",
    "    #df['Tweet_context'] = tweet_original + \" [SEP] \" + df['tweet_respuesta']\n",
    "    df['Full_context'] = tweet_original + \" [SEP] \" + df['tweet_respuesta'] + \" [SEP] \" + context\n",
    "    df['Tweet_context'] = tweet_original + \" [SEP] \" + df['tweet_respuesta']\n",
    "    #df['Full_context'] = df['tweet_respuesta'] + \" [SEP] \" + context\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_type(df, label_column):\n",
    "\n",
    "    if label_column == \"Análisis General\":\n",
    "        # Define the specific labels to keep\n",
    "        #etiquetas = [\"Comentario Positivo\", \"Comentario Negativo\", \"Comentario Neutro\"]\n",
    "        etiquetas = [\"Comentario Positivo\", \"Comentario Negativo\"]\n",
    "        \n",
    "        df['Análisis General'] = df['Análisis General'].where(df['Análisis General'].isin(etiquetas))\n",
    "\n",
    "\n",
    "        # Remove NAs\n",
    "        df = df.dropna(subset=['Análisis General'])\n",
    "        \n",
    "\n",
    "        # Factorize the 'Análisis General' column\n",
    "        labels, labels_names = pd.factorize(df['Análisis General'])\n",
    "\n",
    "        # 'labels' now contains the numeric representation of your original labels\n",
    "        # 'label_names' contains the unique values from your original column in the order they were encoded\n",
    "\n",
    "        # Replace the original column with the numeric labels\n",
    "        df['Análisis General'] = labels\n",
    "\n",
    "        # If you want to keep a record of the mapping from the original labels to the numeric labels\n",
    "        label_mapping = dict(zip(labels_names, range(len(labels_names))))\n",
    "        #print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "    if label_column == \"Contenido Negativo\":\n",
    "\n",
    "        # Filtrar el DataFrame para seleccionar solo los \"Comentario Negativo\"\n",
    "        df = df.loc[df['Análisis General'] == 'Comentario Negativo']\n",
    "\n",
    "        # Define the specific labels to keep\n",
    "        etiquetas = [\"Desprestigiar Víctima\", \"Desprestigiar Acto\", \"Insultos\", \"Desprestigiar Deportista Autora\"]\n",
    "        df['Contenido Negativo'] = df['Contenido Negativo'].where(df['Contenido Negativo'].isin(etiquetas))\n",
    "\n",
    "        # Remove NAs\n",
    "        df = df.dropna(subset=['Contenido Negativo'])\n",
    "        \n",
    "\n",
    "        # Factorize the 'Análisis General' column\n",
    "        labels, labels_names = pd.factorize(df['Contenido Negativo'])\n",
    "\n",
    "        # 'labels' now contains the numeric representation of your original labels\n",
    "        # 'label_names' contains the unique values from your original column in the order they were encoded\n",
    "\n",
    "        # Replace the original column with the numeric labels\n",
    "        df['Contenido Negativo'] = labels\n",
    "\n",
    "        # If you want to keep a record of the mapping from the original labels to the numeric labels\n",
    "        label_mapping = dict(zip(labels_names, range(len(labels_names))))\n",
    "        #print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "\n",
    "    if label_column == \"Insultos\":\n",
    "\n",
    "        # Filtrar el DataFrame para seleccionar solo los \"Comentario Negativo\"\n",
    "        df = df.loc[df['Análisis General'] == 'Comentario Negativo']\n",
    "\n",
    "        # Define the specific labels to keep\n",
    "        etiquetas = [\"Deseo de Dañar\", \"Genéricos\", \"Sexistas/misóginos\"]\n",
    "\n",
    "        # Replace labels that are not in the list with \"Genéricos\"\n",
    "        df['Insultos'] = df['Insultos'].where(df['Insultos'].isin(etiquetas), other=\"Genéricos\")\n",
    "\n",
    "        # Remove NAs\n",
    "        df = df.dropna(subset=['Insultos'])\n",
    "        \n",
    "\n",
    "        # Factorize the 'Insultos' column\n",
    "        labels, labels_names = pd.factorize(df['Insultos'])\n",
    "\n",
    "        # 'labels' now contains the numeric representation of your original labels\n",
    "        # 'label_names' contains the unique values from your original column in the order they were encoded\n",
    "\n",
    "        # Replace the original column with the numeric labels\n",
    "        df['Insultos'] = labels\n",
    "\n",
    "        # If you want to keep a record of the mapping from the original labels to the numeric labels\n",
    "        label_mapping = dict(zip(labels_names, range(len(labels_names))))\n",
    "        #print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "    num_labels = len(etiquetas)\n",
    "\n",
    "\n",
    "\n",
    "    return df, labels_names, num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(name):\n",
    "    \n",
    "    if name == \"dccuchile/bert-base-spanish-wwm-cased\":\n",
    "        tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n",
    "\n",
    "    if name == \"PlanTL-GOB-ES/roberta-large-bne\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('PlanTL-GOB-ES/roberta-large-bne') \n",
    "\n",
    "    if name == \"bert-base-multilingual-cased\":\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "    if name == \"FacebookAI/xlm-roberta-base\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "\n",
    "    if name == \"pysentimiento/robertuito-base-cased\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained('pysentimiento/robertuito-base-cased')\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar inputs tokenizados para cada contexto\n",
    "def prepare_inputs(df, text_column, label_column, tokenizer):\n",
    "    \n",
    "    # Tokenización de los textos\n",
    "    tokenized_data = tokenizer(df[text_column].tolist(), padding=PADDING, truncation=TRUNCATION, max_length=MAX_LENGTH, return_tensors='pt')\n",
    "    \n",
    "    # Factorizar las etiquetas si son categóricas\n",
    "    labels, _ = pd.factorize(df[label_column])\n",
    "    \n",
    "    # Convertir las etiquetas a un tensor\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    # Retorna un diccionario con los inputs tokenizados y las etiquetas\n",
    "    return {**tokenized_data, 'labels': labels}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hf_dataset(tokenized_inputs):\n",
    "    return Dataset.from_dict(tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para dividir un dataset en train, validation y test\n",
    "def split_dataset(dataset, test_size=0.1, val_size=0.3):\n",
    "    # Dividir primero en train+val y test\n",
    "    train_val_dataset, test_dataset = dataset.train_test_split(test_size=test_size).values()\n",
    "\n",
    "    # Ahora dividir train+val en train y val\n",
    "    train_dataset, val_dataset = train_val_dataset.train_test_split(test_size=val_size / (1 - test_size)).values()\n",
    "\n",
    "    return DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'validation': val_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_train_test_val_split(dataset, test_size, val_size, seed=0):\n",
    "    # Convertir a pandas DataFrame para usar la funcionalidad de scikit-learn\n",
    "    df = dataset.to_pandas()\n",
    "    \n",
    "    # Estratificar y dividir el conjunto de datos en entrenamiento+validación y test\n",
    "    train_val_df, test_df = train_test_split(df, test_size=test_size, stratify=df['labels'], random_state=seed)\n",
    "    \n",
    "    # Estratificar y dividir el conjunto de entrenamiento+validación en entrenamiento y validación\n",
    "    train_df, val_df = train_test_split(train_val_df, test_size=val_size/(1.0-test_size), stratify=train_val_df['labels'], random_state=seed)\n",
    "    \n",
    "    # Convertir de nuevo a datasets de HuggingFace\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "    \n",
    "    # Crear un DatasetDict\n",
    "    split_dataset = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'validation': val_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n",
    "    \n",
    "    # Calcular el número de ejemplos por clase en cada subconjunto\n",
    "    train_class_counts = Counter(train_df['labels'])\n",
    "    val_class_counts = Counter(val_df['labels'])\n",
    "    test_class_counts = Counter(test_df['labels'])\n",
    "\n",
    "    # Retornar el DatasetDict y las cuentas de clases\n",
    "    return split_dataset, {'train': train_class_counts, 'validation': val_class_counts, 'test': test_class_counts}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name, num_labels):\n",
    "    if name == \"dccuchile/bert-base-spanish-wwm-cased\":\n",
    "        model = BertForSequenceClassification.from_pretrained(name, num_labels=num_labels)\n",
    "\n",
    "    if name == \"PlanTL-GOB-ES/roberta-large-bne\":\n",
    "        model = RobertaForSequenceClassification.from_pretrained(name, num_labels=num_labels) \n",
    "\n",
    "    if name == \"bert-base-multilingual-cased\":\n",
    "        model = BertForSequenceClassification.from_pretrained(name, num_labels=num_labels)\n",
    "\n",
    "    if name == \"FacebookAI/xlm-roberta-base\":\n",
    "        model = XLMRobertaForSequenceClassification.from_pretrained(name, num_labels=num_labels)\n",
    "\n",
    "    if name == \"pysentimiento/robertuito-base-cased\":\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(name, num_labels=num_labels)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Obtener reporte completo\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    \n",
    "    # Obtener la matriz de confusión\n",
    "    conf_matrix = confusion_matrix(labels, predictions)\n",
    "    \n",
    "    # Extraer métricas para cada clase y globales\n",
    "    metrics = {\n",
    "        'accuracy': report['accuracy'],\n",
    "        'weighted_f1': report['weighted avg']['f1-score'],\n",
    "        'weighted_precision': report['weighted avg']['precision'],\n",
    "        'weighted_recall': report['weighted avg']['recall'],\n",
    "        # La matriz de confusión no se incluye normalmente como una métrica devuelta porque no es un escalar\n",
    "        'confusion_matrix': conf_matrix.tolist()  # Convertir a lista para asegurarse de que es serializable si es necesario\n",
    "    }\n",
    "    \n",
    "    # Añadir métricas específicas por clase si se requiere\n",
    "    for label, scores in report.items():\n",
    "        if label not in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "            metrics[f'{label}_precision'] = scores['precision']\n",
    "            metrics[f'{label}_recall'] = scores['recall']\n",
    "            metrics[f'{label}_f1'] = scores['f1-score']\n",
    "            metrics[f'{label}_support'] = scores['support']\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_args(context_type, learning_rate_scheduler_type=\"linear\", num_train_epochs=3, per_device_train_batch_size=16, per_device_eval_batch_size=16, warmup_steps=500, weight_decay=0.01, logging_steps=10):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results/{context_type}',\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        warmup_steps=warmup_steps,\n",
    "        weight_decay=weight_decay,\n",
    "        logging_dir=f'./logs/{context_type}',\n",
    "        logging_steps=logging_steps,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        #save_strategy=\"epoch\",\n",
    "        #load_best_model_at_end=True\n",
    "        save_strategy=\"no\",  # No guardar modelos\n",
    "        save_total_limit=0,  # No mantener checkpoints\n",
    "        metric_for_best_model=\"eval_f1\",\n",
    "        load_best_model_at_end=False,  # No cargar el mejor modelo al final del entrenamiento\n",
    "    )\n",
    "\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveResultsCallback(TrainerCallback):\n",
    "    def __init__(self, excel_path, training_args, run_id, additional_info, type_d, num_labels):\n",
    "        self.excel_path = excel_path\n",
    "        self.training_args = vars(training_args)  # Convert training arguments to dictionary\n",
    "        self.run_id = run_id\n",
    "        self.additional_info = additional_info  # Assumed to be a dictionary\n",
    "        self.type_d = type_d  # Type of dataset or context\n",
    "        self.rows = []\n",
    "        self.initialized = False\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def on_train_begin(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if state.is_local_process_zero:\n",
    "            if not self.initialized:\n",
    "                self.init_excel()\n",
    "                self.initialized = True\n",
    "\n",
    "    def init_excel(self):\n",
    "        # Initialize the Excel file with proper headers if it does not exist\n",
    "        if not Path(self.excel_path).exists():\n",
    "            with pd.ExcelWriter(self.excel_path, engine='openpyxl') as writer:\n",
    "                # Inicializar los encabezados dinámicamente basados en el número de clases\n",
    "                metric_headers = ['eval_loss', 'eval_accuracy', 'eval_weighted_f1', 'eval_weighted_precision', 'eval_weighted_recall']\n",
    "                metric_headers.append('eval_confusion_matrix')\n",
    "                # Agregar encabezados de métricas para cada clase\n",
    "                for i in range(self.num_labels):\n",
    "                    metric_headers.extend([\n",
    "                        f'eval_{i}_precision',\n",
    "                        f'eval_{i}_recall',\n",
    "                        f'eval_{i}_f1',\n",
    "                        f'eval_{i}_support'\n",
    "                    ])\n",
    "                \n",
    "                metric_headers.append('eval_runtime')\n",
    "                metric_headers.append('eval_samples_per_second')\n",
    "                metric_headers.append('eval_steps_per_second')\n",
    "\n",
    "                # Definir encabezados para la creación del archivo Excel\n",
    "                headers = [\"run_id\", \"type_d\", *self.additional_info.keys(), \"epoch\", *metric_headers, *self.training_args.keys()]\n",
    "                df_header = pd.DataFrame(columns=headers)\n",
    "                df_header.to_excel(writer, sheet_name=self.type_d, index=False)\n",
    "\n",
    "    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, metrics, **kwargs):\n",
    "        if state.is_local_process_zero:\n",
    "            # Prepare data dictionary for the current epoch\n",
    "            data = {\"run_id\": self.run_id, \"type_d\": self.type_d, **self.additional_info, \"epoch\": state.epoch, **metrics, **self.training_args}\n",
    "            self.rows.append(data)\n",
    "\n",
    "    def on_train_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if state.is_local_process_zero:\n",
    "            self.save_to_excel()\n",
    "\n",
    "    def save_to_excel(self):\n",
    "        # Save collected data to Excel, appending to the existing sheet or creating it if not exists\n",
    "        with pd.ExcelWriter(self.excel_path, mode='a', engine='openpyxl', if_sheet_exists='overlay') as writer:\n",
    "            df = pd.DataFrame(self.rows)\n",
    "            start_row = writer.sheets[self.type_d].max_row if self.type_d in writer.book.sheetnames else 0\n",
    "            df.to_excel(writer, sheet_name=self.type_d, index=False, header=not bool(start_row), startrow=start_row)\n",
    "            print(f\"Data logged to {self.excel_path} in sheet {self.type_d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_data(excel_path, run_id, eval_results, training_args, additional_info, type_d):\n",
    "    # Define el nombre de la hoja basado en el tipo de dataset\n",
    "    sheet_name = f\"{type_d}\"\n",
    "\n",
    "    # Crear el archivo Excel si no existe\n",
    "    if not Path(excel_path).exists():\n",
    "        # Si el archivo no existe, crearlo con una hoja dummy para inicializarlo\n",
    "        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "            pd.DataFrame().to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "\n",
    "    # Cargar o inicializar el DataFrame dependiendo de la existencia de la hoja\n",
    "    if sheet_name in pd.ExcelFile(excel_path).sheet_names:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "    else:\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    # Convertir el objeto de argumentos de entrenamiento a diccionario si es necesario\n",
    "    training_args_dict = vars(training_args) if not isinstance(training_args, dict) else training_args\n",
    "\n",
    "    # Asegurarte de que los resultados de evaluación son un diccionario\n",
    "    eval_results_dict = eval_results if isinstance(eval_results, dict) else vars(eval_results)\n",
    "\n",
    "    # Construir la fila de datos a agregar\n",
    "    data = {\n",
    "        **{'Run_ID': run_id},\n",
    "        **training_args_dict,\n",
    "        **eval_results_dict,\n",
    "        **additional_info\n",
    "    }\n",
    "\n",
    "    # Convertir todos los valores a strings si no son int, float o string\n",
    "    for key, value in data.items():\n",
    "        if not isinstance(value, (int, float, str)):\n",
    "            data[key] = str(value)\n",
    "\n",
    "    # Agregar la nueva fila al DataFrame\n",
    "    df = df._append(data, ignore_index=True)\n",
    "\n",
    "    # Guardar el DataFrame en la hoja correspondiente\n",
    "    with pd.ExcelWriter(excel_path, mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    print(f\"Data logged to {excel_path} in sheet {sheet_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorge\\AppData\\Local\\Temp\\ipykernel_3412\\549421086.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Análisis General'] = labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "----------\n",
      "type_d='Sin_contexto'\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7334, 'grad_norm': 7.788638591766357, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.14}\n",
      "{'loss': 0.6588, 'grad_norm': 4.532946586608887, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.27}\n",
      "{'loss': 0.5533, 'grad_norm': 5.7565789222717285, 'learning_rate': 3e-06, 'epoch': 0.41}\n",
      "{'loss': 0.5273, 'grad_norm': 3.5031936168670654, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.54}\n",
      "{'loss': 0.4242, 'grad_norm': 4.482428073883057, 'learning_rate': 5e-06, 'epoch': 0.68}\n",
      "{'loss': 0.4289, 'grad_norm': 6.188481330871582, 'learning_rate': 6e-06, 'epoch': 0.81}\n",
      "{'loss': 0.4785, 'grad_norm': 2.4934308528900146, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.95}\n",
      "{'eval_loss': 0.3870529532432556, 'eval_accuracy': 0.8095238095238095, 'eval_weighted_f1': 0.7473275024295433, 'eval_weighted_precision': 0.8469179082676016, 'eval_weighted_recall': 0.8095238095238095, 'eval_confusion_matrix': [[131, 0], [32, 5]], 'eval_0_precision': 0.803680981595092, 'eval_0_recall': 1.0, 'eval_0_f1': 0.891156462585034, 'eval_0_support': 131.0, 'eval_1_precision': 1.0, 'eval_1_recall': 0.13513513513513514, 'eval_1_f1': 0.2380952380952381, 'eval_1_support': 37.0, 'eval_runtime': 44.0073, 'eval_samples_per_second': 3.818, 'eval_steps_per_second': 0.477, 'epoch': 1.0}\n",
      "{'loss': 0.336, 'grad_norm': 4.8763532638549805, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.08}\n",
      "{'loss': 0.302, 'grad_norm': 4.349101543426514, 'learning_rate': 9e-06, 'epoch': 1.22}\n",
      "{'loss': 0.3989, 'grad_norm': 5.422347545623779, 'learning_rate': 1e-05, 'epoch': 1.35}\n",
      "{'loss': 0.3526, 'grad_norm': 3.3849496841430664, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.49}\n",
      "{'loss': 0.3683, 'grad_norm': 5.92335319519043, 'learning_rate': 1.2e-05, 'epoch': 1.62}\n",
      "{'loss': 0.2754, 'grad_norm': 5.291497230529785, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.76}\n",
      "{'loss': 0.357, 'grad_norm': 5.113500118255615, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.89}\n",
      "{'eval_loss': 0.20304229855537415, 'eval_accuracy': 0.9107142857142857, 'eval_weighted_f1': 0.9093333586424205, 'eval_weighted_precision': 0.9087623855512355, 'eval_weighted_recall': 0.9107142857142857, 'eval_confusion_matrix': [[125, 6], [9, 28]], 'eval_0_precision': 0.9328358208955224, 'eval_0_recall': 0.9541984732824428, 'eval_0_f1': 0.9433962264150944, 'eval_0_support': 131.0, 'eval_1_precision': 0.8235294117647058, 'eval_1_recall': 0.7567567567567568, 'eval_1_f1': 0.7887323943661971, 'eval_1_support': 37.0, 'eval_runtime': 43.6427, 'eval_samples_per_second': 3.849, 'eval_steps_per_second': 0.481, 'epoch': 2.0}\n",
      "{'loss': 0.2457, 'grad_norm': 6.499358177185059, 'learning_rate': 1.5e-05, 'epoch': 2.03}\n",
      "{'loss': 0.1775, 'grad_norm': 5.674662113189697, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.16}\n",
      "{'loss': 0.1804, 'grad_norm': 4.541062831878662, 'learning_rate': 1.7000000000000003e-05, 'epoch': 2.3}\n",
      "{'loss': 0.171, 'grad_norm': 15.281418800354004, 'learning_rate': 1.8e-05, 'epoch': 2.43}\n",
      "{'loss': 0.1547, 'grad_norm': 6.503503322601318, 'learning_rate': 1.9e-05, 'epoch': 2.57}\n",
      "{'loss': 0.1806, 'grad_norm': 4.758800506591797, 'learning_rate': 2e-05, 'epoch': 2.7}\n",
      "{'loss': 0.1751, 'grad_norm': 6.144309997558594, 'learning_rate': 2.1e-05, 'epoch': 2.84}\n",
      "{'loss': 0.1558, 'grad_norm': 4.823577404022217, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.97}\n",
      "{'eval_loss': 0.26409271359443665, 'eval_accuracy': 0.9166666666666666, 'eval_weighted_f1': 0.9139230318993269, 'eval_weighted_precision': 0.914665253904717, 'eval_weighted_recall': 0.9166666666666666, 'eval_confusion_matrix': [[127, 4], [10, 27]], 'eval_0_precision': 0.927007299270073, 'eval_0_recall': 0.9694656488549618, 'eval_0_f1': 0.9477611940298507, 'eval_0_support': 131.0, 'eval_1_precision': 0.8709677419354839, 'eval_1_recall': 0.7297297297297297, 'eval_1_f1': 0.7941176470588235, 'eval_1_support': 37.0, 'eval_runtime': 43.6068, 'eval_samples_per_second': 3.853, 'eval_steps_per_second': 0.482, 'epoch': 3.0}\n",
      "{'loss': 0.1341, 'grad_norm': 2.83772873878479, 'learning_rate': 2.3000000000000003e-05, 'epoch': 3.11}\n",
      "{'loss': 0.0622, 'grad_norm': 0.3385849595069885, 'learning_rate': 2.4e-05, 'epoch': 3.24}\n",
      "{'loss': 0.0667, 'grad_norm': 0.7213020920753479, 'learning_rate': 2.5e-05, 'epoch': 3.38}\n",
      "{'loss': 0.0286, 'grad_norm': 0.7022736072540283, 'learning_rate': 2.6000000000000002e-05, 'epoch': 3.51}\n",
      "{'loss': 0.1215, 'grad_norm': 6.830504894256592, 'learning_rate': 2.7000000000000002e-05, 'epoch': 3.65}\n",
      "{'loss': 0.1971, 'grad_norm': 20.461130142211914, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.78}\n",
      "{'loss': 0.1855, 'grad_norm': 9.307465553283691, 'learning_rate': 2.9e-05, 'epoch': 3.92}\n",
      "{'eval_loss': 0.4994315207004547, 'eval_accuracy': 0.8988095238095238, 'eval_weighted_f1': 0.8904187069760842, 'eval_weighted_precision': 0.9004216269841271, 'eval_weighted_recall': 0.8988095238095238, 'eval_confusion_matrix': [[129, 2], [15, 22]], 'eval_0_precision': 0.8958333333333334, 'eval_0_recall': 0.9847328244274809, 'eval_0_f1': 0.9381818181818182, 'eval_0_support': 131.0, 'eval_1_precision': 0.9166666666666666, 'eval_1_recall': 0.5945945945945946, 'eval_1_f1': 0.7213114754098361, 'eval_1_support': 37.0, 'eval_runtime': 44.0343, 'eval_samples_per_second': 3.815, 'eval_steps_per_second': 0.477, 'epoch': 4.0}\n",
      "{'loss': 0.0293, 'grad_norm': 3.1698215007781982, 'learning_rate': 3e-05, 'epoch': 4.05}\n",
      "{'loss': 0.0215, 'grad_norm': 0.372823029756546, 'learning_rate': 3.1e-05, 'epoch': 4.19}\n",
      "{'loss': 0.1264, 'grad_norm': 0.01480794046074152, 'learning_rate': 3.2000000000000005e-05, 'epoch': 4.32}\n",
      "{'loss': 0.114, 'grad_norm': 2.147447109222412, 'learning_rate': 3.3e-05, 'epoch': 4.46}\n",
      "{'loss': 0.0787, 'grad_norm': 0.15256237983703613, 'learning_rate': 3.4000000000000007e-05, 'epoch': 4.59}\n",
      "{'loss': 0.079, 'grad_norm': 0.11582460254430771, 'learning_rate': 3.5e-05, 'epoch': 4.73}\n",
      "{'loss': 0.0534, 'grad_norm': 0.15804722905158997, 'learning_rate': 3.6e-05, 'epoch': 4.86}\n",
      "{'loss': 0.0167, 'grad_norm': 0.08915221691131592, 'learning_rate': 3.7e-05, 'epoch': 5.0}\n",
      "{'eval_loss': 0.38136351108551025, 'eval_accuracy': 0.9285714285714286, 'eval_weighted_f1': 0.9270676691729324, 'eval_weighted_precision': 0.9270963604296938, 'eval_weighted_recall': 0.9285714285714286, 'eval_confusion_matrix': [[127, 4], [8, 29]], 'eval_0_precision': 0.9407407407407408, 'eval_0_recall': 0.9694656488549618, 'eval_0_f1': 0.9548872180451129, 'eval_0_support': 131.0, 'eval_1_precision': 0.8787878787878788, 'eval_1_recall': 0.7837837837837838, 'eval_1_f1': 0.8285714285714285, 'eval_1_support': 37.0, 'eval_runtime': 43.9465, 'eval_samples_per_second': 3.823, 'eval_steps_per_second': 0.478, 'epoch': 5.0}\n",
      "{'loss': 0.0296, 'grad_norm': 0.010220742784440517, 'learning_rate': 3.8e-05, 'epoch': 5.14}\n",
      "{'loss': 0.0165, 'grad_norm': 0.2369476556777954, 'learning_rate': 3.9000000000000006e-05, 'epoch': 5.27}\n",
      "{'loss': 0.0025, 'grad_norm': 0.019564716145396233, 'learning_rate': 4e-05, 'epoch': 5.41}\n",
      "{'loss': 0.0349, 'grad_norm': 0.344278484582901, 'learning_rate': 4.1e-05, 'epoch': 5.54}\n",
      "{'loss': 0.0356, 'grad_norm': 0.084678515791893, 'learning_rate': 4.2e-05, 'epoch': 5.68}\n",
      "{'loss': 0.0618, 'grad_norm': 0.014502124860882759, 'learning_rate': 4.3e-05, 'epoch': 5.81}\n",
      "{'loss': 0.0114, 'grad_norm': 10.1045560836792, 'learning_rate': 4.4000000000000006e-05, 'epoch': 5.95}\n",
      "{'eval_loss': 0.4387251138687134, 'eval_accuracy': 0.9226190476190477, 'eval_weighted_f1': 0.919583574427385, 'eval_weighted_precision': 0.9214717046238786, 'eval_weighted_recall': 0.9226190476190477, 'eval_confusion_matrix': [[128, 3], [10, 27]], 'eval_0_precision': 0.927536231884058, 'eval_0_recall': 0.9770992366412213, 'eval_0_f1': 0.9516728624535316, 'eval_0_support': 131.0, 'eval_1_precision': 0.9, 'eval_1_recall': 0.7297297297297297, 'eval_1_f1': 0.8059701492537312, 'eval_1_support': 37.0, 'eval_runtime': 43.6092, 'eval_samples_per_second': 3.852, 'eval_steps_per_second': 0.482, 'epoch': 6.0}\n",
      "{'loss': 0.0323, 'grad_norm': 0.16931496560573578, 'learning_rate': 4.5e-05, 'epoch': 6.08}\n",
      "{'loss': 0.0414, 'grad_norm': 0.13360647857189178, 'learning_rate': 4.600000000000001e-05, 'epoch': 6.22}\n",
      "{'loss': 0.0341, 'grad_norm': 0.008525743149220943, 'learning_rate': 4.7e-05, 'epoch': 6.35}\n",
      "{'loss': 0.0708, 'grad_norm': 0.5983818769454956, 'learning_rate': 4.8e-05, 'epoch': 6.49}\n",
      "{'loss': 0.0477, 'grad_norm': 119.0744857788086, 'learning_rate': 4.9e-05, 'epoch': 6.62}\n",
      "{'loss': 0.1118, 'grad_norm': 0.006725109647959471, 'learning_rate': 5e-05, 'epoch': 6.76}\n",
      "{'loss': 0.1105, 'grad_norm': 8.330304145812988, 'learning_rate': 4.791666666666667e-05, 'epoch': 6.89}\n",
      "{'eval_loss': 0.42789408564567566, 'eval_accuracy': 0.9345238095238095, 'eval_weighted_f1': 0.9319553322077875, 'eval_weighted_precision': 0.9344634230503795, 'eval_weighted_recall': 0.9345238095238095, 'eval_confusion_matrix': [[129, 2], [9, 28]], 'eval_0_precision': 0.9347826086956522, 'eval_0_recall': 0.9847328244274809, 'eval_0_f1': 0.9591078066914499, 'eval_0_support': 131.0, 'eval_1_precision': 0.9333333333333333, 'eval_1_recall': 0.7567567567567568, 'eval_1_f1': 0.835820895522388, 'eval_1_support': 37.0, 'eval_runtime': 45.579, 'eval_samples_per_second': 3.686, 'eval_steps_per_second': 0.461, 'epoch': 7.0}\n",
      "{'loss': 0.0373, 'grad_norm': 0.02081725373864174, 'learning_rate': 4.5833333333333334e-05, 'epoch': 7.03}\n",
      "{'loss': 0.0429, 'grad_norm': 0.032583992928266525, 'learning_rate': 4.375e-05, 'epoch': 7.16}\n",
      "{'loss': 0.0946, 'grad_norm': 0.053883034735918045, 'learning_rate': 4.166666666666667e-05, 'epoch': 7.3}\n",
      "{'loss': 0.0014, 'grad_norm': 0.00854103360325098, 'learning_rate': 3.958333333333333e-05, 'epoch': 7.43}\n",
      "{'loss': 0.05, 'grad_norm': 0.127200186252594, 'learning_rate': 3.7500000000000003e-05, 'epoch': 7.57}\n",
      "{'loss': 0.0652, 'grad_norm': 97.56464385986328, 'learning_rate': 3.541666666666667e-05, 'epoch': 7.7}\n",
      "{'loss': 0.0019, 'grad_norm': 0.014047615230083466, 'learning_rate': 3.3333333333333335e-05, 'epoch': 7.84}\n",
      "{'loss': 0.0005, 'grad_norm': 0.0331515371799469, 'learning_rate': 3.125e-05, 'epoch': 7.97}\n",
      "{'eval_loss': 0.4129410684108734, 'eval_accuracy': 0.9226190476190477, 'eval_weighted_f1': 0.9205399306767161, 'eval_weighted_precision': 0.9208683473389355, 'eval_weighted_recall': 0.9226190476190477, 'eval_confusion_matrix': [[127, 4], [9, 28]], 'eval_0_precision': 0.9338235294117647, 'eval_0_recall': 0.9694656488549618, 'eval_0_f1': 0.951310861423221, 'eval_0_support': 131.0, 'eval_1_precision': 0.875, 'eval_1_recall': 0.7567567567567568, 'eval_1_f1': 0.8115942028985507, 'eval_1_support': 37.0, 'eval_runtime': 32.1911, 'eval_samples_per_second': 5.219, 'eval_steps_per_second': 0.652, 'epoch': 8.0}\n",
      "{'loss': 0.0003, 'grad_norm': 0.006551771890372038, 'learning_rate': 2.916666666666667e-05, 'epoch': 8.11}\n",
      "{'loss': 0.0034, 'grad_norm': 60.09789276123047, 'learning_rate': 2.7083333333333332e-05, 'epoch': 8.24}\n",
      "{'loss': 0.0005, 'grad_norm': 0.005130317062139511, 'learning_rate': 2.5e-05, 'epoch': 8.38}\n",
      "{'loss': 0.0004, 'grad_norm': 0.010389810428023338, 'learning_rate': 2.2916666666666667e-05, 'epoch': 8.51}\n",
      "{'loss': 0.0002, 'grad_norm': 0.003796280361711979, 'learning_rate': 2.0833333333333336e-05, 'epoch': 8.65}\n",
      "{'loss': 0.0006, 'grad_norm': 0.003580014919862151, 'learning_rate': 1.8750000000000002e-05, 'epoch': 8.78}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0034491796977818012, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.92}\n",
      "{'eval_loss': 0.6002488136291504, 'eval_accuracy': 0.9226190476190477, 'eval_weighted_f1': 0.919583574427385, 'eval_weighted_precision': 0.9214717046238786, 'eval_weighted_recall': 0.9226190476190477, 'eval_confusion_matrix': [[128, 3], [10, 27]], 'eval_0_precision': 0.927536231884058, 'eval_0_recall': 0.9770992366412213, 'eval_0_f1': 0.9516728624535316, 'eval_0_support': 131.0, 'eval_1_precision': 0.9, 'eval_1_recall': 0.7297297297297297, 'eval_1_f1': 0.8059701492537312, 'eval_1_support': 37.0, 'eval_runtime': 31.4775, 'eval_samples_per_second': 5.337, 'eval_steps_per_second': 0.667, 'epoch': 9.0}\n",
      "{'loss': 0.0002, 'grad_norm': 0.33998405933380127, 'learning_rate': 1.4583333333333335e-05, 'epoch': 9.05}\n",
      "{'loss': 0.0002, 'grad_norm': 0.07049214839935303, 'learning_rate': 1.25e-05, 'epoch': 9.19}\n",
      "{'loss': 0.0001, 'grad_norm': 0.004409030079841614, 'learning_rate': 1.0416666666666668e-05, 'epoch': 9.32}\n",
      "{'loss': 0.0001, 'grad_norm': 0.003589556086808443, 'learning_rate': 8.333333333333334e-06, 'epoch': 9.46}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0036306444089859724, 'learning_rate': 6.25e-06, 'epoch': 9.59}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0034098646137863398, 'learning_rate': 4.166666666666667e-06, 'epoch': 9.73}\n",
      "{'loss': 0.0229, 'grad_norm': 0.0035319007001817226, 'learning_rate': 2.0833333333333334e-06, 'epoch': 9.86}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0025806287303566933, 'learning_rate': 0.0, 'epoch': 10.0}\n",
      "{'eval_loss': 0.5645637512207031, 'eval_accuracy': 0.9285714285714286, 'eval_weighted_f1': 0.9262197416279944, 'eval_weighted_precision': 0.9274614013253052, 'eval_weighted_recall': 0.9285714285714286, 'eval_confusion_matrix': [[128, 3], [9, 28]], 'eval_0_precision': 0.9343065693430657, 'eval_0_recall': 0.9770992366412213, 'eval_0_f1': 0.9552238805970148, 'eval_0_support': 131.0, 'eval_1_precision': 0.9032258064516129, 'eval_1_recall': 0.7567567567567568, 'eval_1_f1': 0.823529411764706, 'eval_1_support': 37.0, 'eval_runtime': 43.6865, 'eval_samples_per_second': 3.846, 'eval_steps_per_second': 0.481, 'epoch': 10.0}\n",
      "{'train_runtime': 8119.4684, 'train_samples_per_second': 1.448, 'train_steps_per_second': 0.091, 'train_loss': 0.133977934297621, 'epoch': 10.0}\n",
      "Data logged to DL2_Training_Análisis General.xlsx in sheet Sin_contexto\n",
      "{'eval_loss': 0.7541131973266602, 'eval_accuracy': 0.9050445103857567, 'eval_weighted_f1': 0.902489872916414, 'eval_weighted_precision': 0.902226945902762, 'eval_weighted_recall': 0.9050445103857567, 'eval_confusion_matrix': [[252, 11], [21, 53]], 'eval_0_precision': 0.9230769230769231, 'eval_0_recall': 0.9581749049429658, 'eval_0_f1': 0.9402985074626866, 'eval_0_support': 263.0, 'eval_1_precision': 0.828125, 'eval_1_recall': 0.7162162162162162, 'eval_1_f1': 0.7681159420289855, 'eval_1_support': 74.0, 'eval_runtime': 86.5869, 'eval_samples_per_second': 3.892, 'eval_steps_per_second': 0.497, 'epoch': 10.0}\n",
      "Data logged to DL2_Análisis General.xlsx in sheet Sin_contexto\n",
      "----------\n",
      "type_d='Tweet_context'\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.684, 'grad_norm': 6.245037078857422, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.14}\n",
      "{'loss': 0.6237, 'grad_norm': 3.5096049308776855, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.27}\n",
      "{'loss': 0.5399, 'grad_norm': 5.082241058349609, 'learning_rate': 3e-06, 'epoch': 0.41}\n",
      "{'loss': 0.5294, 'grad_norm': 1.9473881721496582, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.54}\n",
      "{'loss': 0.4251, 'grad_norm': 4.318591594696045, 'learning_rate': 5e-06, 'epoch': 0.68}\n",
      "{'loss': 0.4267, 'grad_norm': 6.815649509429932, 'learning_rate': 6e-06, 'epoch': 0.81}\n",
      "{'loss': 0.4776, 'grad_norm': 3.081890344619751, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3871278762817383, 'eval_accuracy': 0.7797619047619048, 'eval_weighted_f1': 0.683269628921803, 'eval_weighted_precision': 0.6080286281179138, 'eval_weighted_recall': 0.7797619047619048, 'eval_confusion_matrix': [[131, 0], [37, 0]], 'eval_0_precision': 0.7797619047619048, 'eval_0_recall': 1.0, 'eval_0_f1': 0.8762541806020068, 'eval_0_support': 131.0, 'eval_1_precision': 0.0, 'eval_1_recall': 0.0, 'eval_1_f1': 0.0, 'eval_1_support': 37.0, 'eval_runtime': 47.461, 'eval_samples_per_second': 3.54, 'eval_steps_per_second': 0.442, 'epoch': 1.0}\n",
      "{'loss': 0.3403, 'grad_norm': 8.173259735107422, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.08}\n",
      "{'loss': 0.2914, 'grad_norm': 4.809120178222656, 'learning_rate': 9e-06, 'epoch': 1.22}\n",
      "{'loss': 0.4083, 'grad_norm': 9.677495002746582, 'learning_rate': 1e-05, 'epoch': 1.35}\n",
      "{'loss': 0.3358, 'grad_norm': 2.4799954891204834, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.49}\n",
      "{'loss': 0.3199, 'grad_norm': 12.951946258544922, 'learning_rate': 1.2e-05, 'epoch': 1.62}\n",
      "{'loss': 0.2585, 'grad_norm': 8.287487983703613, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.76}\n",
      "{'loss': 0.3153, 'grad_norm': 9.949606895446777, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.89}\n",
      "{'eval_loss': 0.24274587631225586, 'eval_accuracy': 0.9107142857142857, 'eval_weighted_f1': 0.9083153046269802, 'eval_weighted_precision': 0.9082523634453782, 'eval_weighted_recall': 0.9107142857142857, 'eval_confusion_matrix': [[126, 5], [10, 27]], 'eval_0_precision': 0.9264705882352942, 'eval_0_recall': 0.9618320610687023, 'eval_0_f1': 0.9438202247191011, 'eval_0_support': 131.0, 'eval_1_precision': 0.84375, 'eval_1_recall': 0.7297297297297297, 'eval_1_f1': 0.7826086956521738, 'eval_1_support': 37.0, 'eval_runtime': 47.6495, 'eval_samples_per_second': 3.526, 'eval_steps_per_second': 0.441, 'epoch': 2.0}\n",
      "{'loss': 0.2594, 'grad_norm': 11.006592750549316, 'learning_rate': 1.5e-05, 'epoch': 2.03}\n",
      "{'loss': 0.2018, 'grad_norm': 10.492549896240234, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.16}\n",
      "{'loss': 0.2573, 'grad_norm': 8.19095516204834, 'learning_rate': 1.7000000000000003e-05, 'epoch': 2.3}\n",
      "{'loss': 0.1923, 'grad_norm': 9.569119453430176, 'learning_rate': 1.8e-05, 'epoch': 2.43}\n",
      "{'loss': 0.1622, 'grad_norm': 2.829641819000244, 'learning_rate': 1.9e-05, 'epoch': 2.57}\n",
      "{'loss': 0.1873, 'grad_norm': 10.359050750732422, 'learning_rate': 2e-05, 'epoch': 2.7}\n",
      "{'loss': 0.1816, 'grad_norm': 4.883455276489258, 'learning_rate': 2.1e-05, 'epoch': 2.84}\n",
      "{'loss': 0.1596, 'grad_norm': 4.167625427246094, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.97}\n",
      "{'eval_loss': 0.2680390179157257, 'eval_accuracy': 0.9285714285714286, 'eval_weighted_f1': 0.9262197416279944, 'eval_weighted_precision': 0.9274614013253052, 'eval_weighted_recall': 0.9285714285714286, 'eval_confusion_matrix': [[128, 3], [9, 28]], 'eval_0_precision': 0.9343065693430657, 'eval_0_recall': 0.9770992366412213, 'eval_0_f1': 0.9552238805970148, 'eval_0_support': 131.0, 'eval_1_precision': 0.9032258064516129, 'eval_1_recall': 0.7567567567567568, 'eval_1_f1': 0.823529411764706, 'eval_1_support': 37.0, 'eval_runtime': 36.5972, 'eval_samples_per_second': 4.591, 'eval_steps_per_second': 0.574, 'epoch': 3.0}\n",
      "{'loss': 0.1216, 'grad_norm': 3.6290690898895264, 'learning_rate': 2.3000000000000003e-05, 'epoch': 3.11}\n",
      "{'loss': 0.1317, 'grad_norm': 29.80828285217285, 'learning_rate': 2.4e-05, 'epoch': 3.24}\n",
      "{'loss': 0.0537, 'grad_norm': 0.7506250739097595, 'learning_rate': 2.5e-05, 'epoch': 3.38}\n",
      "{'loss': 0.0563, 'grad_norm': 0.21143224835395813, 'learning_rate': 2.6000000000000002e-05, 'epoch': 3.51}\n",
      "{'loss': 0.1363, 'grad_norm': 11.88323974609375, 'learning_rate': 2.7000000000000002e-05, 'epoch': 3.65}\n",
      "{'loss': 0.1642, 'grad_norm': 3.0199570655822754, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.78}\n",
      "{'loss': 0.0882, 'grad_norm': 28.140853881835938, 'learning_rate': 2.9e-05, 'epoch': 3.92}\n",
      "{'eval_loss': 0.28598323464393616, 'eval_accuracy': 0.9226190476190477, 'eval_weighted_f1': 0.9205399306767161, 'eval_weighted_precision': 0.9208683473389355, 'eval_weighted_recall': 0.9226190476190477, 'eval_confusion_matrix': [[127, 4], [9, 28]], 'eval_0_precision': 0.9338235294117647, 'eval_0_recall': 0.9694656488549618, 'eval_0_f1': 0.951310861423221, 'eval_0_support': 131.0, 'eval_1_precision': 0.875, 'eval_1_recall': 0.7567567567567568, 'eval_1_f1': 0.8115942028985507, 'eval_1_support': 37.0, 'eval_runtime': 35.4174, 'eval_samples_per_second': 4.743, 'eval_steps_per_second': 0.593, 'epoch': 4.0}\n",
      "{'loss': 0.0362, 'grad_norm': 0.15290437638759613, 'learning_rate': 3e-05, 'epoch': 4.05}\n",
      "{'loss': 0.0041, 'grad_norm': 0.2820141911506653, 'learning_rate': 3.1e-05, 'epoch': 4.19}\n",
      "{'loss': 0.0676, 'grad_norm': 0.022691942751407623, 'learning_rate': 3.2000000000000005e-05, 'epoch': 4.32}\n",
      "{'loss': 0.0975, 'grad_norm': 0.11792789399623871, 'learning_rate': 3.3e-05, 'epoch': 4.46}\n",
      "{'loss': 0.1238, 'grad_norm': 0.15592990815639496, 'learning_rate': 3.4000000000000007e-05, 'epoch': 4.59}\n",
      "{'loss': 0.035, 'grad_norm': 0.14142701029777527, 'learning_rate': 3.5e-05, 'epoch': 4.73}\n",
      "{'loss': 0.0673, 'grad_norm': 0.03302561119198799, 'learning_rate': 3.6e-05, 'epoch': 4.86}\n",
      "{'loss': 0.0483, 'grad_norm': 1.3390439748764038, 'learning_rate': 3.7e-05, 'epoch': 5.0}\n",
      "{'eval_loss': 0.3671186864376068, 'eval_accuracy': 0.9226190476190477, 'eval_weighted_f1': 0.9222361531726604, 'eval_weighted_precision': 0.9219426406926408, 'eval_weighted_recall': 0.9226190476190477, 'eval_confusion_matrix': [[125, 6], [7, 30]], 'eval_0_precision': 0.946969696969697, 'eval_0_recall': 0.9541984732824428, 'eval_0_f1': 0.9505703422053233, 'eval_0_support': 131.0, 'eval_1_precision': 0.8333333333333334, 'eval_1_recall': 0.8108108108108109, 'eval_1_f1': 0.8219178082191781, 'eval_1_support': 37.0, 'eval_runtime': 47.9036, 'eval_samples_per_second': 3.507, 'eval_steps_per_second': 0.438, 'epoch': 5.0}\n",
      "{'loss': 0.0014, 'grad_norm': 0.012579572387039661, 'learning_rate': 3.8e-05, 'epoch': 5.14}\n",
      "{'loss': 0.0011, 'grad_norm': 2.609257698059082, 'learning_rate': 3.9000000000000006e-05, 'epoch': 5.27}\n",
      "{'loss': 0.0588, 'grad_norm': 0.006895158905535936, 'learning_rate': 4e-05, 'epoch': 5.41}\n",
      "{'loss': 0.1718, 'grad_norm': 86.6944808959961, 'learning_rate': 4.1e-05, 'epoch': 5.54}\n",
      "{'loss': 0.0418, 'grad_norm': 0.26466473937034607, 'learning_rate': 4.2e-05, 'epoch': 5.68}\n",
      "{'loss': 0.0511, 'grad_norm': 0.04433821514248848, 'learning_rate': 4.3e-05, 'epoch': 5.81}\n",
      "{'loss': 0.0101, 'grad_norm': 17.80461883544922, 'learning_rate': 4.4000000000000006e-05, 'epoch': 5.95}\n",
      "{'eval_loss': 0.4450382590293884, 'eval_accuracy': 0.9285714285714286, 'eval_weighted_f1': 0.9292365529207635, 'eval_weighted_precision': 0.9302467558281512, 'eval_weighted_recall': 0.9285714285714286, 'eval_confusion_matrix': [[124, 7], [5, 32]], 'eval_0_precision': 0.9612403100775194, 'eval_0_recall': 0.9465648854961832, 'eval_0_f1': 0.9538461538461538, 'eval_0_support': 131.0, 'eval_1_precision': 0.8205128205128205, 'eval_1_recall': 0.8648648648648649, 'eval_1_f1': 0.8421052631578947, 'eval_1_support': 37.0, 'eval_runtime': 47.6049, 'eval_samples_per_second': 3.529, 'eval_steps_per_second': 0.441, 'epoch': 6.0}\n",
      "{'loss': 0.0452, 'grad_norm': 0.03849980980157852, 'learning_rate': 4.5e-05, 'epoch': 6.08}\n",
      "{'loss': 0.0167, 'grad_norm': 0.15887542068958282, 'learning_rate': 4.600000000000001e-05, 'epoch': 6.22}\n",
      "{'loss': 0.001, 'grad_norm': 0.3404427766799927, 'learning_rate': 4.7e-05, 'epoch': 6.35}\n",
      "{'loss': 0.0917, 'grad_norm': 0.03675570711493492, 'learning_rate': 4.8e-05, 'epoch': 6.49}\n",
      "{'loss': 0.0108, 'grad_norm': 0.017448971047997475, 'learning_rate': 4.9e-05, 'epoch': 6.62}\n",
      "{'loss': 0.01, 'grad_norm': 0.6312808990478516, 'learning_rate': 5e-05, 'epoch': 6.76}\n",
      "{'loss': 0.0026, 'grad_norm': 21.663545608520508, 'learning_rate': 4.791666666666667e-05, 'epoch': 6.89}\n",
      "{'eval_loss': 0.5549703240394592, 'eval_accuracy': 0.9166666666666666, 'eval_weighted_f1': 0.9188151041666666, 'eval_weighted_precision': 0.923829457364341, 'eval_weighted_recall': 0.9166666666666666, 'eval_confusion_matrix': [[121, 10], [4, 33]], 'eval_0_precision': 0.968, 'eval_0_recall': 0.9236641221374046, 'eval_0_f1': 0.9453124999999999, 'eval_0_support': 131.0, 'eval_1_precision': 0.7674418604651163, 'eval_1_recall': 0.8918918918918919, 'eval_1_f1': 0.825, 'eval_1_support': 37.0, 'eval_runtime': 35.0647, 'eval_samples_per_second': 4.791, 'eval_steps_per_second': 0.599, 'epoch': 7.0}\n",
      "{'loss': 0.0022, 'grad_norm': 0.0051500857807695866, 'learning_rate': 4.5833333333333334e-05, 'epoch': 7.03}\n",
      "{'loss': 0.063, 'grad_norm': 2.093860387802124, 'learning_rate': 4.375e-05, 'epoch': 7.16}\n",
      "{'loss': 0.0655, 'grad_norm': 0.13329678773880005, 'learning_rate': 4.166666666666667e-05, 'epoch': 7.3}\n",
      "{'loss': 0.0042, 'grad_norm': 0.014804171398282051, 'learning_rate': 3.958333333333333e-05, 'epoch': 7.43}\n",
      "{'loss': 0.0009, 'grad_norm': 0.012800414115190506, 'learning_rate': 3.7500000000000003e-05, 'epoch': 7.57}\n",
      "{'loss': 0.0018, 'grad_norm': 0.04160204902291298, 'learning_rate': 3.541666666666667e-05, 'epoch': 7.7}\n",
      "{'loss': 0.0003, 'grad_norm': 0.007080317474901676, 'learning_rate': 3.3333333333333335e-05, 'epoch': 7.84}\n",
      "{'loss': 0.0003, 'grad_norm': 0.06028969585895538, 'learning_rate': 3.125e-05, 'epoch': 7.97}\n",
      "{'eval_loss': 0.5739718079566956, 'eval_accuracy': 0.9226190476190477, 'eval_weighted_f1': 0.9214222441567647, 'eval_weighted_precision': 0.9210590952799029, 'eval_weighted_recall': 0.9226190476190477, 'eval_confusion_matrix': [[126, 5], [8, 29]], 'eval_0_precision': 0.9402985074626866, 'eval_0_recall': 0.9618320610687023, 'eval_0_f1': 0.9509433962264152, 'eval_0_support': 131.0, 'eval_1_precision': 0.8529411764705882, 'eval_1_recall': 0.7837837837837838, 'eval_1_f1': 0.8169014084507041, 'eval_1_support': 37.0, 'eval_runtime': 47.756, 'eval_samples_per_second': 3.518, 'eval_steps_per_second': 0.44, 'epoch': 8.0}\n",
      "{'loss': 0.0002, 'grad_norm': 0.004313662648200989, 'learning_rate': 2.916666666666667e-05, 'epoch': 8.11}\n",
      "{'loss': 0.0381, 'grad_norm': 0.021066004410386086, 'learning_rate': 2.7083333333333332e-05, 'epoch': 8.24}\n",
      "{'loss': 0.0458, 'grad_norm': 0.011854873970150948, 'learning_rate': 2.5e-05, 'epoch': 8.38}\n",
      "{'loss': 0.0002, 'grad_norm': 0.00568546075373888, 'learning_rate': 2.2916666666666667e-05, 'epoch': 8.51}\n",
      "{'loss': 0.0003, 'grad_norm': 0.0024229122791439295, 'learning_rate': 2.0833333333333336e-05, 'epoch': 8.65}\n",
      "{'loss': 0.0125, 'grad_norm': 0.00471405079588294, 'learning_rate': 1.8750000000000002e-05, 'epoch': 8.78}\n",
      "{'loss': 0.1087, 'grad_norm': 0.003804147243499756, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.92}\n",
      "{'eval_loss': 0.46006983518600464, 'eval_accuracy': 0.9404761904761905, 'eval_weighted_f1': 0.9392230576441104, 'eval_weighted_precision': 0.939546256212923, 'eval_weighted_recall': 0.9404761904761905, 'eval_confusion_matrix': [[128, 3], [7, 30]], 'eval_0_precision': 0.9481481481481482, 'eval_0_recall': 0.9770992366412213, 'eval_0_f1': 0.962406015037594, 'eval_0_support': 131.0, 'eval_1_precision': 0.9090909090909091, 'eval_1_recall': 0.8108108108108109, 'eval_1_f1': 0.8571428571428571, 'eval_1_support': 37.0, 'eval_runtime': 47.2835, 'eval_samples_per_second': 3.553, 'eval_steps_per_second': 0.444, 'epoch': 9.0}\n",
      "{'loss': 0.0011, 'grad_norm': 0.005252743139863014, 'learning_rate': 1.4583333333333335e-05, 'epoch': 9.05}\n",
      "{'loss': 0.053, 'grad_norm': 0.006501867901533842, 'learning_rate': 1.25e-05, 'epoch': 9.19}\n",
      "{'loss': 0.0003, 'grad_norm': 0.007713865954428911, 'learning_rate': 1.0416666666666668e-05, 'epoch': 9.32}\n",
      "{'loss': 0.0023, 'grad_norm': 0.005367500241845846, 'learning_rate': 8.333333333333334e-06, 'epoch': 9.46}\n",
      "{'loss': 0.0312, 'grad_norm': 0.0070807188749313354, 'learning_rate': 6.25e-06, 'epoch': 9.59}\n",
      "{'loss': 0.0003, 'grad_norm': 0.005551363341510296, 'learning_rate': 4.166666666666667e-06, 'epoch': 9.73}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0055116624571383, 'learning_rate': 2.0833333333333334e-06, 'epoch': 9.86}\n",
      "{'loss': 0.0003, 'grad_norm': 0.005698401480913162, 'learning_rate': 0.0, 'epoch': 10.0}\n",
      "{'eval_loss': 0.44781243801116943, 'eval_accuracy': 0.9345238095238095, 'eval_weighted_f1': 0.9348348841452291, 'eval_weighted_precision': 0.9352347214189319, 'eval_weighted_recall': 0.9345238095238095, 'eval_confusion_matrix': [[125, 6], [5, 32]], 'eval_0_precision': 0.9615384615384616, 'eval_0_recall': 0.9541984732824428, 'eval_0_f1': 0.9578544061302683, 'eval_0_support': 131.0, 'eval_1_precision': 0.8421052631578947, 'eval_1_recall': 0.8648648648648649, 'eval_1_f1': 0.8533333333333334, 'eval_1_support': 37.0, 'eval_runtime': 38.6533, 'eval_samples_per_second': 4.346, 'eval_steps_per_second': 0.543, 'epoch': 10.0}\n",
      "{'train_runtime': 8576.7871, 'train_samples_per_second': 1.371, 'train_steps_per_second': 0.086, 'train_loss': 0.131833115109318, 'epoch': 10.0}\n",
      "Data logged to DL2_Training_Análisis General.xlsx in sheet Tweet_context\n",
      "{'eval_loss': 0.6697214841842651, 'eval_accuracy': 0.8961424332344213, 'eval_weighted_f1': 0.8973410954529345, 'eval_weighted_precision': 0.8990359795560581, 'eval_weighted_recall': 0.8961424332344213, 'eval_confusion_matrix': [[243, 20], [15, 59]], 'eval_0_precision': 0.9418604651162791, 'eval_0_recall': 0.9239543726235742, 'eval_0_f1': 0.9328214971209213, 'eval_0_support': 263.0, 'eval_1_precision': 0.7468354430379747, 'eval_1_recall': 0.7972972972972973, 'eval_1_f1': 0.7712418300653594, 'eval_1_support': 74.0, 'eval_runtime': 94.4407, 'eval_samples_per_second': 3.568, 'eval_steps_per_second': 0.455, 'epoch': 10.0}\n",
      "Data logged to DL2_Análisis General.xlsx in sheet Tweet_context\n",
      "----------\n",
      "type_d='Full_context'\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6659, 'grad_norm': 6.87648344039917, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.14}\n",
      "{'loss': 0.5904, 'grad_norm': 2.6855781078338623, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.27}\n",
      "{'loss': 0.5352, 'grad_norm': 4.862576484680176, 'learning_rate': 3e-06, 'epoch': 0.41}\n",
      "{'loss': 0.5589, 'grad_norm': 1.9258991479873657, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.54}\n",
      "{'loss': 0.4526, 'grad_norm': 4.372073650360107, 'learning_rate': 5e-06, 'epoch': 0.68}\n",
      "{'loss': 0.4817, 'grad_norm': 8.488607406616211, 'learning_rate': 6e-06, 'epoch': 0.81}\n",
      "{'loss': 0.5383, 'grad_norm': 4.598710060119629, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4440833032131195, 'eval_accuracy': 0.7797619047619048, 'eval_weighted_f1': 0.683269628921803, 'eval_weighted_precision': 0.6080286281179138, 'eval_weighted_recall': 0.7797619047619048, 'eval_confusion_matrix': [[131, 0], [37, 0]], 'eval_0_precision': 0.7797619047619048, 'eval_0_recall': 1.0, 'eval_0_f1': 0.8762541806020068, 'eval_0_support': 131.0, 'eval_1_precision': 0.0, 'eval_1_recall': 0.0, 'eval_1_f1': 0.0, 'eval_1_support': 37.0, 'eval_runtime': 73.9935, 'eval_samples_per_second': 2.27, 'eval_steps_per_second': 0.284, 'epoch': 1.0}\n",
      "{'loss': 0.4085, 'grad_norm': 7.28504753112793, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.08}\n",
      "{'loss': 0.3469, 'grad_norm': 3.4334473609924316, 'learning_rate': 9e-06, 'epoch': 1.22}\n",
      "{'loss': 0.4574, 'grad_norm': 8.746891021728516, 'learning_rate': 1e-05, 'epoch': 1.35}\n",
      "{'loss': 0.3954, 'grad_norm': 4.131123065948486, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.49}\n",
      "{'loss': 0.3879, 'grad_norm': 7.899631977081299, 'learning_rate': 1.2e-05, 'epoch': 1.62}\n",
      "{'loss': 0.3724, 'grad_norm': 6.750528335571289, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.76}\n",
      "{'loss': 0.4026, 'grad_norm': 9.57828426361084, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.89}\n",
      "{'eval_loss': 0.34891122579574585, 'eval_accuracy': 0.8273809523809523, 'eval_weighted_f1': 0.795422379478107, 'eval_weighted_precision': 0.8211580086580086, 'eval_weighted_recall': 0.8273809523809523, 'eval_confusion_matrix': [[128, 3], [26, 11]], 'eval_0_precision': 0.8311688311688312, 'eval_0_recall': 0.9770992366412213, 'eval_0_f1': 0.8982456140350877, 'eval_0_support': 131.0, 'eval_1_precision': 0.7857142857142857, 'eval_1_recall': 0.2972972972972973, 'eval_1_f1': 0.43137254901960786, 'eval_1_support': 37.0, 'eval_runtime': 73.8319, 'eval_samples_per_second': 2.275, 'eval_steps_per_second': 0.284, 'epoch': 2.0}\n",
      "{'loss': 0.4337, 'grad_norm': 12.063960075378418, 'learning_rate': 1.5e-05, 'epoch': 2.03}\n",
      "{'loss': 0.3344, 'grad_norm': 5.7293171882629395, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.16}\n",
      "{'loss': 0.2885, 'grad_norm': 3.510270118713379, 'learning_rate': 1.7000000000000003e-05, 'epoch': 2.3}\n",
      "{'loss': 0.2728, 'grad_norm': 16.714759826660156, 'learning_rate': 1.8e-05, 'epoch': 2.43}\n",
      "{'loss': 0.3822, 'grad_norm': 16.99664306640625, 'learning_rate': 1.9e-05, 'epoch': 2.57}\n",
      "{'loss': 0.2674, 'grad_norm': 5.0828328132629395, 'learning_rate': 2e-05, 'epoch': 2.7}\n",
      "{'loss': 0.2705, 'grad_norm': 7.235605716705322, 'learning_rate': 2.1e-05, 'epoch': 2.84}\n",
      "{'loss': 0.1903, 'grad_norm': 4.493076324462891, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.97}\n",
      "{'eval_loss': 0.20102666318416595, 'eval_accuracy': 0.9285714285714286, 'eval_weighted_f1': 0.9285714285714286, 'eval_weighted_precision': 0.9285714285714286, 'eval_weighted_recall': 0.9285714285714286, 'eval_confusion_matrix': [[125, 6], [6, 31]], 'eval_0_precision': 0.9541984732824428, 'eval_0_recall': 0.9541984732824428, 'eval_0_f1': 0.9541984732824428, 'eval_0_support': 131.0, 'eval_1_precision': 0.8378378378378378, 'eval_1_recall': 0.8378378378378378, 'eval_1_f1': 0.8378378378378378, 'eval_1_support': 37.0, 'eval_runtime': 73.562, 'eval_samples_per_second': 2.284, 'eval_steps_per_second': 0.285, 'epoch': 3.0}\n",
      "{'loss': 0.1267, 'grad_norm': 5.075680255889893, 'learning_rate': 2.3000000000000003e-05, 'epoch': 3.11}\n",
      "{'loss': 0.2332, 'grad_norm': 5.279847145080566, 'learning_rate': 2.4e-05, 'epoch': 3.24}\n",
      "{'loss': 0.0641, 'grad_norm': 4.377535343170166, 'learning_rate': 2.5e-05, 'epoch': 3.38}\n",
      "{'loss': 0.1849, 'grad_norm': 5.1525044441223145, 'learning_rate': 2.6000000000000002e-05, 'epoch': 3.51}\n",
      "{'loss': 0.2149, 'grad_norm': 10.73629379272461, 'learning_rate': 2.7000000000000002e-05, 'epoch': 3.65}\n",
      "{'loss': 0.1816, 'grad_norm': 11.283446311950684, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.78}\n",
      "{'loss': 0.1854, 'grad_norm': 7.201165199279785, 'learning_rate': 2.9e-05, 'epoch': 3.92}\n",
      "{'eval_loss': 0.28794875741004944, 'eval_accuracy': 0.8869047619047619, 'eval_weighted_f1': 0.8734852363884622, 'eval_weighted_precision': 0.8941521879021879, 'eval_weighted_recall': 0.8869047619047619, 'eval_confusion_matrix': [[130, 1], [18, 19]], 'eval_0_precision': 0.8783783783783784, 'eval_0_recall': 0.9923664122137404, 'eval_0_f1': 0.931899641577061, 'eval_0_support': 131.0, 'eval_1_precision': 0.95, 'eval_1_recall': 0.5135135135135135, 'eval_1_f1': 0.6666666666666667, 'eval_1_support': 37.0, 'eval_runtime': 67.7788, 'eval_samples_per_second': 2.479, 'eval_steps_per_second': 0.31, 'epoch': 4.0}\n",
      "{'loss': 0.1858, 'grad_norm': 10.714200019836426, 'learning_rate': 3e-05, 'epoch': 4.05}\n",
      "{'loss': 0.072, 'grad_norm': 0.2403552085161209, 'learning_rate': 3.1e-05, 'epoch': 4.19}\n",
      "{'loss': 0.1494, 'grad_norm': 0.130294531583786, 'learning_rate': 3.2000000000000005e-05, 'epoch': 4.32}\n",
      "{'loss': 0.0908, 'grad_norm': 0.6178023815155029, 'learning_rate': 3.3e-05, 'epoch': 4.46}\n",
      "{'loss': 0.1889, 'grad_norm': 0.09878759831190109, 'learning_rate': 3.4000000000000007e-05, 'epoch': 4.59}\n",
      "{'loss': 0.1623, 'grad_norm': 6.656754493713379, 'learning_rate': 3.5e-05, 'epoch': 4.73}\n",
      "{'loss': 0.1261, 'grad_norm': 19.59833335876465, 'learning_rate': 3.6e-05, 'epoch': 4.86}\n",
      "{'loss': 0.1279, 'grad_norm': 61.36819839477539, 'learning_rate': 3.7e-05, 'epoch': 5.0}\n",
      "{'eval_loss': 0.24129506945610046, 'eval_accuracy': 0.9404761904761905, 'eval_weighted_f1': 0.9404761904761905, 'eval_weighted_precision': 0.9404761904761905, 'eval_weighted_recall': 0.9404761904761905, 'eval_confusion_matrix': [[126, 5], [5, 32]], 'eval_0_precision': 0.9618320610687023, 'eval_0_recall': 0.9618320610687023, 'eval_0_f1': 0.9618320610687023, 'eval_0_support': 131.0, 'eval_1_precision': 0.8648648648648649, 'eval_1_recall': 0.8648648648648649, 'eval_1_f1': 0.8648648648648649, 'eval_1_support': 37.0, 'eval_runtime': 53.8626, 'eval_samples_per_second': 3.119, 'eval_steps_per_second': 0.39, 'epoch': 5.0}\n",
      "{'loss': 0.0469, 'grad_norm': 1.5452884435653687, 'learning_rate': 3.8e-05, 'epoch': 5.14}\n",
      "{'loss': 0.0041, 'grad_norm': 0.06986178457736969, 'learning_rate': 3.9000000000000006e-05, 'epoch': 5.27}\n",
      "{'loss': 0.089, 'grad_norm': 0.027342917397618294, 'learning_rate': 4e-05, 'epoch': 5.41}\n",
      "{'loss': 0.0938, 'grad_norm': 98.8696060180664, 'learning_rate': 4.1e-05, 'epoch': 5.54}\n",
      "{'loss': 0.2038, 'grad_norm': 0.10901721566915512, 'learning_rate': 4.2e-05, 'epoch': 5.68}\n",
      "{'loss': 0.0512, 'grad_norm': 8.071258544921875, 'learning_rate': 4.3e-05, 'epoch': 5.81}\n",
      "{'loss': 0.2303, 'grad_norm': 46.147987365722656, 'learning_rate': 4.4000000000000006e-05, 'epoch': 5.95}\n",
      "{'eval_loss': 0.31561383605003357, 'eval_accuracy': 0.9166666666666666, 'eval_weighted_f1': 0.9199735449735451, 'eval_weighted_precision': 0.9308803704354961, 'eval_weighted_recall': 0.9166666666666666, 'eval_confusion_matrix': [[119, 12], [2, 35]], 'eval_0_precision': 0.9834710743801653, 'eval_0_recall': 0.9083969465648855, 'eval_0_f1': 0.9444444444444445, 'eval_0_support': 131.0, 'eval_1_precision': 0.7446808510638298, 'eval_1_recall': 0.9459459459459459, 'eval_1_f1': 0.8333333333333334, 'eval_1_support': 37.0, 'eval_runtime': 60.4274, 'eval_samples_per_second': 2.78, 'eval_steps_per_second': 0.348, 'epoch': 6.0}\n",
      "{'loss': 0.1133, 'grad_norm': 0.25956887006759644, 'learning_rate': 4.5e-05, 'epoch': 6.08}\n",
      "{'loss': 0.0049, 'grad_norm': 0.5396708846092224, 'learning_rate': 4.600000000000001e-05, 'epoch': 6.22}\n",
      "{'loss': 0.038, 'grad_norm': 0.08336003124713898, 'learning_rate': 4.7e-05, 'epoch': 6.35}\n",
      "{'loss': 0.084, 'grad_norm': 4.933199882507324, 'learning_rate': 4.8e-05, 'epoch': 6.49}\n",
      "{'loss': 0.0727, 'grad_norm': 0.3920222222805023, 'learning_rate': 4.9e-05, 'epoch': 6.62}\n",
      "{'loss': 0.0514, 'grad_norm': 47.24224090576172, 'learning_rate': 5e-05, 'epoch': 6.76}\n",
      "{'loss': 0.0022, 'grad_norm': 0.00977267138659954, 'learning_rate': 4.791666666666667e-05, 'epoch': 6.89}\n",
      "{'eval_loss': 0.5567038655281067, 'eval_accuracy': 0.8988095238095238, 'eval_weighted_f1': 0.8934843139640187, 'eval_weighted_precision': 0.8961309523809524, 'eval_weighted_recall': 0.8988095238095238, 'eval_confusion_matrix': [[127, 4], [13, 24]], 'eval_0_precision': 0.9071428571428571, 'eval_0_recall': 0.9694656488549618, 'eval_0_f1': 0.9372693726937269, 'eval_0_support': 131.0, 'eval_1_precision': 0.8571428571428571, 'eval_1_recall': 0.6486486486486487, 'eval_1_f1': 0.7384615384615384, 'eval_1_support': 37.0, 'eval_runtime': 74.0156, 'eval_samples_per_second': 2.27, 'eval_steps_per_second': 0.284, 'epoch': 7.0}\n",
      "{'loss': 0.0918, 'grad_norm': 0.01333620585501194, 'learning_rate': 4.5833333333333334e-05, 'epoch': 7.03}\n",
      "{'loss': 0.0761, 'grad_norm': 0.012241639196872711, 'learning_rate': 4.375e-05, 'epoch': 7.16}\n",
      "{'loss': 0.1042, 'grad_norm': 49.700279235839844, 'learning_rate': 4.166666666666667e-05, 'epoch': 7.3}\n",
      "{'loss': 0.0472, 'grad_norm': 0.01206219382584095, 'learning_rate': 3.958333333333333e-05, 'epoch': 7.43}\n",
      "{'loss': 0.095, 'grad_norm': 24.40251350402832, 'learning_rate': 3.7500000000000003e-05, 'epoch': 7.57}\n",
      "{'loss': 0.0583, 'grad_norm': 6.780909538269043, 'learning_rate': 3.541666666666667e-05, 'epoch': 7.7}\n",
      "{'loss': 0.0022, 'grad_norm': 0.041338350623846054, 'learning_rate': 3.3333333333333335e-05, 'epoch': 7.84}\n",
      "{'loss': 0.0016, 'grad_norm': 3.2915449142456055, 'learning_rate': 3.125e-05, 'epoch': 7.97}\n",
      "{'eval_loss': 0.43692630529403687, 'eval_accuracy': 0.9226190476190477, 'eval_weighted_f1': 0.919583574427385, 'eval_weighted_precision': 0.9214717046238786, 'eval_weighted_recall': 0.9226190476190477, 'eval_confusion_matrix': [[128, 3], [10, 27]], 'eval_0_precision': 0.927536231884058, 'eval_0_recall': 0.9770992366412213, 'eval_0_f1': 0.9516728624535316, 'eval_0_support': 131.0, 'eval_1_precision': 0.9, 'eval_1_recall': 0.7297297297297297, 'eval_1_f1': 0.8059701492537312, 'eval_1_support': 37.0, 'eval_runtime': 73.4338, 'eval_samples_per_second': 2.288, 'eval_steps_per_second': 0.286, 'epoch': 8.0}\n",
      "{'loss': 0.0005, 'grad_norm': 0.01243564486503601, 'learning_rate': 2.916666666666667e-05, 'epoch': 8.11}\n",
      "{'loss': 0.0006, 'grad_norm': 0.01150510460138321, 'learning_rate': 2.7083333333333332e-05, 'epoch': 8.24}\n",
      "{'loss': 0.0351, 'grad_norm': 0.009616194292902946, 'learning_rate': 2.5e-05, 'epoch': 8.38}\n",
      "{'loss': 0.0004, 'grad_norm': 0.029643163084983826, 'learning_rate': 2.2916666666666667e-05, 'epoch': 8.51}\n",
      "{'loss': 0.0004, 'grad_norm': 0.0043921624310314655, 'learning_rate': 2.0833333333333336e-05, 'epoch': 8.65}\n",
      "{'loss': 0.0482, 'grad_norm': 0.008186603896319866, 'learning_rate': 1.8750000000000002e-05, 'epoch': 8.78}\n",
      "{'loss': 0.0007, 'grad_norm': 0.007789482828229666, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.92}\n",
      "{'eval_loss': 0.40365418791770935, 'eval_accuracy': 0.9345238095238095, 'eval_weighted_f1': 0.9327645567264521, 'eval_weighted_precision': 0.9334843312324931, 'eval_weighted_recall': 0.9345238095238095, 'eval_confusion_matrix': [[128, 3], [8, 29]], 'eval_0_precision': 0.9411764705882353, 'eval_0_recall': 0.9770992366412213, 'eval_0_f1': 0.9588014981273407, 'eval_0_support': 131.0, 'eval_1_precision': 0.90625, 'eval_1_recall': 0.7837837837837838, 'eval_1_f1': 0.8405797101449275, 'eval_1_support': 37.0, 'eval_runtime': 73.2172, 'eval_samples_per_second': 2.295, 'eval_steps_per_second': 0.287, 'epoch': 9.0}\n",
      "{'loss': 0.0005, 'grad_norm': 0.007022599224001169, 'learning_rate': 1.4583333333333335e-05, 'epoch': 9.05}\n",
      "{'loss': 0.0004, 'grad_norm': 0.013981888070702553, 'learning_rate': 1.25e-05, 'epoch': 9.19}\n",
      "{'loss': 0.0004, 'grad_norm': 0.010636155493557453, 'learning_rate': 1.0416666666666668e-05, 'epoch': 9.32}\n",
      "{'loss': 0.0052, 'grad_norm': 0.008054574951529503, 'learning_rate': 8.333333333333334e-06, 'epoch': 9.46}\n",
      "{'loss': 0.0003, 'grad_norm': 0.010576917789876461, 'learning_rate': 6.25e-06, 'epoch': 9.59}\n",
      "{'loss': 0.0003, 'grad_norm': 0.008272641338407993, 'learning_rate': 4.166666666666667e-06, 'epoch': 9.73}\n",
      "{'loss': 0.0202, 'grad_norm': 0.007147251162678003, 'learning_rate': 2.0833333333333334e-06, 'epoch': 9.86}\n",
      "{'loss': 0.0062, 'grad_norm': 0.016872163861989975, 'learning_rate': 0.0, 'epoch': 10.0}\n",
      "{'eval_loss': 0.35747432708740234, 'eval_accuracy': 0.9404761904761905, 'eval_weighted_f1': 0.9404761904761905, 'eval_weighted_precision': 0.9404761904761905, 'eval_weighted_recall': 0.9404761904761905, 'eval_confusion_matrix': [[126, 5], [5, 32]], 'eval_0_precision': 0.9618320610687023, 'eval_0_recall': 0.9618320610687023, 'eval_0_f1': 0.9618320610687023, 'eval_0_support': 131.0, 'eval_1_precision': 0.8648648648648649, 'eval_1_recall': 0.8648648648648649, 'eval_1_f1': 0.8648648648648649, 'eval_1_support': 37.0, 'eval_runtime': 72.9335, 'eval_samples_per_second': 2.303, 'eval_steps_per_second': 0.288, 'epoch': 10.0}\n",
      "{'train_runtime': 13713.7823, 'train_samples_per_second': 0.858, 'train_steps_per_second': 0.054, 'train_loss': 0.17579639505915898, 'epoch': 10.0}\n",
      "Data logged to DL2_Training_Análisis General.xlsx in sheet Full_context\n",
      "{'eval_loss': 0.5828076601028442, 'eval_accuracy': 0.913946587537092, 'eval_weighted_f1': 0.9156640723920231, 'eval_weighted_precision': 0.919008635772089, 'eval_weighted_recall': 0.913946587537092, 'eval_confusion_matrix': [[244, 19], [10, 64]], 'eval_0_precision': 0.9606299212598425, 'eval_0_recall': 0.9277566539923955, 'eval_0_f1': 0.9439071566731141, 'eval_0_support': 263.0, 'eval_1_precision': 0.7710843373493976, 'eval_1_recall': 0.8648648648648649, 'eval_1_f1': 0.8152866242038217, 'eval_1_support': 74.0, 'eval_runtime': 146.6474, 'eval_samples_per_second': 2.298, 'eval_steps_per_second': 0.293, 'epoch': 10.0}\n",
      "Data logged to DL2_Análisis General.xlsx in sheet Full_context\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "DATA = \"../data/BBDD_SeAcabo.csv\"\n",
    "\n",
    "# Target Column\n",
    "LABEL_COLUMN = 'Análisis General' # [\"Análisis General\", \"Contenido Negativo\", \"Insultos\"]\n",
    "\n",
    "# New Tokens\n",
    "NEW_TOKENS = [\"[INSULT]\", \"[VICTIM]\", \"[AGGRESSOR]\"]\n",
    "\n",
    "# Types Dataset\n",
    "TYPES_DATASET = [\"Sin_contexto\", \"Tweet_context\", \"Full_context\"] # [\"Sin_contexto\", \"Tweet_context\", \"Full_context\"]\n",
    "\n",
    "# Tweet original Alexia Putellas\n",
    "TWEET_ORIGINAL = \"Esto es inaceptable. Se acabó. Contigo compañera @Jennihermoso\"\n",
    "\n",
    "# Contexto\n",
    "CONTEXT =   \"\"\"\n",
    "            En agosto de 2023, tras la victoria de la Selección femenina de fútbol de España en la Copa Mundial Femenina de Fútbol de 2023, durante la celebración en la entrega de las medallas y tras abrazar efusivamente a varias jugadoras, Luis Rubiales besó en los labios a la centrocampista Jennifer Hermoso mientras sujetaba su cabeza con las manos. Hermoso lo denunció ante la Fiscalía por acoso sexual, coacciones y agresión sexual. La Fiscalía presentó una demanda contra Rubiales ante la Audiencia Nacional en Madrid\n",
    "            \"\"\"\n",
    "\n",
    "# Model\n",
    "MODEL_NAME = \"dccuchile/bert-base-spanish-wwm-cased\" #[\"dccuchile/bert-base-spanish-wwm-cased\", \"PlanTL-GOB-ES/roberta-large-bne\", \"bert-base-multilingual-cased\", \"FacebookAI/xlm-roberta-base\", \"pysentimiento/robertuito-base-cased\"]\n",
    "\n",
    "# Hyperparameters\n",
    "PADDING = True\n",
    "TRUNCATION = True\n",
    "MAX_LENGTH = 512\n",
    "NUM_TRAIN_EPOCHS = 10\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 16\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 8\n",
    "WARMUP_STEPS = 500\n",
    "WEIGHT_DECAY = 0.01\n",
    "LOGGING_STEPS = 10\n",
    "LEARNING_RATE_SCHEDULER_TYPE = \"cosine\" # \"linear\", \"cosine\"\n",
    "\n",
    "# EXCEL\n",
    "EXCEL_PATH = 'DL2'\n",
    "\n",
    "\n",
    "models = [MODEL_NAME]\n",
    "for model in models:\n",
    "\n",
    "    start_time = time.time()\n",
    "    timestamp = datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "    # Load data\n",
    "    df = load_data(dataset_name=DATA, embedding_name=None)\n",
    "\n",
    "    # Filter by lang\n",
    "    df = filter_by_lang(df)\n",
    "\n",
    "    ## Add special tokens\n",
    "    df = add_special_tokens(df, NEW_TOKENS)\n",
    "\n",
    "    # Preprocess data\n",
    "    df = preprocess_function(df, context=CONTEXT, tweet_original=TWEET_ORIGINAL)\n",
    "\n",
    "    # Labels\n",
    "    df, labels_names, num_labels = filter_by_type(df, LABEL_COLUMN)\n",
    "    print(num_labels)\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = load_tokenizer(name=MODEL_NAME)\n",
    "    tokenizer.add_tokens(NEW_TOKENS)\n",
    "\n",
    "    for type_d in TYPES_DATASET:\n",
    "        print(\"-\"*10)\n",
    "        print(f\"{type_d=}\")\n",
    "        print(\"-\"*10)\n",
    "\n",
    "\n",
    "        # Prepare input\n",
    "        inputs_dataset = prepare_inputs(df, text_column=type_d, label_column=LABEL_COLUMN, tokenizer=tokenizer)\n",
    "\n",
    "        # Create datasets\n",
    "        dataset = create_hf_dataset(inputs_dataset)\n",
    "\n",
    "        # Split dataset\n",
    "        dataset, class_counts = stratified_train_test_val_split(dataset, test_size=0.2, val_size=0.1)\n",
    "\n",
    "        # Información adicional para registrar\n",
    "        run_id = str(uuid.uuid4())\n",
    "        additional_info = {\n",
    "            'Model_Description': MODEL_NAME,\n",
    "            'Data_File': DATA,\n",
    "            'Type': type_d,\n",
    "            'Class_Counts': class_counts,\n",
    "        }\n",
    "        \n",
    "        # Load model\n",
    "        model = load_model(name=MODEL_NAME, num_labels=num_labels)\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        # Train\n",
    "        training_args = load_training_args(context_type=type_d, num_train_epochs=NUM_TRAIN_EPOCHS, per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE, per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE, warmup_steps=WARMUP_STEPS, weight_decay=WEIGHT_DECAY, logging_steps=LOGGING_STEPS)\n",
    "\n",
    "        save_results_callback = SaveResultsCallback(f\"{EXCEL_PATH}_Training_{LABEL_COLUMN}.xlsx\", training_args, run_id, additional_info, type_d, num_labels)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset['train'],\n",
    "            eval_dataset=dataset['validation'],\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[save_results_callback]\n",
    "        )\n",
    "        trainer.train()\n",
    "\n",
    "        # Eval\n",
    "        eval_results = trainer.evaluate(dataset['test'])\n",
    "        results_df = pd.DataFrame([eval_results])\n",
    "\n",
    "        # Llamar a log_data para guardar los resultados y la configuración\n",
    "        end_time = time.time()\n",
    "        exec_time = end_time - start_time\n",
    "        additional_info['Total Time'] = exec_time\n",
    "        log_data(f\"{EXCEL_PATH}_{LABEL_COLUMN}.xlsx\", run_id, eval_results, training_args, additional_info, type_d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline with OpenAI Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import Trainer\n",
    "from datasets import Dataset, DatasetDict, ClassLabel\n",
    "\n",
    "\n",
    "def load_embeddings(embeddings_path):\n",
    "    embeddings_df = pd.read_csv(embeddings_path)\n",
    "    embeddings_df['embeddings'] = embeddings_df['embeddings'].apply(lambda x: np.fromstring(x.strip('[]'), sep=' '))\n",
    "    return embeddings_df\n",
    "\n",
    "\n",
    "def prepare_inputs_openai(df, label_column):\n",
    "    \"\"\"\n",
    "    Prepare inputs from a DataFrame for training, validation, and testing.\n",
    "    Since we're using precomputed embeddings, this function will bypass tokenization.\n",
    "    \"\"\"\n",
    "    inputs = {\n",
    "        'input_ids': df['embeddings'].tolist(),\n",
    "        'labels': df[label_column].tolist()\n",
    "    }\n",
    "    return inputs\n",
    "\n",
    "# Path to your embeddings CSV file\n",
    "EMBEDDINGS_PATH = 'C:/Users/jorge/Desktop/UNI/4-CUARTO/4-2-TFG/CODE/Gender-Bias/OpenAI/seacabo_embeddings.csv'\n",
    "# Embedding name\n",
    "EMBEDDING_NAME = \"text-embedding-3-large\"\n",
    "\n",
    "# Load the embeddings\n",
    "embeddings_df = load_embeddings(EMBEDDINGS_PATH)\n",
    "\n",
    "# Configuration parameters\n",
    "DATA = \"../data/BBDD_SeAcabo.csv\"\n",
    "LABEL_COLUMN = 'Análisis General'\n",
    "NEW_TOKENS = [\"[INSULT]\", \"[VICTIM]\", \"[AGGRESSOR]\"]\n",
    "TYPES_DATASET = [\"Sin_contexto\", \"Tweet_context\", \"Full_context\"]\n",
    "TWEET_ORIGINAL = \"Esto es inaceptable. Se acabó. Contigo compañera @Jennihermoso\"\n",
    "CONTEXT = \"\"\"\n",
    "            En agosto de 2023, tras la victoria de la Selección femenina de fútbol de España en la Copa Mundial Femenina de Fútbol de 2023, durante la celebración en la entrega de las medallas y tras abrazar efusivamente a varias jugadoras, Luis Rubiales besó en los labios a la centrocampista Jennifer Hermoso mientras sujetaba su cabeza con las manos. Hermoso lo denunció ante la Fiscalía por acoso sexual, coacciones y agresión sexual. La Fiscalía presentó una demanda contra Rubiales ante la Audiencia Nacional en Madrid\n",
    "            \"\"\"\n",
    "MODEL_NAME = \"dccuchile/bert-base-spanish-wwm-cased\" #[\"dccuchile/bert-base-spanish-wwm-cased\", \"PlanTL-GOB-ES/roberta-large-bne\", \"bert-base-multilingual-cased\", \"FacebookAI/xlm-roberta-base\", \"pysentimiento/robertuito-base-cased\"]\n",
    "\n",
    "# Hyperparameters\n",
    "PADDING = True\n",
    "TRUNCATION = True\n",
    "MAX_LENGTH = 512\n",
    "NUM_TRAIN_EPOCHS = 10\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 16\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 8\n",
    "WARMUP_STEPS = 500\n",
    "WEIGHT_DECAY = 0.01\n",
    "LOGGING_STEPS = 10\n",
    "LEARNING_RATE_SCHEDULER_TYPE = \"cosine\" # \"linear\", \"cosine\"\n",
    "\n",
    "# EXCEL\n",
    "EXCEL_PATH = 'DL2'\n",
    "\n",
    "models = [MODEL_NAME]\n",
    "for model in models:\n",
    "    start_time = time.time()\n",
    "    timestamp = datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "    # Load data\n",
    "    df = load_data(dataset_name=DATA, embedding_name=EMBEDDING_NAME)\n",
    "\n",
    "    # Filter by lang\n",
    "    df = filter_by_lang(df)\n",
    "\n",
    "    # Add special tokens\n",
    "    df = add_special_tokens(df, NEW_TOKENS)\n",
    "\n",
    "    # Preprocess data\n",
    "    df = preprocess_function(df, context=CONTEXT, tweet_original=TWEET_ORIGINAL)\n",
    "\n",
    "    # Labels\n",
    "    df, labels_names, num_labels = filter_by_type(df, LABEL_COLUMN)\n",
    "    print(f\"{num_labels=}\")\n",
    "\n",
    "    for type_d in TYPES_DATASET:\n",
    "        print(\"-\"*10)\n",
    "        print(f\"{type_d=}\")\n",
    "        print(\"-\"*10)\n",
    "\n",
    "        # Prepare input (this step assumes you have a function for tokenizing or other preprocessing)\n",
    "        inputs_dataset = prepare_inputs_openai(df, label_column=LABEL_COLUMN)\n",
    "\n",
    "        # Create datasets\n",
    "        dataset = Dataset.from_dict(inputs_dataset)\n",
    "        dataset = dataset.class_encode_column(\"labels\")\n",
    "\n",
    "        # Split dataset into train, validation, and test sets\n",
    "        train_val_split = dataset.train_test_split(test_size=0.3, stratify_by_column='labels')\n",
    "        val_test_split = train_val_split['test'].train_test_split(test_size=0.5, stratify_by_column='labels')\n",
    "        \n",
    "        train_dataset = train_val_split['train']\n",
    "        val_dataset = val_test_split['train']\n",
    "        test_dataset = val_test_split['test']\n",
    "\n",
    "        # Create DatasetDict\n",
    "        datasets = DatasetDict({\n",
    "            'train': train_dataset,\n",
    "            'validation': val_dataset,\n",
    "            'test': test_dataset\n",
    "        })\n",
    "\n",
    "        # Información adicional para registrar\n",
    "        run_id = str(uuid.uuid4())\n",
    "        additional_info = {\n",
    "            'Model_Description': MODEL_NAME,\n",
    "            'Data_File': DATA,\n",
    "            'Type': type_d,\n",
    "            'Class_Counts': {\n",
    "                'train': train_dataset.features['labels'].names,\n",
    "                'validation': val_dataset.features['labels'].names,\n",
    "                'test': test_dataset.features['labels'].names,\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Load model\n",
    "        model = load_model(name=MODEL_NAME, num_labels=num_labels)\n",
    "\n",
    "        # Define training arguments\n",
    "        training_args = load_training_args(\n",
    "            context_type=type_d,\n",
    "            num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "            per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "            per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "            warmup_steps=WARMUP_STEPS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_steps=LOGGING_STEPS\n",
    "        )\n",
    "\n",
    "        save_results_callback = SaveResultsCallback(f\"{EXCEL_PATH}_Training_{LABEL_COLUMN}.xlsx\", training_args, run_id, additional_info, type_d, num_labels)\n",
    "\n",
    "        # Define Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=datasets['train'],\n",
    "            eval_dataset=datasets['validation'],\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[save_results_callback]\n",
    "        )\n",
    "\n",
    "        # Train\n",
    "        trainer.train()\n",
    "\n",
    "        # Evaluate\n",
    "        eval_results = trainer.evaluate(eval_dataset=datasets['test'])\n",
    "        results_df = pd.DataFrame([eval_results])\n",
    "\n",
    "        # Log data\n",
    "        end_time = time.time()\n",
    "        exec_time = end_time - start_time\n",
    "        additional_info['Total Time'] = exec_time\n",
    "        log_data(f\"{EXCEL_PATH}_{LABEL_COLUMN}.xlsx\", run_id, eval_results, training_args, additional_info, type_d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probar Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cargar el Modelo y el Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Ruta al checkpoint\n",
    "checkpoint_path = \"./results/checkpoint-1000\"\n",
    "\n",
    "# Cargar el tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n",
    "\n",
    "# Cargar el modelo\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preparar el Texto para la Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prediction_input(text, tokenizer):\n",
    "    # Preprocesa el texto como lo hiciste antes de entrenar (por ejemplo, limpieza básica, truncar, etc.)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=MAX_LENGTH, truncation=TRUNCATION, padding=PADDING)\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Realizar la Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, tokenizer, model):\n",
    "    # Preparar el texto para el modelo\n",
    "    model_inputs = prepare_prediction_input(text, tokenizer)\n",
    "    \n",
    "    # Mover el modelo a CPU o GPU según esté configurado\n",
    "    model.eval()  # Poner el modelo en modo de evaluación\n",
    "    with torch.no_grad():  # No calcular gradientes\n",
    "        outputs = model(**model_inputs)\n",
    "    \n",
    "    # Obtener logits\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Convertir los logits a probabilidades (opcional)\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Obtener la clase predicha\n",
    "    predicted_class_index = probabilities.argmax().item()\n",
    "    \n",
    "    return predicted_class_index, probabilities.numpy()\n",
    "\n",
    "# Ejemplo de uso\n",
    "tweet = \"@Jennihermoso, ánimo campeona\"\n",
    "text = tweet_original + \" [SEP] \" + tweet + \" [SEP] \" + contexto\n",
    "print(text)\n",
    "predicted_class, probabilities = predict(text, tokenizer, model)\n",
    "print(f\"Clase predicha: {predicted_class}\")\n",
    "print(f\"Probabilidades: {probabilities}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
