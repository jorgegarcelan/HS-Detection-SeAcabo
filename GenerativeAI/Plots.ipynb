{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI - Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file\n",
    "path = \"Training_GenAI.xlsx\"\n",
    "df = pd.read_excel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset to excel file\n",
    "df.to_excel('GenAI Experiments/Training_GenAI.xlsx', index=False, sheet_name='Data', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of labels and colors\n",
    "labels = ['Comentario Positivo', 'Comentario Negativo']\n",
    "colors = ['darkgreen', 'darkred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset for \"Análisis General\"\n",
    "ageneral_df = df[df[\"Type\"] == \"analisis_general\"]\n",
    "len(ageneral_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to categorical: {'Comentario Positivo': 0, 'Comentario Negativo': 1}\n",
    "mapping = {'Comentario Positivo': 0, 'Comentario Negativo': 1}\n",
    "\n",
    "# Create a dictionary for renaming\n",
    "rename_dict = {}\n",
    "for key, value in mapping.items():\n",
    "    rename_dict[f'{value}_precision'] = f'{key}_precision'\n",
    "    rename_dict[f'{value}_recall'] = f'{key}_recall'\n",
    "    rename_dict[f'{value}_f1-score'] = f'{key}_f1-score'\n",
    "    rename_dict[f'{value}_support'] = f'{key}_support'\n",
    "\n",
    "# Rename the columns\n",
    "ageneral_df.rename(columns=rename_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ageneral_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Timestamp' column to datetime if it's not already\n",
    "ageneral_df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "# Sort the DataFrame by 'Timestamp' if needed\n",
    "ageneral_df.sort_values('Timestamp', inplace=True)\n",
    "\n",
    "# Resetting the index to ensure it's sequential\n",
    "ageneral_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric columns\n",
    "numeric_cols = ['Accuracy_Global', 'Std_Global', 'Time (s)', 'weighted avg_precision', 'weighted avg_recall', 'weighted avg_f1-score', 'Comentario Positivo_f1-score', 'Comentario Negativo_f1-score']\n",
    "\n",
    "# Agrupar el DataFrame por context\n",
    "df_grouped_context = ageneral_df.groupby(['Context'])[numeric_cols].mean().reset_index()\n",
    "\n",
    "df_grouped_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric columns\n",
    "numeric_cols = ['Accuracy_Global', 'Std_Global', 'Time (s)', 'weighted avg_precision', 'weighted avg_recall', 'weighted avg_f1-score', 'Comentario Positivo_f1-score', 'Comentario Negativo_f1-score']\n",
    "\n",
    "# Agrupar el DataFrame por prompt\n",
    "df_grouped_prompt = ageneral_df.groupby(['Prompt'])[numeric_cols].mean().reset_index()\n",
    "\n",
    "df_grouped_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric columns\n",
    "numeric_cols = ['Accuracy_Global', 'Std_Global', 'Time (s)', 'weighted avg_precision', 'weighted avg_recall', 'weighted avg_f1-score', 'Comentario Positivo_f1-score', 'Comentario Negativo_f1-score']\n",
    "\n",
    "# Agrupar el DataFrame por prompt\n",
    "df_grouped_prompt = ageneral_df.groupby(['Context', 'Prompt'])[numeric_cols].mean().reset_index()\n",
    "\n",
    "df_grouped_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of specific dates for vertical lines\n",
    "specific_dates = ['2024-08-10 02:55:23', '2024-08-10 04:19:28', '2024-08-10 05:50:11', '2024-08-10 07:15:46', '2024-08-10 08:47:33', '2024-08-10 10:13:58', '2024-08-10 11:45:28', '2024-08-10 13:13:12', '2024-08-10 14:35:16']\n",
    "specific_dates = pd.to_datetime(specific_dates)  # Convert to datetime if not already\n",
    "\n",
    "# Find the indices of the specific dates in the DataFrame\n",
    "specific_indices = ageneral_df[ageneral_df['Timestamp'].isin(specific_dates)].index.tolist()\n",
    "specific_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 8))\n",
    "\n",
    "\n",
    "# Add a vertical line for best window\n",
    "for date, index in zip(specific_dates, specific_indices):\n",
    "    if index in [32]:\n",
    "        plt.axvline(x=index, color='#1a2e49', linestyle='-', linewidth=3)\n",
    "    else:\n",
    "        plt.axvline(x=index, color='#1a2e49', linestyle='--', linewidth=2)\n",
    "\n",
    "# Plotting the F1-Score as dots\n",
    "plt.plot(ageneral_df.index, ageneral_df['weighted avg_f1-score'], linewidth=6, linestyle='-', marker='o', markersize=16, color='black', alpha=0.7, label='Weighted Avg. F1-Score')\n",
    "\n",
    "\n",
    "# Calculate and plot trend line\n",
    "slope, intercept, r_value, p_value, std_err = linregress(ageneral_df.index, ageneral_df['weighted avg_f1-score'])\n",
    "trend = intercept + slope * ageneral_df.index\n",
    "plt.plot(ageneral_df.index, trend, linewidth=2, color='darkred', linestyle='-', alpha=0.5, label=f'Trend Line (R² = {r_value**2:.2f})')\n",
    "\n",
    "\n",
    "# Move legend to the top\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=2, fontsize=22)\n",
    "\n",
    "\n",
    "# Setting labels and title\n",
    "plt.title('Weighted Avg. F1-Score for \"Análisis General\"', fontsize=42)\n",
    "plt.xlabel('Experiments', fontsize=30)\n",
    "plt.ylabel('F1-Score', fontsize=30)\n",
    "plt.gca().set_ylim(0.75, 0.85)\n",
    "plt.gca().tick_params(axis='x', labelsize=22)\n",
    "plt.gca().tick_params(axis='y', labelsize=22)\n",
    "\n",
    "# Add a grid to improve readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "# Remove spines for a clean look\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"../../../IMAGES/Análisis General/GenAI/GenAI_Experiments in _Análisis General_ weighted.pdf\", format='pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 24))\n",
    "\n",
    "# Add a vertical line for best window\n",
    "for date, index in zip(specific_dates, specific_indices):\n",
    "    if index in [32]:\n",
    "        plt.axvline(x=index, color='#1a2e49', linestyle='-', linewidth=3)\n",
    "    else:\n",
    "        plt.axvline(x=index, color='#1a2e49', linestyle='--', linewidth=2)\n",
    "\n",
    "# Plotting the F1-Score as dots\n",
    "plt.plot(ageneral_df.index, ageneral_df['weighted avg_f1-score'], linewidth=9, linestyle='-', marker='o', markersize=18, color='black', alpha=0.7, label=\"Weighted Avg.\")\n",
    "for i, label in enumerate(labels):\n",
    "    plt.plot(ageneral_df.index, ageneral_df[f'{label}_f1-score'], linewidth=7, linestyle='-', marker='o', markersize=16, color=colors[i], alpha=0.7, label=label)\n",
    "\n",
    "\n",
    "# Calculate and plot trend line\n",
    "for i in range(len(labels)):\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(ageneral_df.index, ageneral_df[f'{labels[i]}_f1-score'])\n",
    "    trend = intercept + slope * ageneral_df.index\n",
    "    plt.plot(ageneral_df.index, trend, linewidth=2, color=colors[i], linestyle='-', alpha=0.5)\n",
    "\n",
    "plt.legend(loc='upper center', fontsize=22, bbox_to_anchor=(0.5, 0.99), ncol=3)\n",
    "\n",
    "\n",
    "# Setting labels and title\n",
    "plt.title('F1-Score per Class for \"Análisis General\"', fontsize=42)\n",
    "plt.xlabel('Experiments', fontsize=30)\n",
    "plt.ylabel('F1-Score', fontsize=30)\n",
    "plt.ylim(0.63, 0.90)\n",
    "plt.gca().tick_params(axis='x', labelsize=22)\n",
    "plt.gca().tick_params(axis='y', labelsize=22)\n",
    "\n",
    "\n",
    "# Remove spines for a clean look\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "# Add a grid to improve readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF \n",
    "plt.savefig(\"../../../IMAGES/Análisis General/GenAI/GenAI_Experiments in _Análisis General_ f1score.pdf\", format='pdf')\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig(\"../../../IMAGES/Análisis General/GenAI/GenAI_Experiments in _Análisis General_ f1score.png\", format='png', transparent=True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(len(labels), 1, figsize=(24, len(labels) * 4), sharex=True)\n",
    "\n",
    "# Plot each balance in a separate subplot\n",
    "for i, label in enumerate(labels):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Plot Weighted F1-Scores for each label\n",
    "    ax.plot(ageneral_df.index, \n",
    "            ageneral_df[f'{label}_f1-score'], \n",
    "            linewidth=4, linestyle='-', marker='o', markersize=12, color=colors[i], alpha=0.7)\n",
    "    \n",
    "    \n",
    "    for j in range(len(specific_indices) - 1):\n",
    "        # Calculate and plot trend line for each window\n",
    "        start_idx = specific_indices[j]\n",
    "        end_idx = specific_indices[j + 1]\n",
    "        \n",
    "        window_indices = ageneral_df.index[start_idx:end_idx+1]\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(window_indices, ageneral_df[f'{label}_f1-score'][start_idx:end_idx+1])\n",
    "        trend = intercept + slope * window_indices\n",
    "        ax.plot(window_indices, trend, linewidth=2, color=colors[i], linestyle='-', alpha=0.5) # label=f'Trend Line W{j} (R² = {r_value**2:.2f})'\n",
    "    \n",
    "    ax.set_title(f'F1-Score for \"{label}\"', fontsize=20)\n",
    "    ax.set_ylabel('F1-Score', fontsize=14)\n",
    "    ax.grid(True)\n",
    "    ax.legend(loc='lower right', fontsize=10)\n",
    "\n",
    "    # Add vertical lines for specific indices\n",
    "    for date, index in zip(specific_dates, specific_indices):\n",
    "        # if index in [160, 184]:\n",
    "        #     ax.axvline(x=index, color='#fd7b6e', linestyle='--', linewidth=2)\n",
    "        # else:\n",
    "            ax.axvline(x=index, color='#1a2e49', linestyle='--', linewidth=2)\n",
    "\n",
    "\n",
    "# Set common labels\n",
    "fig.text(0.5, 0.04, 'Experiments', ha='center', fontsize=18)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0.03, 0.03, 1, 0.97])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate the upper limit for the y-axis dynamically\n",
    "y_max = ageneral_df['weighted avg_f1-score'].mean() + 4 * ageneral_df['weighted avg_f1-score'].std()\n",
    "\n",
    "# Create a figure with subplots using GridSpec\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "gs = GridSpec(1, 1, figure=fig)\n",
    "\n",
    "# Plot for MODELS\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.barplot(ax=ax1, x='Context', y='weighted avg_f1-score', hue='Model', data=ageneral_df, palette='crest_r', alpha=0.7, ci=None)\n",
    "ax1.set_title('Weighted Avg. F1-Score for Different Contexts', fontsize=30)\n",
    "ax1.set_xlabel('', fontsize=18)\n",
    "ax1.set_ylabel('F1-Score', fontsize=18)\n",
    "ax1.tick_params(axis='x', labelsize=18)\n",
    "ax1.tick_params(axis='y', labelsize=18)\n",
    "ax1.set_ylim(0.5, y_max)\n",
    "\n",
    "# Move legend to the top\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=4, fontsize=14.75)\n",
    "\n",
    "\n",
    "# Adding grid for better readability\n",
    "ax1.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of balances\n",
    "contexts = ['context/context_0.txt', 'context/context_1.txt']\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(len(contexts), 1, figsize=(24, len(contexts) * 4), sharex=True)\n",
    "\n",
    "# Initialize a list to store all lines for the legend\n",
    "lines = []\n",
    "labels_legend = []\n",
    "\n",
    "\n",
    "\n",
    "# Plot each balance in a separate subplot\n",
    "for i, context in enumerate(contexts):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Plot Weighted F1-Scores\n",
    "    line1, = ax.plot(ageneral_df[ageneral_df['Context'] == context].index, \n",
    "            ageneral_df[ageneral_df['Context'] == context]['weighted avg_f1-score'], \n",
    "            linewidth=6, linestyle='-', marker='o', markersize=12, color='black', alpha=0.7, label=context)\n",
    "    \n",
    "    # Plot \"Comentario Positivo\" F1-Scores\n",
    "    line2, = ax.plot(ageneral_df[ageneral_df['Context'] == context].index, \n",
    "            ageneral_df[ageneral_df['Context'] == context]['Comentario Positivo_f1-score'], \n",
    "            linewidth=4, linestyle='-', marker='o', markersize=9, color=colors[0], alpha=0.7, label=context)\n",
    "    \n",
    "    # Plot \"Comentario Negativo\" F1-Scores\n",
    "    line3, =  ax.plot(ageneral_df[ageneral_df['Context'] == context].index, \n",
    "            ageneral_df[ageneral_df['Context'] == context]['Comentario Negativo_f1-score'], \n",
    "            linewidth=4, linestyle='-', marker='o', markersize=9, color=colors[1], alpha=0.7, label=context)\n",
    "    \n",
    "    ax.set_title(f'F1-Score for \"{context}\"', fontsize=20)\n",
    "    ax.set_ylabel('F1-Score', fontsize=14)\n",
    "    ax.set_ylim(0.6, 1)\n",
    "    ax.grid(True)\n",
    "\n",
    "# Append lines and labels only once\n",
    "lines.extend([line1, line2, line3])\n",
    "labels_legend.extend(['Weighted Avg. F1-Score', 'Comentario Positivo', 'Comentario Negativo'])\n",
    "\n",
    "# Set common labels\n",
    "fig.text(0.5, 0.02, 'Experiments', ha='center', fontsize=18)\n",
    "\n",
    "# Add a single legend for all subplots\n",
    "fig.legend(lines, labels_legend, loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=3, fontsize=14)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0.03, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate the upper limit for the y-axis dynamically\n",
    "y_max = ageneral_df['weighted avg_f1-score'].mean() + 4 * ageneral_df['weighted avg_f1-score'].std()\n",
    "\n",
    "# Create a figure with subplots using GridSpec\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "gs = GridSpec(1, 1, figure=fig)\n",
    "\n",
    "# Plot for MODELS\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.barplot(ax=ax1, x='Prompt', y='weighted avg_f1-score', hue='Model', data=ageneral_df, palette='crest_r', alpha=0.7, ci=None)\n",
    "ax1.set_title('Weighted Avg. F1-Score for Different Prompts', fontsize=30)\n",
    "ax1.set_xlabel('', fontsize=18)\n",
    "ax1.set_ylabel('F1-Score', fontsize=18)\n",
    "ax1.tick_params(axis='x', labelsize=18)\n",
    "ax1.tick_params(axis='y', labelsize=18)\n",
    "ax1.set_ylim(0.5, y_max)\n",
    "\n",
    "# Move legend to the top\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=4, fontsize=14.75)\n",
    "\n",
    "\n",
    "# Adding grid for better readability\n",
    "ax1.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of balances\n",
    "prompts = ['prompts/analisis_general/prompt_0_EN.txt', 'prompts/analisis_general/prompt_0_ES.txt', 'prompts/analisis_general/prompt_1_EN.txt', 'prompts/analisis_general/prompt_1_ES.txt']\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(len(prompts), 1, figsize=(24, len(prompts) * 4), sharex=True)\n",
    "\n",
    "# Initialize a list to store all lines for the legend\n",
    "lines = []\n",
    "labels_legend = []\n",
    "\n",
    "\n",
    "\n",
    "# Plot each balance in a separate subplot\n",
    "for i, prompt in enumerate(prompts):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Plot Weighted F1-Scores\n",
    "    line1, = ax.plot(ageneral_df[ageneral_df['Prompt'] == prompt].index, \n",
    "            ageneral_df[ageneral_df['Prompt'] == prompt]['weighted avg_f1-score'], \n",
    "            linewidth=6, linestyle='-', marker='o', markersize=12, color='black', alpha=0.7, label=prompt)\n",
    "    \n",
    "    # Plot \"Comentario Positivo\" F1-Scores\n",
    "    line2, = ax.plot(ageneral_df[ageneral_df['Prompt'] == prompt].index, \n",
    "            ageneral_df[ageneral_df['Prompt'] == prompt]['Comentario Positivo_f1-score'], \n",
    "            linewidth=4, linestyle='-', marker='o', markersize=9, color=colors[0], alpha=0.7, label=prompt)\n",
    "    \n",
    "    # Plot \"Comentario Negativo\" F1-Scores\n",
    "    line3, =  ax.plot(ageneral_df[ageneral_df['Prompt'] == prompt].index, \n",
    "            ageneral_df[ageneral_df['Prompt'] == prompt]['Comentario Negativo_f1-score'], \n",
    "            linewidth=4, linestyle='-', marker='o', markersize=9, color=colors[1], alpha=0.7, label=prompt)\n",
    "    \n",
    "    ax.set_title(f'F1-Score for \"{prompt}\"', fontsize=20)\n",
    "    ax.set_ylabel('F1-Score', fontsize=14)\n",
    "    ax.set_ylim(0.6, 1)\n",
    "    ax.grid(True)\n",
    "\n",
    "# Append lines and labels only once\n",
    "lines.extend([line1, line2, line3])\n",
    "labels_legend.extend(['Weighted Avg. F1-Score', 'Comentario Positivo', 'Comentario Negativo'])\n",
    "\n",
    "# Set common labels\n",
    "fig.text(0.5, 0.02, 'Experiments', ha='center', fontsize=18)\n",
    "\n",
    "# Add a single legend for all subplots\n",
    "fig.legend(lines, labels_legend, loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=3, fontsize=14)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0.03, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance (Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate the upper limit for the y-axis dynamically\n",
    "y_max = ageneral_df['Time (s)'].mean() + 4 * ageneral_df['Time (s)'].std()\n",
    "\n",
    "# Create a figure with subplots using GridSpec\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "gs = GridSpec(1, 1, figure=fig)\n",
    "\n",
    "# Plot for MODELS\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.barplot(ax=ax1, x='Prompt', y='Time (s)', hue='Model', data=ageneral_df, palette='crest_r', alpha=0.7, ci=None)\n",
    "ax1.set_title('Average Performance Time for Different Prompts', fontsize=30)\n",
    "ax1.set_xlabel('', fontsize=18)\n",
    "ax1.set_ylabel('Average Time (s)', fontsize=18)\n",
    "ax1.tick_params(axis='x', labelsize=18)\n",
    "ax1.tick_params(axis='y', labelsize=18)\n",
    "ax1.set_ylim(0, y_max)\n",
    "\n",
    "# Move legend to the top\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=4, fontsize=15)\n",
    "\n",
    "\n",
    "# Adding grid for better readability\n",
    "ax1.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate the upper limit for the y-axis dynamically\n",
    "y_max = ageneral_df['Time (s)'].mean() + 4 * ageneral_df['Time (s)'].std()\n",
    "\n",
    "# Create a figure with subplots using GridSpec\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "gs = GridSpec(1, 1, figure=fig)\n",
    "\n",
    "# Plot for MODELS\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.barplot(ax=ax1, x='Context', y='Time (s)', hue='Model', data=ageneral_df, palette='crest_r', alpha=0.7, ci=None)\n",
    "ax1.set_title('Average Performance Time for Different Contexts', fontsize=30)\n",
    "ax1.set_xlabel('', fontsize=18)\n",
    "ax1.set_ylabel('Average Time (s)', fontsize=18)\n",
    "ax1.tick_params(axis='x', labelsize=18)\n",
    "ax1.tick_params(axis='y', labelsize=18)\n",
    "ax1.set_ylim(0, y_max)\n",
    "\n",
    "# Move legend to the top\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=5, fontsize=15)\n",
    "\n",
    "\n",
    "# Adding grid for better readability\n",
    "ax1.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = ageneral_df[['weighted avg_f1-score', f'{labels[0]}_f1-score', f'{labels[1]}_f1-score', 'Accuracy_Global', 'Std_Global','Time (s)']].corr()\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a heatmap for the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "heatmap = sns.heatmap(correlation_matrix, annot=True, cmap='crest_r', fmt='.2f', linewidths=0.5, linecolor='black')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Correlation Matrix of Performance Metrics', fontsize=20)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confussion Matrix - Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Datos de la matriz de confusión para el modelo 'c1c0f5ad-38ae-4c8b-be38-c98019e67891'\n",
    "y_true = ['Comentario Positivo'] * 1034 + ['Comentario Negativo'] * 616\n",
    "y_pred = [\n",
    "    'Comentario Positivo'] * 1006 + ['Comentario Negativo'] * 28 + \\\n",
    "    ['Comentario Positivo'] * 277 + ['Comentario Negativo'] * 339\n",
    "\n",
    "\n",
    "# Crear la matriz de confusión\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "# Plotear la matriz de confusión\n",
    "plt.figure(figsize=(24, 22))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='crest', xticklabels=labels, \n",
    "            yticklabels=labels, annot_kws={\"size\": 52}, cbar=False)\n",
    "\n",
    "plt.xlabel('Predicted label', fontsize=30)\n",
    "plt.ylabel('True label', fontsize=30)\n",
    "plt.xticks(rotation=45, fontsize=50)\n",
    "plt.yticks(rotation=0, fontsize=50)\n",
    "# plt.title('Confusion Matrix for Best Model of GenAI Experiments')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"../../../IMAGES/Análisis General/GenAI/GenAI_Experiments in _Análisis General_ best_model.pdf\", format='pdf')\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig(\"../../../IMAGES/Análisis General/GenAI/GenAI_Experiments in _Análisis General_ best_model.png\", format='png', transparent=True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido Negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of labels and colors\n",
    "labels = ['Desprestigiar Víctima', 'Desprestigiar Acto', 'Insultos', 'Desprestigiar Deportista Autora']\n",
    "colors = ['#478CCF', '#36C2CE', '#77E4C8', '#4535C1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset for \"Contenido Negativo\"\n",
    "cnegativo_df = df[df[\"Type\"] == \"contenido_negativo\"]\n",
    "len(cnegativo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnegativo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to categorical: {'Desprestigiar Víctima': 0, 'Desprestigiar Acto': 1, 'Insultos': 2, 'Desprestigiar Deportista Autora': 3}\n",
    "mapping = {'Desprestigiar Víctima': 0, 'Desprestigiar Acto': 1, 'Insultos': 2, 'Desprestigiar Deportista Autora': 3}\n",
    "\n",
    "# Create a dictionary for renaming\n",
    "rename_dict = {}\n",
    "for key, value in mapping.items():\n",
    "    rename_dict[f'{value}_precision'] = f'{key}_precision'\n",
    "    rename_dict[f'{value}_recall'] = f'{key}_recall'\n",
    "    rename_dict[f'{value}_f1-score'] = f'{key}_f1-score'\n",
    "    rename_dict[f'{value}_support'] = f'{key}_support'\n",
    "\n",
    "# Rename the columns\n",
    "cnegativo_df.rename(columns=rename_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Timestamp' column to datetime if it's not already\n",
    "cnegativo_df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "# Sort the DataFrame by 'Timestamp' if needed\n",
    "cnegativo_df.sort_values('Timestamp', inplace=True)\n",
    "\n",
    "# Resetting the index to ensure it's sequential\n",
    "cnegativo_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric columns\n",
    "numeric_cols = ['Accuracy_Global', 'Std_Global', 'Time (s)', 'weighted avg_precision', 'weighted avg_recall', 'weighted avg_f1-score', f'{labels[0]}_f1-score', f'{labels[1]}_f1-score', f'{labels[2]}_f1-score', f'{labels[3]}_f1-score']\n",
    "\n",
    "# Agrupar el DataFrame por context\n",
    "df_grouped_context = cnegativo_df.groupby(['Context'])[numeric_cols].mean().reset_index()\n",
    "\n",
    "df_grouped_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric columns\n",
    "numeric_cols = ['Accuracy_Global', 'Std_Global', 'Time (s)', 'weighted avg_precision', 'weighted avg_recall', 'weighted avg_f1-score', f'{labels[0]}_f1-score', f'{labels[1]}_f1-score', f'{labels[2]}_f1-score', f'{labels[3]}_f1-score']\n",
    "\n",
    "# Agrupar el DataFrame por prompt\n",
    "df_grouped_prompt = cnegativo_df.groupby(['Prompt'])[numeric_cols].mean().reset_index()\n",
    "\n",
    "df_grouped_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric columns\n",
    "numeric_cols = ['Accuracy_Global', 'Std_Global', 'Time (s)', 'weighted avg_precision', 'weighted avg_recall', 'weighted avg_f1-score', f'{labels[0]}_f1-score', f'{labels[1]}_f1-score', f'{labels[2]}_f1-score', f'{labels[3]}_f1-score']\n",
    "\n",
    "# Agrupar el DataFrame por prompt\n",
    "df_grouped_prompt = cnegativo_df.groupby(['Context', 'Prompt'])[numeric_cols].mean().reset_index()\n",
    "\n",
    "df_grouped_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of specific dates for vertical lines\n",
    "specific_dates = ['2024-08-10 15:23:40', '2024-08-10 15:42:11', '2024-08-10 16:04:27', '2024-08-10 16:25:22', '2024-08-10 16:46:02', '2024-08-10 17:07:23', '2024-08-10 17:27:53', '2024-08-10 17:48:57', '2024-08-10 18:07:47']\n",
    "specific_dates = pd.to_datetime(specific_dates)  # Convert to datetime if not already\n",
    "\n",
    "# Find the indices of the specific dates in the DataFrame\n",
    "specific_indices = cnegativo_df[cnegativo_df['Timestamp'].isin(specific_dates)].index.tolist()\n",
    "specific_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 10))\n",
    "\n",
    "\n",
    "# Add a vertical line for best window\n",
    "for date, index in zip(specific_dates, specific_indices):\n",
    "    if index in [32]:\n",
    "        plt.axvline(x=index, color='#1a2e49', linestyle='-', linewidth=3)\n",
    "    else:\n",
    "        plt.axvline(x=index, color='#1a2e49', linestyle='--', linewidth=2)\n",
    "\n",
    "# Plotting the F1-Score as dots\n",
    "plt.plot(cnegativo_df.index, cnegativo_df['weighted avg_f1-score'], linewidth=6, linestyle='-', marker='o', markersize=16, color='black', alpha=0.7, label='Weighted Avg. F1-Score')\n",
    "\n",
    "\n",
    "# Calculate and plot trend line\n",
    "slope, intercept, r_value, p_value, std_err = linregress(cnegativo_df.index, cnegativo_df['weighted avg_f1-score'])\n",
    "trend = intercept + slope * cnegativo_df.index\n",
    "plt.plot(cnegativo_df.index, trend, linewidth=2, color='darkred', linestyle='-', alpha=0.5, label=f'Trend Line (R² = {r_value**2:.2f})')\n",
    "\n",
    "\n",
    "# Move legend to the top\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=2, fontsize=22)\n",
    "\n",
    "\n",
    "# Setting labels and title\n",
    "plt.title('Weighted Avg. F1-Score for \"Contenido Negativo\"', fontsize=42)\n",
    "plt.xlabel('Experiments', fontsize=30)\n",
    "plt.ylabel('F1-Score', fontsize=30)\n",
    "plt.gca().set_ylim(0.25, 0.40)\n",
    "plt.gca().tick_params(axis='x', labelsize=22)\n",
    "plt.gca().tick_params(axis='y', labelsize=22)\n",
    "\n",
    "# Add a grid to improve readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "# Remove spines for a clean look\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"../../../IMAGES/Contenido Negativo/GenAI/GenAI_Experiments in _ContenidoNegativo_ weighted.pdf\", format='pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 18))\n",
    "\n",
    "# Add a vertical line for best window\n",
    "for date, index in zip(specific_dates, specific_indices):\n",
    "    if index in [32]:\n",
    "        plt.axvline(x=index, color='#1a2e49', linestyle='-', linewidth=3)\n",
    "    else:\n",
    "        plt.axvline(x=index, color='#1a2e49', linestyle='--', linewidth=2)\n",
    "\n",
    "# Plotting the F1-Score as dots\n",
    "plt.plot(cnegativo_df.index, cnegativo_df['weighted avg_f1-score'], linewidth=9, linestyle='-', marker='o', markersize=18, color='black', alpha=0.7, label=\"Weighted Avg.\")\n",
    "for i, label in enumerate(labels):\n",
    "    plt.plot(cnegativo_df.index, cnegativo_df[f'{label}_f1-score'], linewidth=7, linestyle='-', marker='o', markersize=16, color=colors[i], alpha=0.7, label=label)\n",
    "\n",
    "\n",
    "# Calculate and plot trend line\n",
    "for i in range(len(labels)):\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(cnegativo_df.index, cnegativo_df[f'{labels[i]}_f1-score'])\n",
    "    trend = intercept + slope * cnegativo_df.index\n",
    "    plt.plot(cnegativo_df.index, trend, linewidth=2, color=colors[i], linestyle='-', alpha=0.5)\n",
    "\n",
    "# Move legend to the top\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=5, fontsize=22)\n",
    "\n",
    "\n",
    "# Setting labels and title\n",
    "plt.title('F1-Score per Class for \"Contenido Negativo\"', fontsize=42)\n",
    "plt.xlabel('Experiments', fontsize=30)\n",
    "plt.ylabel('F1-Score', fontsize=30)\n",
    "plt.gca().set_ylim(-0.01, 0.5)\n",
    "plt.gca().tick_params(axis='x', labelsize=22)\n",
    "plt.gca().tick_params(axis='y', labelsize=22)\n",
    "\n",
    "# Add a grid to improve readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "# Remove spines for a clean look\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"../../../IMAGES/Contenido Negativo/GenAI/GenAI_Experiments in _ContenidoNegativo_f1score.pdf\", format='pdf')\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig(\"../../../IMAGES/Contenido Negativo/GenAI/GenAI_Experiments in _ContenidoNegativo_f1score.png\", format='png', transparent=True)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance (Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate the upper limit for the y-axis dynamically\n",
    "y_max = cnegativo_df['Time (s)'].mean() + 4 * cnegativo_df['Time (s)'].std()\n",
    "\n",
    "# Create a figure with subplots using GridSpec\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "gs = GridSpec(1, 1, figure=fig)\n",
    "\n",
    "# Plot for MODELS\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.barplot(ax=ax1, x='Prompt', y='Time (s)', hue='Model', data=cnegativo_df, palette='Blues', alpha=0.7, ci=None)\n",
    "ax1.set_title('Average Performance Time for Different Prompts', fontsize=30)\n",
    "ax1.set_xlabel('', fontsize=18)\n",
    "ax1.set_ylabel('Average Time (s)', fontsize=18)\n",
    "ax1.tick_params(axis='x', labelsize=18)\n",
    "ax1.tick_params(axis='y', labelsize=18)\n",
    "ax1.set_ylim(0, y_max)\n",
    "\n",
    "# Move legend to the top\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=4, fontsize=15)\n",
    "\n",
    "\n",
    "# Adding grid for better readability\n",
    "ax1.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate the upper limit for the y-axis dynamically\n",
    "y_max = cnegativo_df['Time (s)'].mean() + 4 * cnegativo_df['Time (s)'].std()\n",
    "\n",
    "# Create a figure with subplots using GridSpec\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "gs = GridSpec(1, 1, figure=fig)\n",
    "\n",
    "# Plot for MODELS\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.barplot(ax=ax1, x='Context', y='Time (s)', hue='Model', data=cnegativo_df, palette='Blues', alpha=0.7, ci=None)\n",
    "ax1.set_title('Average Performance Time for Different Contexts', fontsize=30)\n",
    "ax1.set_xlabel('', fontsize=18)\n",
    "ax1.set_ylabel('Average Time (s)', fontsize=18)\n",
    "ax1.tick_params(axis='x', labelsize=18)\n",
    "ax1.tick_params(axis='y', labelsize=18)\n",
    "ax1.set_ylim(0, y_max)\n",
    "\n",
    "# Move legend to the top\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=5, fontsize=15)\n",
    "\n",
    "\n",
    "# Adding grid for better readability\n",
    "ax1.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = cnegativo_df[['weighted avg_f1-score', f'{labels[0]}_f1-score', f'{labels[1]}_f1-score', f'{labels[2]}_f1-score', 'Accuracy_Global', 'Std_Global','Time (s)']].corr()\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a heatmap for the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "heatmap = sns.heatmap(correlation_matrix, annot=True, cmap='Blues', fmt='.2f', linewidths=0.5, linecolor='black')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Correlation Matrix of Performance Metrics', fontsize=20)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confussion Matrix - Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Datos de la matriz de confusión para el modelo '1f8f1a43-c705-4c45-8894-7ed7f610917c'\n",
    "y_true = ['Desprestigiar Víctima'] * 171 + ['Desprestigiar Acto'] * 24 + ['Insultos'] * 144 + ['Desprestigiar Deportista Autora'] * 13\n",
    "y_pred = [\n",
    "    'Desprestigiar Víctima'] * 42 + ['Desprestigiar Acto'] * 41 + ['Insultos'] * 20 + ['Desprestigiar Deportista Autora'] * 68 + \\\n",
    "    ['Desprestigiar Víctima'] * 6 + ['Desprestigiar Acto'] * 9 + ['Insultos'] * 0 + ['Desprestigiar Deportista Autora'] * 9 + \\\n",
    "    ['Desprestigiar Víctima'] * 22 + ['Desprestigiar Acto'] * 20 + ['Insultos'] * 39 + ['Desprestigiar Deportista Autora'] * 63 + \\\n",
    "    ['Desprestigiar Víctima'] * 1 + ['Desprestigiar Acto'] * 3 + ['Insultos'] * 1 + ['Desprestigiar Deportista Autora'] * 8\n",
    "\n",
    "\n",
    "# Crear la matriz de confusión\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "# Plotear la matriz de confusión\n",
    "plt.figure(figsize=(32, 30))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, \n",
    "            yticklabels=labels, annot_kws={\"size\": 52}, cbar=False)\n",
    "\n",
    "plt.xlabel('Predicted label', fontsize=30)\n",
    "plt.ylabel('True label', fontsize=30)\n",
    "plt.xticks(rotation=45, fontsize=46)\n",
    "plt.yticks(rotation=0, fontsize=46)\n",
    "# plt.title('Confusion Matrix for Best Model of ML Experiments')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"../../../IMAGES/Contenido Negativo/GenAI/GenAI_Experiments in _ContenidoNegativo_ best_model.pdf\", format='pdf')\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig(\"../../../IMAGES/Contenido Negativo/GenAI/GenAI_Experiments in _ContenidoNegativo_ best_model.png\", format='png', transparent=True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insultos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of labels and colors\n",
    "labels = ['Sexistas/misóginos', 'Genéricos', 'Deseo de Dañar']\n",
    "colors = ['#FFAF45', '#FB6D48', '#D74B76']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset for \"Insultos\"\n",
    "insultos_df = df[df[\"Type\"] == \"insultos\"]\n",
    "len(insultos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to categorical: {'Sexistas/misóginos': 0, 'Genéricos': 1, 'Deseo de Dañar': 2}\n",
    "mapping = {'Sexistas/misóginos': 0, 'Genéricos': 1, 'Deseo de Dañar': 2}\n",
    "\n",
    "# Create a dictionary for renaming\n",
    "rename_dict = {}\n",
    "for key, value in mapping.items():\n",
    "    rename_dict[f'{value}_precision'] = f'{key}_precision'\n",
    "    rename_dict[f'{value}_recall'] = f'{key}_recall'\n",
    "    rename_dict[f'{value}_f1-score'] = f'{key}_f1-score'\n",
    "    rename_dict[f'{value}_support'] = f'{key}_support'\n",
    "\n",
    "# Rename the columns\n",
    "insultos_df.rename(columns=rename_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Timestamp' column to datetime if it's not already\n",
    "insultos_df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "# Sort the DataFrame by 'Timestamp' if needed\n",
    "insultos_df.sort_values('Timestamp', inplace=True)\n",
    "\n",
    "# Resetting the index to ensure it's sequential\n",
    "insultos_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric columns\n",
    "numeric_cols = ['Accuracy_Global', 'Std_Global', 'Time (s)', 'weighted avg_precision', 'weighted avg_recall', 'weighted avg_f1-score', f'{labels[0]}_f1-score', f'{labels[1]}_f1-score', f'{labels[2]}_f1-score']\n",
    "\n",
    "# Agrupar el DataFrame por context\n",
    "df_grouped_context = insultos_df.groupby(['Context'])[numeric_cols].mean().reset_index()\n",
    "\n",
    "df_grouped_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric columns\n",
    "numeric_cols = ['Accuracy_Global', 'Std_Global', 'Time (s)', 'weighted avg_precision', 'weighted avg_recall', 'weighted avg_f1-score', f'{labels[0]}_f1-score', f'{labels[1]}_f1-score', f'{labels[2]}_f1-score']\n",
    "\n",
    "# Agrupar el DataFrame por prompt\n",
    "df_grouped_prompt = insultos_df.groupby(['Prompt'])[numeric_cols].mean().reset_index()\n",
    "\n",
    "df_grouped_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric columns\n",
    "numeric_cols = ['Accuracy_Global', 'Std_Global', 'Time (s)', 'weighted avg_precision', 'weighted avg_recall', 'weighted avg_f1-score', f'{labels[0]}_f1-score', f'{labels[1]}_f1-score', f'{labels[2]}_f1-score']\n",
    "\n",
    "# Agrupar el DataFrame por prompt\n",
    "df_grouped_prompt = insultos_df.groupby(['Context', 'Prompt'])[numeric_cols].mean().reset_index()\n",
    "\n",
    "df_grouped_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of specific dates for vertical lines\n",
    "specific_dates = ['2024-08-10 22:17:29', '2024-08-10 22:28:59', '2024-08-10 22:41:58', '2024-08-10 22:54:20', '2024-08-10 23:06:20', '2024-08-10 23:19:04', '2024-08-10 23:30:54', '2024-08-10 23:43:35', '2024-08-10 23:54:36']\n",
    "specific_dates = pd.to_datetime(specific_dates)  # Convert to datetime if not already\n",
    "\n",
    "# Find the indices of the specific dates in the DataFrame\n",
    "specific_indices = insultos_df[insultos_df['Timestamp'].isin(specific_dates)].index.tolist()\n",
    "specific_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 10))\n",
    "\n",
    "\n",
    "# Add a vertical line for best window\n",
    "for date, index in zip(specific_dates, specific_indices):\n",
    "    if index in [32]:\n",
    "        plt.axvline(x=index, color='#1a2e49', linestyle='-', linewidth=3)\n",
    "    else:\n",
    "        plt.axvline(x=index, color='#1a2e49', linestyle='--', linewidth=2)\n",
    "\n",
    "# Plotting the F1-Score as dots\n",
    "plt.plot(insultos_df.index, insultos_df['weighted avg_f1-score'], linewidth=9, linestyle='-', marker='o', markersize=18, color='black', alpha=0.7, label='Weighted Avg. F1-Score')\n",
    "\n",
    "\n",
    "# Calculate and plot trend line\n",
    "slope, intercept, r_value, p_value, std_err = linregress(insultos_df.index, insultos_df['weighted avg_f1-score'])\n",
    "trend = intercept + slope * insultos_df.index\n",
    "plt.plot(insultos_df.index, trend, linewidth=2, color='darkred', linestyle='-', alpha=0.5, label=f'Trend Line (R² = {r_value**2:.2f})')\n",
    "\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=2, fontsize=22)\n",
    "\n",
    "\n",
    "# Setting labels and title\n",
    "plt.title('Weighted Avg. F1-Score for \"Insultos\"', fontsize=42)\n",
    "plt.xlabel('Experiments', fontsize=30)\n",
    "plt.ylabel('F1-Score', fontsize=30)\n",
    "plt.gca().set_ylim(0.3, 0.8)\n",
    "plt.gca().tick_params(axis='x', labelsize=22)\n",
    "plt.gca().tick_params(axis='y', labelsize=22)\n",
    "\n",
    "# Add a grid to improve readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "# Remove spines for a clean look\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"../../../IMAGES/Insultos/GenAI/GenAI_Experiments in _Insultos_ weighted.pdf\", format='pdf')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 18))\n",
    "\n",
    "# Add a vertical line for best window\n",
    "for date, index in zip(specific_dates, specific_indices):\n",
    "    if index in [32]:\n",
    "        plt.axvline(x=index, color='#1a2e49', linestyle='-', linewidth=3)\n",
    "    else:\n",
    "        plt.axvline(x=index, color='#1a2e49', linestyle='--', linewidth=2)\n",
    "\n",
    "# Plotting the F1-Score as dots\n",
    "plt.plot(insultos_df.index, insultos_df['weighted avg_f1-score'], linewidth=9, linestyle='-', marker='o', markersize=18, color='black', alpha=0.7, label=\"Weighted Avg.\")\n",
    "for i, label in enumerate(labels):\n",
    "    plt.plot(insultos_df.index, insultos_df[f'{label}_f1-score'], linewidth=7, linestyle='-', marker='o', markersize=16, color=colors[i], alpha=0.7, label=label)\n",
    "\n",
    "\n",
    "# Calculate and plot trend line\n",
    "for i in range(len(labels)):\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(insultos_df.index, insultos_df[f'{labels[i]}_f1-score'])\n",
    "    trend = intercept + slope * insultos_df.index\n",
    "    plt.plot(insultos_df.index, trend, linewidth=2, color=colors[i], linestyle='-', alpha=0.5)\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=5, fontsize=22)\n",
    "\n",
    "\n",
    "# Setting labels and title\n",
    "plt.title('F1-Score per Class for \"Insultos\"', fontsize=42)\n",
    "plt.xlabel('Experiments', fontsize=30)\n",
    "plt.ylabel('F1-Score', fontsize=30)\n",
    "plt.gca().set_ylim(0, 0.8)\n",
    "plt.gca().tick_params(axis='x', labelsize=22)\n",
    "plt.gca().tick_params(axis='y', labelsize=22)\n",
    "\n",
    "# Add a grid to improve readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "# Remove spines for a clean look\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"../../../IMAGES/Insultos/GenAI/GenAI_Experiments in _Insultos_f1score.pdf\", format='pdf')\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig(\"../../../IMAGES/Insultos/GenAI/GenAI_Experiments in _Insultos_f1score.png\", format='png', transparent=True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance (Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate the upper limit for the y-axis dynamically\n",
    "y_max = insultos_df['Time (s)'].mean() + 4 * insultos_df['Time (s)'].std()\n",
    "\n",
    "# Create a figure with subplots using GridSpec\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "gs = GridSpec(1, 1, figure=fig)\n",
    "\n",
    "# Plot for MODELS\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.barplot(ax=ax1, x='Prompt', y='Time (s)', hue='Model', data=insultos_df, palette='flare', alpha=0.7, ci=None)\n",
    "ax1.set_title('Average Performance Time for Different Prompts', fontsize=30)\n",
    "ax1.set_xlabel('', fontsize=18)\n",
    "ax1.set_ylabel('Average Time (s)', fontsize=18)\n",
    "ax1.tick_params(axis='x', labelsize=18)\n",
    "ax1.tick_params(axis='y', labelsize=18)\n",
    "ax1.set_ylim(0, y_max)\n",
    "\n",
    "# Move legend to the top\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=4, fontsize=15)\n",
    "\n",
    "\n",
    "# Adding grid for better readability\n",
    "ax1.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate the upper limit for the y-axis dynamically\n",
    "y_max = insultos_df['Time (s)'].mean() + 4 * insultos_df['Time (s)'].std()\n",
    "\n",
    "# Create a figure with subplots using GridSpec\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "gs = GridSpec(1, 1, figure=fig)\n",
    "\n",
    "# Plot for MODELS\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.barplot(ax=ax1, x='Context', y='Time (s)', hue='Model', data=insultos_df, palette='flare', alpha=0.7, ci=None)\n",
    "ax1.set_title('Average Performance Time for Different Contexts', fontsize=30)\n",
    "ax1.set_xlabel('', fontsize=18)\n",
    "ax1.set_ylabel('Average Time (s)', fontsize=18)\n",
    "ax1.tick_params(axis='x', labelsize=18)\n",
    "ax1.tick_params(axis='y', labelsize=18)\n",
    "ax1.set_ylim(0, y_max)\n",
    "\n",
    "# Move legend to the top\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=5, fontsize=15)\n",
    "\n",
    "\n",
    "# Adding grid for better readability\n",
    "ax1.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = insultos_df[['weighted avg_f1-score', f'{labels[0]}_f1-score', f'{labels[1]}_f1-score', f'{labels[2]}_f1-score', 'Accuracy_Global', 'Std_Global','Time (s)']].corr()\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a heatmap for the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "heatmap = sns.heatmap(correlation_matrix, annot=True, cmap='flare', fmt='.2f', linewidths=0.5, linecolor='black')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Correlation Matrix of Performance Metrics', fontsize=20)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confussion Matrix - Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Datos de la matriz de confusión para el modelo '50764c95-6e1e-4d74-8bfc-1100950944d0'\n",
    "y_true = ['Sexistas/misóginos'] * 12 + ['Genéricos'] * 189 + ['Deseo de Dañar'] * 5\n",
    "y_pred = [\n",
    "    'Sexistas/misóginos'] * 4 + ['Genéricos'] * 5 + ['Deseo de Dañar'] * 3 + \\\n",
    "    ['Sexistas/misóginos'] * 43 + ['Genéricos'] * 94 + ['Deseo de Dañar'] * 52 + \\\n",
    "    ['Sexistas/misóginos'] * 1 + ['Genéricos'] * 1 + ['Deseo de Dañar'] * 3\n",
    "\n",
    "\n",
    "# Crear la matriz de confusión\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "# Plotear la matriz de confusión\n",
    "plt.figure(figsize=(24, 22))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='flare', xticklabels=labels, \n",
    "            yticklabels=labels, annot_kws={\"size\": 52}, cbar=False)\n",
    "\n",
    "plt.xlabel('Predicted label', fontsize=30)\n",
    "plt.ylabel('True label', fontsize=30)\n",
    "plt.xticks(rotation=45, fontsize=46)\n",
    "plt.yticks(rotation=0, fontsize=46)\n",
    "# plt.title('Confusion Matrix for Best Model of ML Experiments')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"../../../IMAGES/Insultos/GenAI/GenAI_Experiments in _Insultos_ best_model.pdf\", format='pdf')\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig(\"../../../IMAGES/Insultos/GenAI/GenAI_Experiments in _Insultos_ best_model.png\", format='png', transparent=True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary for task names\n",
    "task_name_mapping = {\n",
    "    \"analisis_general\": \"Análisis General\", \n",
    "    \"contenido_negativo\": \"Contenido Negativo\", \n",
    "    \"insultos\": \"Insultos\"\n",
    "}\n",
    "\n",
    "# Agrupar el DataFrame `df` por 'Type' y calcular la suma, promedio y número de experimentos del 'Time (s)'\n",
    "df_grouped_sum = df.groupby(['Type'])['Time (s)'].sum().reset_index()\n",
    "df_grouped_mean = df.groupby(['Type'])['Time (s)'].mean().reset_index()\n",
    "df_grouped_count = df.groupby(['Type'])['Time (s)'].count().reset_index()\n",
    "\n",
    "# Fusionar las agrupaciones en un solo DataFrame\n",
    "df_grouped = df_grouped_sum.merge(df_grouped_mean, on='Type', suffixes=('_Total', '_Mean'))\n",
    "df_grouped = df_grouped.merge(df_grouped_count, on='Type')\n",
    "\n",
    "# Renombrar las columnas para mayor claridad\n",
    "df_grouped.columns = ['Task', 'Total Time (s)', 'Mean Time (s)', 'Number of Experiments']\n",
    "\n",
    "# Convert seconds to hours and round to 3 decimal places\n",
    "df_grouped['Total Time (h)'] = (df_grouped['Total Time (s)'] / 3600).round(3)\n",
    "df_grouped['Mean Time (h)'] = (df_grouped['Mean Time (s)'] / 3600).round(3)\n",
    "\n",
    "# Apply the mapping to change the task names\n",
    "df_grouped['Task'] = df_grouped['Task'].replace(task_name_mapping)\n",
    "\n",
    "# Drop the original time in seconds columns if not needed\n",
    "df_grouped = df_grouped[['Task', 'Total Time (h)', 'Mean Time (h)', 'Number of Experiments']]\n",
    "\n",
    "df_grouped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Maximum per limit for the y-axis between ML, DL and GenAI\n",
    "y_max = 750\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, ax = plt.subplots(figsize=(24, 8))\n",
    "\n",
    "# Plot for Total Time usando los colores personalizados\n",
    "sns.barplot(ax=ax, x='Task', y='Number of Experiments', data=df_grouped, palette='bone', alpha=0.9)\n",
    "\n",
    "# Set plot title and labels\n",
    "ax.set_title('Total Number of Experiments per Classification Task in GenAI Experiments', fontsize=30)\n",
    "ax.set_xlabel('', fontsize=18)\n",
    "ax.set_ylabel('Time (h)', fontsize=18)\n",
    "ax.tick_params(axis='x', labelsize=18)\n",
    "ax.tick_params(axis='y', labelsize=18)\n",
    "ax.set_ylim(0, y_max)\n",
    "\n",
    "\n",
    "# Adding grid for better readability\n",
    "ax.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Maximum per limit for the y-axis between ML, DL and GenAI\n",
    "y_max = 75\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, ax = plt.subplots(figsize=(24, 8))\n",
    "\n",
    "# Plot for Total Time usando los colores personalizados\n",
    "sns.barplot(ax=ax, x='Task', y='Total Time (h)', data=df_grouped, palette='bone', alpha=0.9)\n",
    "\n",
    "# Set plot title and labels\n",
    "ax.set_title('Total Performance Time per Classification Task in GenAI Experiments', fontsize=30)\n",
    "ax.set_xlabel('', fontsize=18)\n",
    "ax.set_ylabel('Time (h)', fontsize=18)\n",
    "ax.tick_params(axis='x', labelsize=18)\n",
    "ax.tick_params(axis='y', labelsize=18)\n",
    "ax.set_ylim(0, y_max)\n",
    "\n",
    "\n",
    "# Adding grid for better readability\n",
    "ax.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Maximum per limit for the y-axis between ML, DL and GenAI\n",
    "y_max = 75\n",
    "\n",
    "# Define colors for specific tasks\n",
    "task_colors = {\n",
    "    'Análisis General': '#8E809E',\n",
    "    'Contenido Negativo': '#6AA6D4',\n",
    "    'Insultos': '#E38A83'\n",
    "}\n",
    "\n",
    "# Create a list of colors for the bars based on the task\n",
    "bar_colors = [task_colors.get(task, '#CCCCCC') for task in df_grouped['Task']]  # Default color is light grey if not specified\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, ax = plt.subplots(figsize=(24, 8))\n",
    "\n",
    "# Plot for Total Time using the customized colors\n",
    "sns.barplot(ax=ax, x='Task', y='Total Time (h)', data=df_grouped, palette=bar_colors, alpha=0.9)\n",
    "\n",
    "# Set plot title and labels\n",
    "# ax.set_title('Total Performance Time per Classification Task in ML Experiments', fontsize=40)\n",
    "ax.set_xlabel('', fontsize=20)\n",
    "ax.set_ylabel('Time (h)', fontsize=30)\n",
    "ax.tick_params(axis='x', labelsize=34)\n",
    "ax.tick_params(axis='y', labelsize=34)\n",
    "ax.set_ylim(0, y_max)\n",
    "\n",
    "# Add value labels on top of the bars in the same color as the bars\n",
    "for bar, color in zip(ax.patches, bar_colors):\n",
    "    yval = bar.get_height()\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2, yval + 1, f'{yval:.3f}',\n",
    "        ha='center', va='bottom', fontsize=34, fontweight='bold',\n",
    "        color=color  # Match text color with the bar color\n",
    "    )\n",
    "\n",
    "# Add a grid to improve readability\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Remove spines for a clean look\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "# Adjust layout to avoid clipping of labels and titles\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"../../../IMAGES/GenAI_total_performance_time.pdf\", format='pdf')\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig(\"../../../IMAGES/GenAI_total_performance_time.png\", format='png', transparent=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Maximum per limit for the y-axis between ML, DL and GenAI\n",
    "y_max = 6\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, ax = plt.subplots(figsize=(24, 8))\n",
    "\n",
    "# Plot for Total Time usando los colores personalizados\n",
    "sns.barplot(ax=ax, x='Task', y='Mean Time (h)', data=df_grouped, palette='bone', alpha=0.9)\n",
    "\n",
    "# Set plot title and labels\n",
    "ax.set_title('Mean Performance Time per Classification Task in GenAI Experiments', fontsize=30)\n",
    "ax.set_xlabel('', fontsize=18)\n",
    "ax.set_ylabel('Time (h)', fontsize=18)\n",
    "ax.tick_params(axis='x', labelsize=18)\n",
    "ax.tick_params(axis='y', labelsize=18)\n",
    "ax.set_ylim(0, y_max)\n",
    "\n",
    "\n",
    "# Adding grid for better readability\n",
    "ax.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
