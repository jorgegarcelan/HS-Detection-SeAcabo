{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add context to models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, DistilBertTokenizer, BertModel, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, RobertaTokenizer, RobertaModel, XLMRobertaModel, AutoTokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import string\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data):\n",
    "    # Leer el archivo de datos\n",
    "    df = pd.read_csv(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(df, context, tweet_original):\n",
    "\n",
    "    # Preprocesado de datos\n",
    "    df['tweet_respuesta'] = df['full_text'].apply(lambda x: preprocess_tweet(x, lang=\"es\"))\n",
    "\n",
    "    # Añadir columnas para diferentes contextos:\n",
    "    df['Sin_contexto'] = df['full_text']\n",
    "    df['Tweet_context'] = tweet_original + \" [SEP] \" + df['tweet_respuesta']\n",
    "    df['Full_context'] = tweet_original + \" [SEP] \" + df['tweet_respuesta'] + \" [SEP] \" + context\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_type(df, label_column):\n",
    "\n",
    "    if label_column == \"Análisis General\":\n",
    "        # Define the specific labels to keep\n",
    "        #etiquetas = [\"Comentario Positivo\", \"Comentario Negativo\", \"Comentario Neutro\"]\n",
    "        etiquetas = [\"Comentario Positivo\", \"Comentario Negativo\"]\n",
    "        \n",
    "        df['Análisis General'] = df['Análisis General'].where(df['Análisis General'].isin(etiquetas))\n",
    "\n",
    "\n",
    "        # Remove NAs\n",
    "        df = df.dropna(subset=['Análisis General'])\n",
    "        \n",
    "\n",
    "        # Factorize the 'Análisis General' column\n",
    "        labels, labels_names = pd.factorize(df['Análisis General'])\n",
    "\n",
    "        # 'labels' now contains the numeric representation of your original labels\n",
    "        # 'label_names' contains the unique values from your original column in the order they were encoded\n",
    "\n",
    "        # Replace the original column with the numeric labels\n",
    "        df['Análisis General'] = labels\n",
    "\n",
    "        # If you want to keep a record of the mapping from the original labels to the numeric labels\n",
    "        label_mapping = dict(zip(labels_names, range(len(labels_names))))\n",
    "        #print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "    if label_column == \"Contenido Negativo\":\n",
    "\n",
    "        # Filtrar el DataFrame para seleccionar solo los \"Comentario Negativo\"\n",
    "        df = df.loc[df['Análisis General'] == 'Comentario Negativo']\n",
    "\n",
    "        # Define the specific labels to keep\n",
    "        etiquetas = [\"Desprestigiar Víctima\", \"Desprestigiar Acto\", \"Insultos\", \"Desprestigiar Deportista Autora\"]\n",
    "        df['Contenido Negativo'] = df['Contenido Negativo'].where(df['Contenido Negativo'].isin(etiquetas))\n",
    "\n",
    "        # Remove NAs\n",
    "        df = df.dropna(subset=['Contenido Negativo'])\n",
    "        \n",
    "\n",
    "        # Factorize the 'Análisis General' column\n",
    "        labels, labels_names = pd.factorize(df['Contenido Negativo'])\n",
    "\n",
    "        # 'labels' now contains the numeric representation of your original labels\n",
    "        # 'label_names' contains the unique values from your original column in the order they were encoded\n",
    "\n",
    "        # Replace the original column with the numeric labels\n",
    "        df['Contenido Negativo'] = labels\n",
    "\n",
    "        # If you want to keep a record of the mapping from the original labels to the numeric labels\n",
    "        label_mapping = dict(zip(labels_names, range(len(labels_names))))\n",
    "        #print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "\n",
    "    if label_column == \"Insultos\":\n",
    "\n",
    "        # Filtrar el DataFrame para seleccionar solo los \"Comentario Negativo\"\n",
    "        df = df.loc[df['Análisis General'] == 'Comentario Negativo']\n",
    "\n",
    "        # Define the specific labels to keep\n",
    "        etiquetas = [\"Deseo de Dañar\", \"Genéricos\", \"Sexistas/misóginos\", \"\"]\n",
    "\n",
    "        # Replace labels that are not in the list with \"Genéricos\"\n",
    "        df['Insultos'] = df['Insultos'].where(df['Insultos'].isin(etiquetas), other=\"Genéricos\")\n",
    "\n",
    "        # Remove NAs\n",
    "        df = df.dropna(subset=['Insultos'])\n",
    "        \n",
    "\n",
    "        # Factorize the 'Insultos' column\n",
    "        labels, labels_names = pd.factorize(df['Insultos'])\n",
    "\n",
    "        # 'labels' now contains the numeric representation of your original labels\n",
    "        # 'label_names' contains the unique values from your original column in the order they were encoded\n",
    "\n",
    "        # Replace the original column with the numeric labels\n",
    "        df['Insultos'] = labels\n",
    "\n",
    "        # If you want to keep a record of the mapping from the original labels to the numeric labels\n",
    "        label_mapping = dict(zip(labels_names, range(len(labels_names))))\n",
    "        #print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "    num_labels = len(etiquetas)\n",
    "\n",
    "\n",
    "\n",
    "    return df, labels_names, num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(name):\n",
    "    \n",
    "    if name == \"dccuchile/bert-base-spanish-wwm-cased\":\n",
    "        tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n",
    "\n",
    "    if name == \"PlanTL-GOB-ES/roberta-large-bne\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('PlanTL-GOB-ES/roberta-large-bne') \n",
    "\n",
    "    if name == \"bert-base-multilingual-cased\":\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "    if name == \"FacebookAI/xlm-roberta-base\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "\n",
    "    if name == \"pysentimiento/robertuito-base-cased\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained('pysentimiento/robertuito-base-cased')\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar inputs tokenizados para cada contexto\n",
    "def prepare_inputs(df, text_column, label_column, tokenizer):\n",
    "    \n",
    "    # Tokenización de los textos\n",
    "    tokenized_data = tokenizer(df[text_column].tolist(), padding=PADDING, truncation=TRUNCATION, max_length=MAX_LENGTH, return_tensors='pt')\n",
    "    \n",
    "    # Factorizar las etiquetas si son categóricas\n",
    "    labels, _ = pd.factorize(df[label_column])\n",
    "    \n",
    "    # Convertir las etiquetas a un tensor\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    # Retorna un diccionario con los inputs tokenizados y las etiquetas\n",
    "    return {**tokenized_data, 'labels': labels}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hf_dataset(tokenized_inputs):\n",
    "    return Dataset.from_dict(tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para dividir un dataset en train, validation y test\n",
    "def split_dataset(dataset, test_size=0.1, val_size=0.3):\n",
    "    # Dividir primero en train+val y test\n",
    "    train_val_dataset, test_dataset = dataset.train_test_split(test_size=test_size).values()\n",
    "\n",
    "    # Ahora dividir train+val en train y val\n",
    "    train_dataset, val_dataset = train_val_dataset.train_test_split(test_size=val_size / (1 - test_size)).values()\n",
    "\n",
    "    return DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'validation': val_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name, num_labels):\n",
    "    if name == \"dccuchile/bert-base-spanish-wwm-cased\":\n",
    "        model = BertForSequenceClassification.from_pretrained(name, num_labels=num_labels)\n",
    "\n",
    "    if name == \"PlanTL-GOB-ES/roberta-large-bne\":\n",
    "        model = RobertaModel.from_pretrained(name, num_labels=num_labels) \n",
    "\n",
    "    if name == \"bert-base-multilingual-cased\":\n",
    "        model = BertModel.from_pretrained(name, num_labels=num_labels)\n",
    "\n",
    "    if name == \"FacebookAI/xlm-roberta-base\":\n",
    "        model = XLMRobertaModel.from_pretrained(name, num_labels=num_labels)\n",
    "\n",
    "    if name == \"pysentimiento/robertuito-base-cased\":\n",
    "        model = AutoTokenizer.from_pretrained(name, num_labels=num_labels)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Obtener reporte completo\n",
    "    report = classification_report(labels, predictions, output_dict=True)\n",
    "    \n",
    "    # Obtener la matriz de confusión\n",
    "    conf_matrix = confusion_matrix(labels, predictions)\n",
    "    \n",
    "    # Extraer métricas para cada clase y globales\n",
    "    metrics = {\n",
    "        'accuracy': report['accuracy'],\n",
    "        'weighted_f1': report['weighted avg']['f1-score'],\n",
    "        'weighted_precision': report['weighted avg']['precision'],\n",
    "        'weighted_recall': report['weighted avg']['recall'],\n",
    "        # La matriz de confusión no se incluye normalmente como una métrica devuelta porque no es un escalar\n",
    "        'confusion_matrix': conf_matrix.tolist()  # Convertir a lista para asegurarse de que es serializable si es necesario\n",
    "    }\n",
    "    \n",
    "    # Añadir métricas específicas por clase si se requiere\n",
    "    for label, scores in report.items():\n",
    "        if label not in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "            metrics[f'{label}_precision'] = scores['precision']\n",
    "            metrics[f'{label}_recall'] = scores['recall']\n",
    "            metrics[f'{label}_f1'] = scores['f1-score']\n",
    "            metrics[f'{label}_support'] = scores['support']\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_args(context_type, num_train_epochs=3, per_device_train_batch_size=16, per_device_eval_batch_size=16, warmup_steps=500, weight_decay=0.01, logging_steps=10):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results/{context_type}',\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        warmup_steps=warmup_steps,\n",
    "        weight_decay=weight_decay,\n",
    "        logging_dir=f'./logs/{context_type}',\n",
    "        logging_steps=logging_steps,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True\n",
    "    )\n",
    "\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveResultsCallback(TrainerCallback):\n",
    "    def __init__(self, excel_path, training_args):\n",
    "        self.excel_path = excel_path\n",
    "        self.training_args = vars(training_args)  # Convertir a diccionario si es necesario\n",
    "        self.rows = []\n",
    "        self.initialized = False  # Para asegurarse de inicializar el archivo una vez\n",
    "\n",
    "    def on_train_begin(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        # Esta función se llama al comienzo del entrenamiento\n",
    "        if state.is_local_process_zero:\n",
    "            if not self.initialized:\n",
    "                # Inicializa el archivo Excel solo una vez\n",
    "                self.init_excel()\n",
    "                self.initialized = True\n",
    "\n",
    "    def init_excel(self):\n",
    "        # Inicializa el archivo Excel creando el archivo si no existe con encabezados\n",
    "        if not Path(self.excel_path).exists():\n",
    "            df_header = pd.DataFrame(columns=[\"epoch\", *self.training_args.keys()])\n",
    "            df_header.to_excel(self.excel_path, index=False)\n",
    "\n",
    "    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, metrics, **kwargs):\n",
    "        # Esta función se llama después de cada evaluación\n",
    "        if state.is_local_process_zero:\n",
    "            self.rows.append({**metrics, \"epoch\": state.epoch})\n",
    "\n",
    "    def save_to_excel(self):\n",
    "        # Guardar todas las métricas y parámetros acumulados en el archivo Excel\n",
    "        df = pd.DataFrame(self.rows)\n",
    "        with pd.ExcelWriter(self.excel_path, mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "            df.to_excel(writer, index=False, header=False, startrow=writer.sheets['Sheet1'].max_row)\n",
    "            print(f\"Data logged to {self.excel_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_data(excel_path, run_id, eval_results, training_args, additional_info):\n",
    "    # Cargar o crear un DataFrame para registrar los datos\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, engine='openpyxl')\n",
    "    except FileNotFoundError:\n",
    "        # Si el archivo no existe, inicializar un nuevo DataFrame\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    # Convertir el objeto de argumentos de entrenamiento a diccionario si es necesario\n",
    "    if not isinstance(training_args, dict):\n",
    "        training_args = vars(training_args)\n",
    "    \n",
    "    # Convertir los resultados de la evaluación a diccionario si es necesario\n",
    "    if not isinstance(eval_results, dict):\n",
    "        eval_results = vars(eval_results)\n",
    "\n",
    "    # Construir la fila de datos a agregar\n",
    "    data = {\n",
    "        **{'Run_ID': run_id},\n",
    "        **training_args,\n",
    "        **eval_results,\n",
    "        **additional_info\n",
    "    }\n",
    "    \n",
    "    # Asegurarse de que todos los valores sean serializables, convertir a string si es necesario\n",
    "    for key, value in data.items():\n",
    "        if not isinstance(value, (int, float, str)):\n",
    "            data[key] = str(value)\n",
    "\n",
    "    # Agregar la nueva fila al DataFrame\n",
    "    df = df._append(data, ignore_index=True)\n",
    "    \n",
    "    # Guardar el DataFrame actualizado en el archivo Excel\n",
    "    df.to_excel(excel_path, index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorge\\AppData\\Local\\Temp\\ipykernel_37172\\1167848114.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Insultos'] = df['Insultos'].where(df['Insultos'].isin(etiquetas), other=\"Genéricos\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "type_d='Sin_contexto'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3285, 'grad_norm': 5.798182964324951, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.45}\n",
      "{'loss': 1.2531, 'grad_norm': 6.038410186767578, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0631638765335083, 'eval_accuracy': 0.8181818181818182, 'eval_weighted_f1': 0.7363636363636363, 'eval_weighted_precision': 0.6694214876033059, 'eval_weighted_recall': 0.8181818181818182, 'eval_confusion_matrix': [[36, 0, 0], [4, 0, 0], [4, 0, 0]], 'eval_0_precision': 0.8181818181818182, 'eval_0_recall': 1.0, 'eval_0_f1': 0.9, 'eval_0_support': 36.0, 'eval_1_precision': 0.0, 'eval_1_recall': 0.0, 'eval_1_f1': 0.0, 'eval_1_support': 4.0, 'eval_2_precision': 0.0, 'eval_2_recall': 0.0, 'eval_2_f1': 0.0, 'eval_2_support': 4.0, 'eval_runtime': 6.0225, 'eval_samples_per_second': 7.306, 'eval_steps_per_second': 0.498, 'epoch': 1.0}\n",
      "{'loss': 1.0778, 'grad_norm': 7.290902137756348, 'learning_rate': 3e-06, 'epoch': 1.36}\n",
      "{'loss': 0.9426, 'grad_norm': 5.3352742195129395, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.677189290523529, 'eval_accuracy': 0.8181818181818182, 'eval_weighted_f1': 0.7363636363636363, 'eval_weighted_precision': 0.6694214876033059, 'eval_weighted_recall': 0.8181818181818182, 'eval_confusion_matrix': [[36, 0, 0], [4, 0, 0], [4, 0, 0]], 'eval_0_precision': 0.8181818181818182, 'eval_0_recall': 1.0, 'eval_0_f1': 0.9, 'eval_0_support': 36.0, 'eval_1_precision': 0.0, 'eval_1_recall': 0.0, 'eval_1_f1': 0.0, 'eval_1_support': 4.0, 'eval_2_precision': 0.0, 'eval_2_recall': 0.0, 'eval_2_f1': 0.0, 'eval_2_support': 4.0, 'eval_runtime': 5.2308, 'eval_samples_per_second': 8.412, 'eval_steps_per_second': 0.574, 'epoch': 2.0}\n",
      "{'loss': 0.9157, 'grad_norm': 4.083681583404541, 'learning_rate': 5e-06, 'epoch': 2.27}\n",
      "{'loss': 0.7642, 'grad_norm': 3.964444637298584, 'learning_rate': 6e-06, 'epoch': 2.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6270166635513306, 'eval_accuracy': 0.8181818181818182, 'eval_weighted_f1': 0.7363636363636363, 'eval_weighted_precision': 0.6694214876033059, 'eval_weighted_recall': 0.8181818181818182, 'eval_confusion_matrix': [[36, 0, 0], [4, 0, 0], [4, 0, 0]], 'eval_0_precision': 0.8181818181818182, 'eval_0_recall': 1.0, 'eval_0_f1': 0.9, 'eval_0_support': 36.0, 'eval_1_precision': 0.0, 'eval_1_recall': 0.0, 'eval_1_f1': 0.0, 'eval_1_support': 4.0, 'eval_2_precision': 0.0, 'eval_2_recall': 0.0, 'eval_2_f1': 0.0, 'eval_2_support': 4.0, 'eval_runtime': 5.3073, 'eval_samples_per_second': 8.29, 'eval_steps_per_second': 0.565, 'epoch': 3.0}\n",
      "{'train_runtime': 403.5704, 'train_samples_per_second': 2.579, 'train_steps_per_second': 0.164, 'train_loss': 1.0292651870033958, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7365050911903381, 'eval_accuracy': 0.75, 'eval_weighted_f1': 0.6428571428571428, 'eval_weighted_precision': 0.5625, 'eval_weighted_recall': 0.75, 'eval_confusion_matrix': [[33, 0, 0], [5, 0, 0], [6, 0, 0]], 'eval_0_precision': 0.75, 'eval_0_recall': 1.0, 'eval_0_f1': 0.8571428571428571, 'eval_0_support': 33.0, 'eval_1_precision': 0.0, 'eval_1_recall': 0.0, 'eval_1_f1': 0.0, 'eval_1_support': 5.0, 'eval_2_precision': 0.0, 'eval_2_recall': 0.0, 'eval_2_f1': 0.0, 'eval_2_support': 6.0, 'eval_runtime': 5.2111, 'eval_samples_per_second': 8.444, 'eval_steps_per_second': 0.576, 'epoch': 3.0}\n",
      "type_d='Tweet_context'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3127, 'grad_norm': 7.365809917449951, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.45}\n",
      "{'loss': 1.234, 'grad_norm': 5.032135486602783, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.18295156955719, 'eval_accuracy': 0.6136363636363636, 'eval_weighted_f1': 0.4667093469910371, 'eval_weighted_precision': 0.37654958677685957, 'eval_weighted_recall': 0.6136363636363636, 'eval_confusion_matrix': [[27, 0, 0], [7, 0, 0], [10, 0, 0]], 'eval_0_precision': 0.6136363636363636, 'eval_0_recall': 1.0, 'eval_0_f1': 0.7605633802816901, 'eval_0_support': 27.0, 'eval_1_precision': 0.0, 'eval_1_recall': 0.0, 'eval_1_f1': 0.0, 'eval_1_support': 7.0, 'eval_2_precision': 0.0, 'eval_2_recall': 0.0, 'eval_2_f1': 0.0, 'eval_2_support': 10.0, 'eval_runtime': 5.7226, 'eval_samples_per_second': 7.689, 'eval_steps_per_second': 0.524, 'epoch': 1.0}\n",
      "{'loss': 1.0713, 'grad_norm': 3.812204122543335, 'learning_rate': 3e-06, 'epoch': 1.36}\n",
      "{'loss': 0.8683, 'grad_norm': 3.422548770904541, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0576763153076172, 'eval_accuracy': 0.6136363636363636, 'eval_weighted_f1': 0.4667093469910371, 'eval_weighted_precision': 0.37654958677685957, 'eval_weighted_recall': 0.6136363636363636, 'eval_confusion_matrix': [[27, 0, 0], [7, 0, 0], [10, 0, 0]], 'eval_0_precision': 0.6136363636363636, 'eval_0_recall': 1.0, 'eval_0_f1': 0.7605633802816901, 'eval_0_support': 27.0, 'eval_1_precision': 0.0, 'eval_1_recall': 0.0, 'eval_1_f1': 0.0, 'eval_1_support': 7.0, 'eval_2_precision': 0.0, 'eval_2_recall': 0.0, 'eval_2_f1': 0.0, 'eval_2_support': 10.0, 'eval_runtime': 5.6965, 'eval_samples_per_second': 7.724, 'eval_steps_per_second': 0.527, 'epoch': 2.0}\n",
      "{'loss': 0.6941, 'grad_norm': 4.145359039306641, 'learning_rate': 5e-06, 'epoch': 2.27}\n",
      "{'loss': 0.9258, 'grad_norm': 7.036401271820068, 'learning_rate': 6e-06, 'epoch': 2.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9483511447906494, 'eval_accuracy': 0.6136363636363636, 'eval_weighted_f1': 0.4667093469910371, 'eval_weighted_precision': 0.37654958677685957, 'eval_weighted_recall': 0.6136363636363636, 'eval_confusion_matrix': [[27, 0, 0], [7, 0, 0], [10, 0, 0]], 'eval_0_precision': 0.6136363636363636, 'eval_0_recall': 1.0, 'eval_0_f1': 0.7605633802816901, 'eval_0_support': 27.0, 'eval_1_precision': 0.0, 'eval_1_recall': 0.0, 'eval_1_f1': 0.0, 'eval_1_support': 7.0, 'eval_2_precision': 0.0, 'eval_2_recall': 0.0, 'eval_2_f1': 0.0, 'eval_2_support': 10.0, 'eval_runtime': 7.9325, 'eval_samples_per_second': 5.547, 'eval_steps_per_second': 0.378, 'epoch': 3.0}\n",
      "{'train_runtime': 475.1041, 'train_samples_per_second': 2.191, 'train_steps_per_second': 0.139, 'train_loss': 1.0027097282987651, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6376959681510925, 'eval_accuracy': 0.8181818181818182, 'eval_weighted_f1': 0.7363636363636363, 'eval_weighted_precision': 0.6694214876033059, 'eval_weighted_recall': 0.8181818181818182, 'eval_confusion_matrix': [[36, 0, 0], [2, 0, 0], [6, 0, 0]], 'eval_0_precision': 0.8181818181818182, 'eval_0_recall': 1.0, 'eval_0_f1': 0.9, 'eval_0_support': 36.0, 'eval_1_precision': 0.0, 'eval_1_recall': 0.0, 'eval_1_f1': 0.0, 'eval_1_support': 2.0, 'eval_2_precision': 0.0, 'eval_2_recall': 0.0, 'eval_2_f1': 0.0, 'eval_2_support': 6.0, 'eval_runtime': 8.307, 'eval_samples_per_second': 5.297, 'eval_steps_per_second': 0.361, 'epoch': 3.0}\n",
      "type_d='Full_context'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3026, 'grad_norm': 8.927135467529297, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.45}\n",
      "{'loss': 1.1882, 'grad_norm': 5.688700199127197, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1201770305633545, 'eval_accuracy': 0.6136363636363636, 'eval_weighted_f1': 0.4667093469910371, 'eval_weighted_precision': 0.37654958677685957, 'eval_weighted_recall': 0.6136363636363636, 'eval_confusion_matrix': [[27, 0, 0], [7, 0, 0], [10, 0, 0]], 'eval_0_precision': 0.6136363636363636, 'eval_0_recall': 1.0, 'eval_0_f1': 0.7605633802816901, 'eval_0_support': 27.0, 'eval_1_precision': 0.0, 'eval_1_recall': 0.0, 'eval_1_f1': 0.0, 'eval_1_support': 7.0, 'eval_2_precision': 0.0, 'eval_2_recall': 0.0, 'eval_2_f1': 0.0, 'eval_2_support': 10.0, 'eval_runtime': 10.4988, 'eval_samples_per_second': 4.191, 'eval_steps_per_second': 0.286, 'epoch': 1.0}\n",
      "{'loss': 1.0014, 'grad_norm': 4.197778224945068, 'learning_rate': 3e-06, 'epoch': 1.36}\n",
      "{'loss': 0.8071, 'grad_norm': 4.340452194213867, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0657492876052856, 'eval_accuracy': 0.6136363636363636, 'eval_weighted_f1': 0.4667093469910371, 'eval_weighted_precision': 0.37654958677685957, 'eval_weighted_recall': 0.6136363636363636, 'eval_confusion_matrix': [[27, 0, 0], [7, 0, 0], [10, 0, 0]], 'eval_0_precision': 0.6136363636363636, 'eval_0_recall': 1.0, 'eval_0_f1': 0.7605633802816901, 'eval_0_support': 27.0, 'eval_1_precision': 0.0, 'eval_1_recall': 0.0, 'eval_1_f1': 0.0, 'eval_1_support': 7.0, 'eval_2_precision': 0.0, 'eval_2_recall': 0.0, 'eval_2_f1': 0.0, 'eval_2_support': 10.0, 'eval_runtime': 11.1375, 'eval_samples_per_second': 3.951, 'eval_steps_per_second': 0.269, 'epoch': 2.0}\n",
      "{'loss': 0.6774, 'grad_norm': 3.8984687328338623, 'learning_rate': 5e-06, 'epoch': 2.27}\n",
      "{'loss': 0.9218, 'grad_norm': 4.9572601318359375, 'learning_rate': 6e-06, 'epoch': 2.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.935492753982544, 'eval_accuracy': 0.6136363636363636, 'eval_weighted_f1': 0.4667093469910371, 'eval_weighted_precision': 0.37654958677685957, 'eval_weighted_recall': 0.6136363636363636, 'eval_confusion_matrix': [[27, 0, 0], [7, 0, 0], [10, 0, 0]], 'eval_0_precision': 0.6136363636363636, 'eval_0_recall': 1.0, 'eval_0_f1': 0.7605633802816901, 'eval_0_support': 27.0, 'eval_1_precision': 0.0, 'eval_1_recall': 0.0, 'eval_1_f1': 0.0, 'eval_1_support': 7.0, 'eval_2_precision': 0.0, 'eval_2_recall': 0.0, 'eval_2_f1': 0.0, 'eval_2_support': 10.0, 'eval_runtime': 10.6202, 'eval_samples_per_second': 4.143, 'eval_steps_per_second': 0.282, 'epoch': 3.0}\n",
      "{'train_runtime': 789.3887, 'train_samples_per_second': 1.319, 'train_steps_per_second': 0.084, 'train_loss': 0.971211881348581, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6613195538520813, 'eval_accuracy': 0.8181818181818182, 'eval_weighted_f1': 0.7363636363636363, 'eval_weighted_precision': 0.6694214876033059, 'eval_weighted_recall': 0.8181818181818182, 'eval_confusion_matrix': [[36, 0, 0], [2, 0, 0], [6, 0, 0]], 'eval_0_precision': 0.8181818181818182, 'eval_0_recall': 1.0, 'eval_0_f1': 0.9, 'eval_0_support': 36.0, 'eval_1_precision': 0.0, 'eval_1_recall': 0.0, 'eval_1_f1': 0.0, 'eval_1_support': 2.0, 'eval_2_precision': 0.0, 'eval_2_recall': 0.0, 'eval_2_f1': 0.0, 'eval_2_support': 6.0, 'eval_runtime': 10.4359, 'eval_samples_per_second': 4.216, 'eval_steps_per_second': 0.287, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "DATA = \"../data/BBDD_SeAcabo.csv\"\n",
    "\n",
    "# Target Column\n",
    "LABEL_COLUMN = 'Insultos' # [\"Análisis General\", \"Contenido Negativo\", \"Insultos\"]\n",
    "\n",
    "# Types Dataset\n",
    "TYPES_DATASET = [\"Sin_contexto\", \"Tweet_context\", \"Full_context\"]\n",
    "\n",
    "# Tweet original Alexia Putellas\n",
    "TWEET_ORIGINAL = \"Esto es inaceptable. Se acabó. Contigo compañera @Jennihermoso\"\n",
    "\n",
    "# Contexto\n",
    "CONTEXT = \"\"\"\n",
    "                En agosto de 2023, tras la victoria de la Selección femenina de fútbol de España en la Copa Mundial Femenina de Fútbol de 2023, durante la celebración en la entrega de las medallas y tras abrazar efusivamente a varias jugadoras, Luis Rubiales besó en los labios a la centrocampista Jennifer Hermoso mientras sujetaba su cabeza con las manos. Hermoso lo denunció ante la Fiscalía por acoso sexual, coacciones y agresión sexual. La Fiscalía presentó una demanda contra Rubiales ante la Audiencia Nacional en Madrid\n",
    "            \"\"\"\n",
    "\n",
    "# Model\n",
    "MODEL_NAME = 'dccuchile/bert-base-spanish-wwm-cased' #[\"dccuchile/bert-base-spanish-wwm-case\", \"PlanTL-GOB-ES/roberta-large-bne\", \"bert-base-multilingual-cased\", \"FacebookAI/xlm-roberta-base\", \"pysentimiento/robertuito-base-cased\"]\n",
    "\n",
    "# Hyperparameters\n",
    "PADDING = True\n",
    "TRUNCATION = True\n",
    "MAX_LENGTH = 512\n",
    "NUM_TRAIN_EPOCHS = 3\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 16\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 16\n",
    "WARMUP_STEPS = 500\n",
    "WEIGHT_DECAY = 0.01\n",
    "LOGGING_STEPS = 10\n",
    "\n",
    "# EXCEL\n",
    "EXCEL_PATH = 'DL'\n",
    "\n",
    "\n",
    "models = [MODEL_NAME]\n",
    "for model in models:\n",
    "\n",
    "    # Load data\n",
    "    df = load_data(data=DATA)\n",
    "\n",
    "    # Preprocess data\n",
    "    df = preprocess_function(df, context=CONTEXT, tweet_original=TWEET_ORIGINAL)\n",
    "\n",
    "    # Labels\n",
    "    df, labels_names, num_labels = filter_by_type(df, LABEL_COLUMN)\n",
    "    print(num_labels)\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = load_tokenizer(name=MODEL_NAME)\n",
    "\n",
    "    for type_d in TYPES_DATASET:\n",
    "        print(f\"{type_d=}\")\n",
    "        # Prepare input\n",
    "        inputs_dataset = prepare_inputs(df, text_column=type_d, label_column=LABEL_COLUMN, tokenizer=tokenizer)\n",
    "\n",
    "        # Create datasets\n",
    "        dataset = create_hf_dataset(inputs_dataset)\n",
    "\n",
    "        # Split dataset\n",
    "        dataset = split_dataset(dataset, test_size=0.2, val_size=0.1)\n",
    "        \n",
    "        # Load model\n",
    "        model = load_model(name=MODEL_NAME, num_labels=num_labels)\n",
    "\n",
    "        # Train\n",
    "        training_args = load_training_args(context_type=type_d, num_train_epochs=NUM_TRAIN_EPOCHS, per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE, per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE, warmup_steps=WARMUP_STEPS, weight_decay=WEIGHT_DECAY, logging_steps=LOGGING_STEPS)\n",
    "\n",
    "        save_results_callback = SaveResultsCallback(f\"{EXCEL_PATH}_Training_{LABEL_COLUMN}.xlsx\", training_args)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset['train'],\n",
    "            eval_dataset=dataset['validation'],\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[save_results_callback]\n",
    "        )\n",
    "        trainer.train()\n",
    "\n",
    "        # Eval\n",
    "        eval_results = trainer.evaluate(dataset['test'])\n",
    "        results_df = pd.DataFrame([eval_results])\n",
    "\n",
    "\n",
    "        # Información adicional que quieres registrar\n",
    "        additional_info = {\n",
    "            'Model_Description': MODEL_NAME,\n",
    "            'Data_File': DATA,\n",
    "            'Type': type_d\n",
    "        }\n",
    "\n",
    "        # Llamar a log_data para guardar los resultados y la configuración\n",
    "        log_data(f\"{EXCEL_PATH}_{LABEL_COLUMN}.xlsx\", str(uuid.uuid4()) , eval_results, training_args, additional_info)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Leer los datos de Excel\n",
    "df = pd.read_excel(\"path_to_excel.xlsx\")\n",
    "\n",
    "# Asegurarse de que las épocas sean numéricas (excluye la fila de parámetros si está presente)\n",
    "df = df[df['epoch'] != 'Params']\n",
    "df['epoch'] = pd.to_numeric(df['epoch'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar la precisión\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df['epoch'], df['accuracy'], label='Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy per Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Graficar el F1 score\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df['epoch'], df['f1_score'], label='F1 Score', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Model F1 Score per Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar múltiples métricas en el mismo gráfico\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df['epoch'], df['accuracy'], label='Accuracy')\n",
    "plt.plot(df['epoch'], df['f1_score'], label='F1 Score')\n",
    "plt.plot(df['epoch'], df['weighted_precision'], label='Weighted Precision')\n",
    "plt.plot(df['epoch'], df['weighted_recall'], label='Weighted Recall')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Metrics')\n",
    "plt.title('Model Performance Metrics per Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar el soporte para cada clase por época\n",
    "classes = [col for col in df.columns if col.endswith('_support')]\n",
    "for cls in classes:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(df['epoch'], df[cls], label=cls)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Support')\n",
    "    plt.title(f'Support for {cls} per Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sin Contexto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,  # Asumiendo que 'model' es tu modelo de transformers ya inicializado\n",
    "    args=training_args,\n",
    "    train_dataset=split_datasets_sin_contexto['train'],\n",
    "    eval_dataset=split_datasets_sin_contexto['validation'],\n",
    "    compute_metrics=compute_metrics  # Asumiendo que tienes una función para calcular métricas\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener resultados de evaluación\n",
    "results = trainer.evaluate(split_datasets_sin_contexto['test'])\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con Tweet Original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,  # Asumiendo que 'model' es tu modelo de transformers ya inicializado\n",
    "    args=training_args,\n",
    "    train_dataset=split_datasets_tweet_context['train'],\n",
    "    eval_dataset=split_datasets_tweet_context['validation'],\n",
    "    compute_metrics=compute_metrics  # Asumiendo que tienes una función para calcular métricas\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener resultados de evaluación\n",
    "results = trainer.evaluate(split_datasets_tweet_context['test'])\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con Tweet Original y Contexto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,  # Asumiendo que 'model' es tu modelo de transformers ya inicializado\n",
    "    args=training_args,\n",
    "    train_dataset=split_datasets_full_context['train'],\n",
    "    eval_dataset=split_datasets_full_context['validation'],\n",
    "    compute_metrics=compute_metrics  # Asumiendo que tienes una función para calcular métricas\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener resultados de evaluación\n",
    "results = trainer.evaluate(split_datasets_full_context['test'])\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probar Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cargar el Modelo y el Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Ruta al checkpoint\n",
    "checkpoint_path = \"./results/checkpoint-1000\"\n",
    "\n",
    "# Cargar el tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n",
    "\n",
    "# Cargar el modelo\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preparar el Texto para la Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prediction_input(text, tokenizer):\n",
    "    # Preprocesa el texto como lo hiciste antes de entrenar (por ejemplo, limpieza básica, truncar, etc.)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=MAX_LENGTH, truncation=TRUNCATION, padding=PADDING)\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Realizar la Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, tokenizer, model):\n",
    "    # Preparar el texto para el modelo\n",
    "    model_inputs = prepare_prediction_input(text, tokenizer)\n",
    "    \n",
    "    # Mover el modelo a CPU o GPU según esté configurado\n",
    "    model.eval()  # Poner el modelo en modo de evaluación\n",
    "    with torch.no_grad():  # No calcular gradientes\n",
    "        outputs = model(**model_inputs)\n",
    "    \n",
    "    # Obtener logits\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Convertir los logits a probabilidades (opcional)\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Obtener la clase predicha\n",
    "    predicted_class_index = probabilities.argmax().item()\n",
    "    \n",
    "    return predicted_class_index, probabilities.numpy()\n",
    "\n",
    "# Ejemplo de uso\n",
    "tweet = \"@Jennihermoso, ánimo campeona\"\n",
    "text = tweet_original + \" [SEP] \" + tweet + \" [SEP] \" + contexto\n",
    "print(text)\n",
    "predicted_class, probabilities = predict(text, tokenizer, model)\n",
    "print(f\"Clase predicha: {predicted_class}\")\n",
    "print(f\"Probabilidades: {probabilities}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
