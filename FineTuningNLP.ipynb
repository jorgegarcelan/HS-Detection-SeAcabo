{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importaci√≥n de librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.2 (tags/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# Importaci√≥n de librer√≠as\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer, DistilBertTokenizer, BertModel, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, RobertaTokenizer, RobertaModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import random\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from joblib import dump\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lectura y filtrado de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Soporte de etiquetas con nombres originales:\n",
      "Comentario Positivo: 2117\n",
      "Comentario Neutro: 120\n",
      "Comentario Negativo: 435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Autora</th>\n",
       "      <th>full_text</th>\n",
       "      <th>An√°lisis General</th>\n",
       "      <th>Contenido Negativo</th>\n",
       "      <th>Insultos</th>\n",
       "      <th>Insulto 1</th>\n",
       "      <th>Insulto 2</th>\n",
       "      <th>Insulto 3</th>\n",
       "      <th>Emisor</th>\n",
       "      <th>Contenido AV.</th>\n",
       "      <th>...</th>\n",
       "      <th>profile_url</th>\n",
       "      <th>avatar_url</th>\n",
       "      <th>verified</th>\n",
       "      <th>is_blue_verified</th>\n",
       "      <th>view_count_scaled</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>num_adjectives</th>\n",
       "      <th>label</th>\n",
       "      <th>full_text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jenni Hermoso</td>\n",
       "      <td>@Jennihermoso TODA ESPA√ëA EST√Å CONTIGO https:/...</td>\n",
       "      <td>Comentario Positivo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.twitter.com/Araujismoo</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/167886449...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>14.236127</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>toda espa√±a contigo httpstcocgw7wwfpzi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jenni Hermoso</td>\n",
       "      <td>@Jennihermoso Espero que te llegue todo el arr...</td>\n",
       "      <td>Comentario Positivo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.twitter.com/ionebelarra</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/160626561...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>12.886237</td>\n",
       "      <td>1</td>\n",
       "      <td>113</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>espero llegue arrope queremos transmitirte adm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jenni Hermoso</td>\n",
       "      <td>@Jennihermoso No est√°s solaüíú</td>\n",
       "      <td>Comentario Positivo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.twitter.com/carlagaleote</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/161793790...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>12.438174</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>no solaüíú</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jenni Hermoso</td>\n",
       "      <td>@Araujismoo @Jennihermoso She‚Äôs a legend .</td>\n",
       "      <td>Comentario Positivo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.twitter.com/emmaltrix</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/169070741...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>7.259182</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>she ‚Äô s legend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jenni Hermoso</td>\n",
       "      <td>@Jennihermoso Dilo, reina https://t.co/GasZHE70bE</td>\n",
       "      <td>Comentario Positivo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.twitter.com/TirodeGraciah</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/163890386...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>7.113684</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dilo reina httpstcogaszhe70be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2667</th>\n",
       "      <td>Eva Navarro</td>\n",
       "      <td>@evaaanavarro @Paulaa_311 @Jennihermoso üíúüí™üèΩ</td>\n",
       "      <td>Comentario Positivo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.twitter.com/sandrus260</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/166507079...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.254662</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>üíúüí™üèΩ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>Eva Navarro</td>\n",
       "      <td>@evaaanavarro @Jennihermoso Z https://t.co/9vH...</td>\n",
       "      <td>Comentario Positivo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.twitter.com/Giorgio7716</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/164212369...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.265724</td>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>z httpstco9vh1vggz1h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2669</th>\n",
       "      <td>Eva Navarro</td>\n",
       "      <td>@evaaanavarro @Jennihermoso Doblemente campeon...</td>\n",
       "      <td>Comentario Positivo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.twitter.com/framarub</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/125216679...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.273240</td>\n",
       "      <td>2</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>doblemente campeona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2670</th>\n",
       "      <td>Eva Navarro</td>\n",
       "      <td>@evaaanavarro @Jennihermoso Brava!! Siempre co...</td>\n",
       "      <td>Comentario Positivo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.twitter.com/Joannacolomer</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/140020219...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.279278</td>\n",
       "      <td>2</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>brava siempre campeonas üèÜüèÖüíú</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>Eva Navarro</td>\n",
       "      <td>@evaaanavarro @Jennihermoso Y despu√©s de ver e...</td>\n",
       "      <td>Comentario Negativo</td>\n",
       "      <td>Desprestigiar V√≠ctima</td>\n",
       "      <td>Deseo de Da√±ar</td>\n",
       "      <td>Pat√©ticas</td>\n",
       "      <td>Mentirosa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fake</td>\n",
       "      <td>Si</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.twitter.com/ElJovenRicoOf</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/147480715...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.279024</td>\n",
       "      <td>2</td>\n",
       "      <td>131</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>despu√©s ver ¬øte reafirmas mentira pat√©ticas ht...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640 rows √ó 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Autora                                          full_text  \\\n",
       "0     Jenni Hermoso  @Jennihermoso TODA ESPA√ëA EST√Å CONTIGO https:/...   \n",
       "1     Jenni Hermoso  @Jennihermoso Espero que te llegue todo el arr...   \n",
       "2     Jenni Hermoso                       @Jennihermoso No est√°s solaüíú   \n",
       "3     Jenni Hermoso         @Araujismoo @Jennihermoso She‚Äôs a legend .   \n",
       "4     Jenni Hermoso  @Jennihermoso Dilo, reina https://t.co/GasZHE70bE   \n",
       "...             ...                                                ...   \n",
       "2667    Eva Navarro        @evaaanavarro @Paulaa_311 @Jennihermoso üíúüí™üèΩ   \n",
       "2668    Eva Navarro  @evaaanavarro @Jennihermoso Z https://t.co/9vH...   \n",
       "2669    Eva Navarro  @evaaanavarro @Jennihermoso Doblemente campeon...   \n",
       "2670    Eva Navarro  @evaaanavarro @Jennihermoso Brava!! Siempre co...   \n",
       "2671    Eva Navarro  @evaaanavarro @Jennihermoso Y despu√©s de ver e...   \n",
       "\n",
       "         An√°lisis General     Contenido Negativo        Insultos  Insulto 1  \\\n",
       "0     Comentario Positivo                    NaN             NaN        NaN   \n",
       "1     Comentario Positivo                    NaN             NaN        NaN   \n",
       "2     Comentario Positivo                    NaN             NaN        NaN   \n",
       "3     Comentario Positivo                    NaN             NaN        NaN   \n",
       "4     Comentario Positivo                    NaN             NaN        NaN   \n",
       "...                   ...                    ...             ...        ...   \n",
       "2667  Comentario Positivo                    NaN             NaN        NaN   \n",
       "2668  Comentario Positivo                    NaN             NaN        NaN   \n",
       "2669  Comentario Positivo                    NaN             NaN        NaN   \n",
       "2670  Comentario Positivo                    NaN             NaN        NaN   \n",
       "2671  Comentario Negativo  Desprestigiar V√≠ctima  Deseo de Da√±ar  Pat√©ticas   \n",
       "\n",
       "      Insulto 2 Insulto 3 Emisor Contenido AV.  ...  \\\n",
       "0           NaN       NaN    NaN           NaN  ...   \n",
       "1           NaN       NaN    NaN           NaN  ...   \n",
       "2           NaN       NaN    NaN           NaN  ...   \n",
       "3           NaN       NaN    NaN           NaN  ...   \n",
       "4           NaN       NaN    NaN           NaN  ...   \n",
       "...         ...       ...    ...           ...  ...   \n",
       "2667        NaN       NaN    NaN           NaN  ...   \n",
       "2668        NaN       NaN    NaN           NaN  ...   \n",
       "2669        NaN       NaN    NaN           NaN  ...   \n",
       "2670        NaN       NaN    NaN           NaN  ...   \n",
       "2671  Mentirosa       NaN   Fake            Si  ...   \n",
       "\n",
       "                                profile_url  \\\n",
       "0        https://www.twitter.com/Araujismoo   \n",
       "1       https://www.twitter.com/ionebelarra   \n",
       "2      https://www.twitter.com/carlagaleote   \n",
       "3         https://www.twitter.com/emmaltrix   \n",
       "4     https://www.twitter.com/TirodeGraciah   \n",
       "...                                     ...   \n",
       "2667     https://www.twitter.com/sandrus260   \n",
       "2668    https://www.twitter.com/Giorgio7716   \n",
       "2669       https://www.twitter.com/framarub   \n",
       "2670  https://www.twitter.com/Joannacolomer   \n",
       "2671  https://www.twitter.com/ElJovenRicoOf   \n",
       "\n",
       "                                             avatar_url  verified  \\\n",
       "0     https://pbs.twimg.com/profile_images/167886449...     False   \n",
       "1     https://pbs.twimg.com/profile_images/160626561...     False   \n",
       "2     https://pbs.twimg.com/profile_images/161793790...     False   \n",
       "3     https://pbs.twimg.com/profile_images/169070741...     False   \n",
       "4     https://pbs.twimg.com/profile_images/163890386...     False   \n",
       "...                                                 ...       ...   \n",
       "2667  https://pbs.twimg.com/profile_images/166507079...     False   \n",
       "2668  https://pbs.twimg.com/profile_images/164212369...     False   \n",
       "2669  https://pbs.twimg.com/profile_images/125216679...     False   \n",
       "2670  https://pbs.twimg.com/profile_images/140020219...     False   \n",
       "2671  https://pbs.twimg.com/profile_images/147480715...     False   \n",
       "\n",
       "      is_blue_verified  view_count_scaled  mention_count  tweet_length  \\\n",
       "0                 True          14.236127              1            62   \n",
       "1                False          12.886237              1           113   \n",
       "2                False          12.438174              1            28   \n",
       "3                 True           7.259182              2            42   \n",
       "4                 True           7.113684              1            49   \n",
       "...                ...                ...            ...           ...   \n",
       "2667             False          -0.254662              3            43   \n",
       "2668             False          -0.265724              2            53   \n",
       "2669             False          -0.273240              2            73   \n",
       "2670             False          -0.279278              2            72   \n",
       "2671             False          -0.279024              2           131   \n",
       "\n",
       "      num_adjectives label                                full_text_processed  \n",
       "0                  1     0             toda espa√±a contigo httpstcocgw7wwfpzi  \n",
       "1                  3     0  espero llegue arrope queremos transmitirte adm...  \n",
       "2                  1     0                                           no solaüíú  \n",
       "3                  1     0                                     she ‚Äô s legend  \n",
       "4                  1     0                      dilo reina httpstcogaszhe70be  \n",
       "...              ...   ...                                                ...  \n",
       "2667               1     0                                                üíúüí™üèΩ  \n",
       "2668               1     0                               z httpstco9vh1vggz1h  \n",
       "2669               1     0                                doblemente campeona  \n",
       "2670               1     0                        brava siempre campeonas üèÜüèÖüíú  \n",
       "2671               1     2  despu√©s ver ¬øte reafirmas mentira pat√©ticas ht...  \n",
       "\n",
       "[2640 rows x 52 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data():\n",
    "    # Leer el archivo y realizar transformaciones en los datos\n",
    "    df = pd.read_csv(\"data/BBDD_SeAcabo.csv\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def process_data(df, type_id):\n",
    "\n",
    "    # Normalize \"view_count\"\n",
    "    scaler = StandardScaler()\n",
    "    df['view_count_scaled'] = scaler.fit_transform(df[['view_count']])\n",
    "\n",
    "    # User Mentions\n",
    "    def count_user_mentions(mentions):\n",
    "        if pd.isna(mentions) or mentions == \"\":\n",
    "            return 0\n",
    "        else:\n",
    "            return len(mentions.split(';'))\n",
    "        \n",
    "    df['mention_count'] = df['user_mentions'].apply(count_user_mentions)\n",
    "\n",
    "    # Length Tweet\n",
    "    df['tweet_length'] = df['full_text'].str.len()\n",
    "\n",
    "    # Num Adjetives\n",
    "    def count_adjectives(text):\n",
    "        words = word_tokenize(text)\n",
    "        pos_tags = pos_tag(words)\n",
    "        return sum(1 for word, tag in pos_tags if tag.startswith('JJ'))\n",
    "    df['num_adjectives'] = df['full_text'].apply(count_adjectives)\n",
    "\n",
    "    #[\"analisis_general\", \"contenido_negativo\", \"insultos\"]\n",
    "    if type_id == \"analisis_general\":\n",
    "        # Define the specific labels to keep\n",
    "        etiquetas = [\"Comentario Positivo\", \"Comentario Negativo\", \"Comentario Neutro\"]\n",
    "\n",
    "\n",
    "        # Remove NAs\n",
    "        df = df.dropna(subset=['An√°lisis General'])\n",
    "        \n",
    "\n",
    "        # Factorize the 'An√°lisis General' column\n",
    "        labels, labels_names = pd.factorize(df['An√°lisis General'])\n",
    "\n",
    "        # 'labels' now contains the numeric representation of your original labels\n",
    "        # 'label_names' contains the unique values from your original column in the order they were encoded\n",
    "\n",
    "        # Replace the original column with the numeric labels\n",
    "        df['label'] = labels\n",
    "\n",
    "        # If you want to keep a record of the mapping from the original labels to the numeric labels\n",
    "        label_mapping = dict(zip(labels_names, range(len(labels_names))))\n",
    "        #print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "        n_labels = 3\n",
    "        label_dict = {'Comentario Positivo': 0, 'Comentario Negativo': 1, 'Comentario Neutro': 2}\n",
    "\n",
    "    if type_id == \"contenido_negativo\":\n",
    "\n",
    "        # Filtrar el DataFrame para seleccionar solo los \"Comentario Negativo\"\n",
    "        df = df.loc[df['An√°lisis General'] == 'Comentario Negativo']\n",
    "\n",
    "        # Define the specific labels to keep\n",
    "        etiquetas = [\"Desprestigiar V√≠ctima\", \"Desprestigiar Acto\", \"Insultos\", \"Desprestigiar Deportista Autora\", \"Sexualizaci√≥n / Objetivizaci√≥n\", \"Estereotipos de G√©nero\"]\n",
    "        df['Contenido Negativo'] = df['Contenido Negativo'].where(df['Contenido Negativo'].isin(etiquetas))\n",
    "\n",
    "        # Remove NAs\n",
    "        df = df.dropna(subset=['Contenido Negativo'])\n",
    "        \n",
    "\n",
    "        # Factorize the 'An√°lisis General' column\n",
    "        labels, labels_names = pd.factorize(df['Contenido Negativo'])\n",
    "\n",
    "        # 'labels' now contains the numeric representation of your original labels\n",
    "        # 'label_names' contains the unique values from your original column in the order they were encoded\n",
    "\n",
    "        # Replace the original column with the numeric labels\n",
    "        df['label'] = labels\n",
    "\n",
    "        # If you want to keep a record of the mapping from the original labels to the numeric labels\n",
    "        label_mapping = dict(zip(labels_names, range(len(labels_names))))\n",
    "        #print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "        n_labels = 6\n",
    "        label_dict = {'Desprestigiar Acto': 0, 'Desprestigiar Deportista Autora': 1, 'Desprestigiar V√≠ctima': 2, 'Insultos': 3}\n",
    "\n",
    "\n",
    "    if type_id == \"insultos\":\n",
    "\n",
    "        # Filtrar el DataFrame para seleccionar solo los \"Comentario Negativo\"\n",
    "        df = df.loc[df['An√°lisis General'] == 'Comentario Negativo']\n",
    "\n",
    "        # Define the specific labels to keep\n",
    "        etiquetas = [\"Deseo de Da√±ar\", \"Gen√©ricos\", \"Sexistas/mis√≥ginos\", \"\"]\n",
    "\n",
    "        # Replace labels that are not in the list with \"Gen√©ricos\"\n",
    "        df['Insultos'] = df['Insultos'].where(df['Insultos'].isin(etiquetas), other=\"Gen√©ricos\")\n",
    "\n",
    "        # Remove NAs\n",
    "        df = df.dropna(subset=['Insultos'])\n",
    "        \n",
    "\n",
    "        # Factorize the 'Insultos' column\n",
    "        labels, labels_names = pd.factorize(df['Insultos'])\n",
    "\n",
    "        # 'labels' now contains the numeric representation of your original labels\n",
    "        # 'label_names' contains the unique values from your original column in the order they were encoded\n",
    "\n",
    "        # Replace the original column with the numeric labels\n",
    "        df['label'] = labels\n",
    "\n",
    "        # If you want to keep a record of the mapping from the original labels to the numeric labels\n",
    "        label_mapping = dict(zip(labels_names, range(len(labels_names))))\n",
    "        #print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "        n_labels = 3\n",
    "        label_dict = {'Gen√©ricos': 0, 'Sexistas/mis√≥ginos': 1, 'Deseo de Da√±ar': 2}\n",
    "\n",
    "\n",
    "    # Contar el soporte de cada etiqueta\n",
    "    soporte_etiquetas = df['label'].value_counts()\n",
    "\n",
    "    # Imprimir el soporte para cada etiqueta\n",
    "    print(\"\\nSoporte de etiquetas con nombres originales:\")\n",
    "    for nombre_etiqueta, codigo in label_mapping.items():\n",
    "        print(f\"{nombre_etiqueta}: {soporte_etiquetas[codigo]}\")\n",
    "    \n",
    "    # Initialize stemmer\n",
    "    ##stemmer = SnowballStemmer('spanish')\n",
    "    \n",
    "    # Define function to remove stopwords, punctuation, and apply stemming\n",
    "    def remove_spanish_stopwords(text):\n",
    "\n",
    "        # Eliminar menciones a usuarios (palabras que comienzan con @)\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "        # Eliminar enlaces (todo lo que comienza con http o https)\n",
    "        #text = re.sub(r'http\\S+|https\\S+', '', text)\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = ''.join([char for char in text if char not in string.punctuation])\n",
    "        \n",
    "        # Remove stopwords\n",
    "        spanish_stopwords = set(stopwords.words('spanish'))\n",
    "        spanish_stopwords.remove(\"no\")  # Retain \"no\" as it provides negative context\n",
    "        words = word_tokenize(text)\n",
    "        filtered_words = [word.lower() for word in words if word.lower() not in spanish_stopwords]\n",
    "        \n",
    "        # Apply stemming (MIRAR)\n",
    "        ##stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "        \n",
    "        ##return ' '.join(stemmed_words)  \n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    df['full_text_processed'] = df['full_text'].apply(remove_spanish_stopwords)\n",
    "    # Eliminar filas donde 'full_text_processed' es una cadena vac√≠a\n",
    "    df = df[df['full_text_processed'] != \"\"]\n",
    "\n",
    "\n",
    "    return df, labels_names, n_labels, label_dict\n",
    "\n",
    "type_id = \"analisis_general\" #[\"analisis_general\", \"contenido_negativo\", \"insultos\"]\n",
    "df = load_data()\n",
    "df, labels_names, n_labels, label_dict = process_data(df, type_id)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Definici√≥n del modelo y entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at PlanTL-GOB-ES/roberta-large-bne and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizaci√≥n y codificaci√≥n de los datos\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "#model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "#model = BertModel.from_pretrained(\"bert-large-cased\")\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased') #beto\n",
    "#model = BertModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\") #beto\n",
    "\n",
    "#tokenizer = RobertaTokenizer.from_pretrained('PlanTL-GOB-ES/roberta-base-bne') \n",
    "#model = RobertaModel.from_pretrained(\"PlanTL-GOB-ES/roberta-base-bne\") \n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('PlanTL-GOB-ES/roberta-large-bne') \n",
    "model = RobertaModel.from_pretrained(\"PlanTL-GOB-ES/roberta-large-bne\") \n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "#model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorge\\AppData\\Local\\Temp\\ipykernel_20652\\2410103966.py:13: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'train' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[X_train.index, 'data_type'] = 'train'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.self.query.weight\n",
      "encoder.layer.0.attention.self.query.bias\n",
      "encoder.layer.0.attention.self.key.weight\n",
      "encoder.layer.0.attention.self.key.bias\n",
      "encoder.layer.0.attention.self.value.weight\n",
      "encoder.layer.0.attention.self.value.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.layer.1.attention.self.query.weight\n",
      "encoder.layer.1.attention.self.query.bias\n",
      "encoder.layer.1.attention.self.key.weight\n",
      "encoder.layer.1.attention.self.key.bias\n",
      "encoder.layer.1.attention.self.value.weight\n",
      "encoder.layer.1.attention.self.value.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.layer.2.attention.self.query.weight\n",
      "encoder.layer.2.attention.self.query.bias\n",
      "encoder.layer.2.attention.self.key.weight\n",
      "encoder.layer.2.attention.self.key.bias\n",
      "encoder.layer.2.attention.self.value.weight\n",
      "encoder.layer.2.attention.self.value.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.layer.2.intermediate.dense.weight\n",
      "encoder.layer.2.intermediate.dense.bias\n",
      "encoder.layer.2.output.dense.weight\n",
      "encoder.layer.2.output.dense.bias\n",
      "encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.layer.3.attention.self.query.weight\n",
      "encoder.layer.3.attention.self.query.bias\n",
      "encoder.layer.3.attention.self.key.weight\n",
      "encoder.layer.3.attention.self.key.bias\n",
      "encoder.layer.3.attention.self.value.weight\n",
      "encoder.layer.3.attention.self.value.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.layer.3.intermediate.dense.weight\n",
      "encoder.layer.3.intermediate.dense.bias\n",
      "encoder.layer.3.output.dense.weight\n",
      "encoder.layer.3.output.dense.bias\n",
      "encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.layer.4.attention.self.query.weight\n",
      "encoder.layer.4.attention.self.query.bias\n",
      "encoder.layer.4.attention.self.key.weight\n",
      "encoder.layer.4.attention.self.key.bias\n",
      "encoder.layer.4.attention.self.value.weight\n",
      "encoder.layer.4.attention.self.value.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.layer.4.intermediate.dense.weight\n",
      "encoder.layer.4.intermediate.dense.bias\n",
      "encoder.layer.4.output.dense.weight\n",
      "encoder.layer.4.output.dense.bias\n",
      "encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.layer.5.attention.self.query.weight\n",
      "encoder.layer.5.attention.self.query.bias\n",
      "encoder.layer.5.attention.self.key.weight\n",
      "encoder.layer.5.attention.self.key.bias\n",
      "encoder.layer.5.attention.self.value.weight\n",
      "encoder.layer.5.attention.self.value.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.layer.5.intermediate.dense.weight\n",
      "encoder.layer.5.intermediate.dense.bias\n",
      "encoder.layer.5.output.dense.weight\n",
      "encoder.layer.5.output.dense.bias\n",
      "encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.layer.6.attention.self.query.weight\n",
      "encoder.layer.6.attention.self.query.bias\n",
      "encoder.layer.6.attention.self.key.weight\n",
      "encoder.layer.6.attention.self.key.bias\n",
      "encoder.layer.6.attention.self.value.weight\n",
      "encoder.layer.6.attention.self.value.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.layer.6.intermediate.dense.weight\n",
      "encoder.layer.6.intermediate.dense.bias\n",
      "encoder.layer.6.output.dense.weight\n",
      "encoder.layer.6.output.dense.bias\n",
      "encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.layer.7.attention.self.query.weight\n",
      "encoder.layer.7.attention.self.query.bias\n",
      "encoder.layer.7.attention.self.key.weight\n",
      "encoder.layer.7.attention.self.key.bias\n",
      "encoder.layer.7.attention.self.value.weight\n",
      "encoder.layer.7.attention.self.value.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.layer.7.intermediate.dense.weight\n",
      "encoder.layer.7.intermediate.dense.bias\n",
      "encoder.layer.7.output.dense.weight\n",
      "encoder.layer.7.output.dense.bias\n",
      "encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.layer.8.attention.self.query.weight\n",
      "encoder.layer.8.attention.self.query.bias\n",
      "encoder.layer.8.attention.self.key.weight\n",
      "encoder.layer.8.attention.self.key.bias\n",
      "encoder.layer.8.attention.self.value.weight\n",
      "encoder.layer.8.attention.self.value.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.layer.8.intermediate.dense.weight\n",
      "encoder.layer.8.intermediate.dense.bias\n",
      "encoder.layer.8.output.dense.weight\n",
      "encoder.layer.8.output.dense.bias\n",
      "encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.layer.9.attention.self.query.weight\n",
      "encoder.layer.9.attention.self.query.bias\n",
      "encoder.layer.9.attention.self.key.weight\n",
      "encoder.layer.9.attention.self.key.bias\n",
      "encoder.layer.9.attention.self.value.weight\n",
      "encoder.layer.9.attention.self.value.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.layer.9.intermediate.dense.weight\n",
      "encoder.layer.9.intermediate.dense.bias\n",
      "encoder.layer.9.output.dense.weight\n",
      "encoder.layer.9.output.dense.bias\n",
      "encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.layer.10.attention.self.query.weight\n",
      "encoder.layer.10.attention.self.query.bias\n",
      "encoder.layer.10.attention.self.key.weight\n",
      "encoder.layer.10.attention.self.key.bias\n",
      "encoder.layer.10.attention.self.value.weight\n",
      "encoder.layer.10.attention.self.value.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.layer.10.intermediate.dense.weight\n",
      "encoder.layer.10.intermediate.dense.bias\n",
      "encoder.layer.10.output.dense.weight\n",
      "encoder.layer.10.output.dense.bias\n",
      "encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.layer.11.attention.self.query.weight\n",
      "encoder.layer.11.attention.self.query.bias\n",
      "encoder.layer.11.attention.self.key.weight\n",
      "encoder.layer.11.attention.self.key.bias\n",
      "encoder.layer.11.attention.self.value.weight\n",
      "encoder.layer.11.attention.self.value.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.layer.11.intermediate.dense.weight\n",
      "encoder.layer.11.intermediate.dense.bias\n",
      "encoder.layer.11.output.dense.weight\n",
      "encoder.layer.11.output.dense.bias\n",
      "encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.layer.11.output.LayerNorm.bias\n",
      "encoder.layer.12.attention.self.query.weight\n",
      "encoder.layer.12.attention.self.query.bias\n",
      "encoder.layer.12.attention.self.key.weight\n",
      "encoder.layer.12.attention.self.key.bias\n",
      "encoder.layer.12.attention.self.value.weight\n",
      "encoder.layer.12.attention.self.value.bias\n",
      "encoder.layer.12.attention.output.dense.weight\n",
      "encoder.layer.12.attention.output.dense.bias\n",
      "encoder.layer.12.attention.output.LayerNorm.weight\n",
      "encoder.layer.12.attention.output.LayerNorm.bias\n",
      "encoder.layer.12.intermediate.dense.weight\n",
      "encoder.layer.12.intermediate.dense.bias\n",
      "encoder.layer.12.output.dense.weight\n",
      "encoder.layer.12.output.dense.bias\n",
      "encoder.layer.12.output.LayerNorm.weight\n",
      "encoder.layer.12.output.LayerNorm.bias\n",
      "encoder.layer.13.attention.self.query.weight\n",
      "encoder.layer.13.attention.self.query.bias\n",
      "encoder.layer.13.attention.self.key.weight\n",
      "encoder.layer.13.attention.self.key.bias\n",
      "encoder.layer.13.attention.self.value.weight\n",
      "encoder.layer.13.attention.self.value.bias\n",
      "encoder.layer.13.attention.output.dense.weight\n",
      "encoder.layer.13.attention.output.dense.bias\n",
      "encoder.layer.13.attention.output.LayerNorm.weight\n",
      "encoder.layer.13.attention.output.LayerNorm.bias\n",
      "encoder.layer.13.intermediate.dense.weight\n",
      "encoder.layer.13.intermediate.dense.bias\n",
      "encoder.layer.13.output.dense.weight\n",
      "encoder.layer.13.output.dense.bias\n",
      "encoder.layer.13.output.LayerNorm.weight\n",
      "encoder.layer.13.output.LayerNorm.bias\n",
      "encoder.layer.14.attention.self.query.weight\n",
      "encoder.layer.14.attention.self.query.bias\n",
      "encoder.layer.14.attention.self.key.weight\n",
      "encoder.layer.14.attention.self.key.bias\n",
      "encoder.layer.14.attention.self.value.weight\n",
      "encoder.layer.14.attention.self.value.bias\n",
      "encoder.layer.14.attention.output.dense.weight\n",
      "encoder.layer.14.attention.output.dense.bias\n",
      "encoder.layer.14.attention.output.LayerNorm.weight\n",
      "encoder.layer.14.attention.output.LayerNorm.bias\n",
      "encoder.layer.14.intermediate.dense.weight\n",
      "encoder.layer.14.intermediate.dense.bias\n",
      "encoder.layer.14.output.dense.weight\n",
      "encoder.layer.14.output.dense.bias\n",
      "encoder.layer.14.output.LayerNorm.weight\n",
      "encoder.layer.14.output.LayerNorm.bias\n",
      "encoder.layer.15.attention.self.query.weight\n",
      "encoder.layer.15.attention.self.query.bias\n",
      "encoder.layer.15.attention.self.key.weight\n",
      "encoder.layer.15.attention.self.key.bias\n",
      "encoder.layer.15.attention.self.value.weight\n",
      "encoder.layer.15.attention.self.value.bias\n",
      "encoder.layer.15.attention.output.dense.weight\n",
      "encoder.layer.15.attention.output.dense.bias\n",
      "encoder.layer.15.attention.output.LayerNorm.weight\n",
      "encoder.layer.15.attention.output.LayerNorm.bias\n",
      "encoder.layer.15.intermediate.dense.weight\n",
      "encoder.layer.15.intermediate.dense.bias\n",
      "encoder.layer.15.output.dense.weight\n",
      "encoder.layer.15.output.dense.bias\n",
      "encoder.layer.15.output.LayerNorm.weight\n",
      "encoder.layer.15.output.LayerNorm.bias\n",
      "encoder.layer.16.attention.self.query.weight\n",
      "encoder.layer.16.attention.self.query.bias\n",
      "encoder.layer.16.attention.self.key.weight\n",
      "encoder.layer.16.attention.self.key.bias\n",
      "encoder.layer.16.attention.self.value.weight\n",
      "encoder.layer.16.attention.self.value.bias\n",
      "encoder.layer.16.attention.output.dense.weight\n",
      "encoder.layer.16.attention.output.dense.bias\n",
      "encoder.layer.16.attention.output.LayerNorm.weight\n",
      "encoder.layer.16.attention.output.LayerNorm.bias\n",
      "encoder.layer.16.intermediate.dense.weight\n",
      "encoder.layer.16.intermediate.dense.bias\n",
      "encoder.layer.16.output.dense.weight\n",
      "encoder.layer.16.output.dense.bias\n",
      "encoder.layer.16.output.LayerNorm.weight\n",
      "encoder.layer.16.output.LayerNorm.bias\n",
      "encoder.layer.17.attention.self.query.weight\n",
      "encoder.layer.17.attention.self.query.bias\n",
      "encoder.layer.17.attention.self.key.weight\n",
      "encoder.layer.17.attention.self.key.bias\n",
      "encoder.layer.17.attention.self.value.weight\n",
      "encoder.layer.17.attention.self.value.bias\n",
      "encoder.layer.17.attention.output.dense.weight\n",
      "encoder.layer.17.attention.output.dense.bias\n",
      "encoder.layer.17.attention.output.LayerNorm.weight\n",
      "encoder.layer.17.attention.output.LayerNorm.bias\n",
      "encoder.layer.17.intermediate.dense.weight\n",
      "encoder.layer.17.intermediate.dense.bias\n",
      "encoder.layer.17.output.dense.weight\n",
      "encoder.layer.17.output.dense.bias\n",
      "encoder.layer.17.output.LayerNorm.weight\n",
      "encoder.layer.17.output.LayerNorm.bias\n",
      "encoder.layer.18.attention.self.query.weight\n",
      "encoder.layer.18.attention.self.query.bias\n",
      "encoder.layer.18.attention.self.key.weight\n",
      "encoder.layer.18.attention.self.key.bias\n",
      "encoder.layer.18.attention.self.value.weight\n",
      "encoder.layer.18.attention.self.value.bias\n",
      "encoder.layer.18.attention.output.dense.weight\n",
      "encoder.layer.18.attention.output.dense.bias\n",
      "encoder.layer.18.attention.output.LayerNorm.weight\n",
      "encoder.layer.18.attention.output.LayerNorm.bias\n",
      "encoder.layer.18.intermediate.dense.weight\n",
      "encoder.layer.18.intermediate.dense.bias\n",
      "encoder.layer.18.output.dense.weight\n",
      "encoder.layer.18.output.dense.bias\n",
      "encoder.layer.18.output.LayerNorm.weight\n",
      "encoder.layer.18.output.LayerNorm.bias\n",
      "encoder.layer.19.attention.self.query.weight\n",
      "encoder.layer.19.attention.self.query.bias\n",
      "encoder.layer.19.attention.self.key.weight\n",
      "encoder.layer.19.attention.self.key.bias\n",
      "encoder.layer.19.attention.self.value.weight\n",
      "encoder.layer.19.attention.self.value.bias\n",
      "encoder.layer.19.attention.output.dense.weight\n",
      "encoder.layer.19.attention.output.dense.bias\n",
      "encoder.layer.19.attention.output.LayerNorm.weight\n",
      "encoder.layer.19.attention.output.LayerNorm.bias\n",
      "encoder.layer.19.intermediate.dense.weight\n",
      "encoder.layer.19.intermediate.dense.bias\n",
      "encoder.layer.19.output.dense.weight\n",
      "encoder.layer.19.output.dense.bias\n",
      "encoder.layer.19.output.LayerNorm.weight\n",
      "encoder.layer.19.output.LayerNorm.bias\n",
      "encoder.layer.20.attention.self.query.weight\n",
      "encoder.layer.20.attention.self.query.bias\n",
      "encoder.layer.20.attention.self.key.weight\n",
      "encoder.layer.20.attention.self.key.bias\n",
      "encoder.layer.20.attention.self.value.weight\n",
      "encoder.layer.20.attention.self.value.bias\n",
      "encoder.layer.20.attention.output.dense.weight\n",
      "encoder.layer.20.attention.output.dense.bias\n",
      "encoder.layer.20.attention.output.LayerNorm.weight\n",
      "encoder.layer.20.attention.output.LayerNorm.bias\n",
      "encoder.layer.20.intermediate.dense.weight\n",
      "encoder.layer.20.intermediate.dense.bias\n",
      "encoder.layer.20.output.dense.weight\n",
      "encoder.layer.20.output.dense.bias\n",
      "encoder.layer.20.output.LayerNorm.weight\n",
      "encoder.layer.20.output.LayerNorm.bias\n",
      "encoder.layer.21.attention.self.query.weight\n",
      "encoder.layer.21.attention.self.query.bias\n",
      "encoder.layer.21.attention.self.key.weight\n",
      "encoder.layer.21.attention.self.key.bias\n",
      "encoder.layer.21.attention.self.value.weight\n",
      "encoder.layer.21.attention.self.value.bias\n",
      "encoder.layer.21.attention.output.dense.weight\n",
      "encoder.layer.21.attention.output.dense.bias\n",
      "encoder.layer.21.attention.output.LayerNorm.weight\n",
      "encoder.layer.21.attention.output.LayerNorm.bias\n",
      "encoder.layer.21.intermediate.dense.weight\n",
      "encoder.layer.21.intermediate.dense.bias\n",
      "encoder.layer.21.output.dense.weight\n",
      "encoder.layer.21.output.dense.bias\n",
      "encoder.layer.21.output.LayerNorm.weight\n",
      "encoder.layer.21.output.LayerNorm.bias\n",
      "encoder.layer.22.attention.self.query.weight\n",
      "encoder.layer.22.attention.self.query.bias\n",
      "encoder.layer.22.attention.self.key.weight\n",
      "encoder.layer.22.attention.self.key.bias\n",
      "encoder.layer.22.attention.self.value.weight\n",
      "encoder.layer.22.attention.self.value.bias\n",
      "encoder.layer.22.attention.output.dense.weight\n",
      "encoder.layer.22.attention.output.dense.bias\n",
      "encoder.layer.22.attention.output.LayerNorm.weight\n",
      "encoder.layer.22.attention.output.LayerNorm.bias\n",
      "encoder.layer.22.intermediate.dense.weight\n",
      "encoder.layer.22.intermediate.dense.bias\n",
      "encoder.layer.22.output.dense.weight\n",
      "encoder.layer.22.output.dense.bias\n",
      "encoder.layer.22.output.LayerNorm.weight\n",
      "encoder.layer.22.output.LayerNorm.bias\n",
      "encoder.layer.23.attention.self.query.weight\n",
      "encoder.layer.23.attention.self.query.bias\n",
      "encoder.layer.23.attention.self.key.weight\n",
      "encoder.layer.23.attention.self.key.bias\n",
      "encoder.layer.23.attention.self.value.weight\n",
      "encoder.layer.23.attention.self.value.bias\n",
      "encoder.layer.23.attention.output.dense.weight\n",
      "encoder.layer.23.attention.output.dense.bias\n",
      "encoder.layer.23.attention.output.LayerNorm.weight\n",
      "encoder.layer.23.attention.output.LayerNorm.bias\n",
      "encoder.layer.23.intermediate.dense.weight\n",
      "encoder.layer.23.intermediate.dense.bias\n",
      "encoder.layer.23.output.dense.weight\n",
      "encoder.layer.23.output.dense.bias\n",
      "encoder.layer.23.output.LayerNorm.weight\n",
      "encoder.layer.23.output.LayerNorm.bias\n",
      "pooler.dense.weight\n",
      "pooler.dense.bias\n",
      "I will be frozen: embeddings.word_embeddings.weight\n",
      "I will be frozen: embeddings.position_embeddings.weight\n",
      "I will be frozen: embeddings.token_type_embeddings.weight\n",
      "I will be frozen: embeddings.LayerNorm.weight\n",
      "I will be frozen: embeddings.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.0.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.0.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.0.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.0.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.0.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.0.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.0.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.0.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.0.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.0.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.0.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.0.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.0.output.dense.weight\n",
      "I will be frozen: encoder.layer.0.output.dense.bias\n",
      "I will be frozen: encoder.layer.0.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.0.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.1.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.1.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.1.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.1.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.1.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.1.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.1.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.1.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.1.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.1.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.1.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.1.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.1.output.dense.weight\n",
      "I will be frozen: encoder.layer.1.output.dense.bias\n",
      "I will be frozen: encoder.layer.1.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.1.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.2.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.2.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.2.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.2.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.2.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.2.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.2.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.2.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.2.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.2.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.2.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.2.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.2.output.dense.weight\n",
      "I will be frozen: encoder.layer.2.output.dense.bias\n",
      "I will be frozen: encoder.layer.2.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.2.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.3.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.3.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.3.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.3.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.3.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.3.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.3.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.3.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.3.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.3.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.3.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.3.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.3.output.dense.weight\n",
      "I will be frozen: encoder.layer.3.output.dense.bias\n",
      "I will be frozen: encoder.layer.3.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.3.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.4.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.4.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.4.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.4.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.4.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.4.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.4.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.4.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.4.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.4.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.4.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.4.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.4.output.dense.weight\n",
      "I will be frozen: encoder.layer.4.output.dense.bias\n",
      "I will be frozen: encoder.layer.4.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.4.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.5.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.5.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.5.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.5.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.5.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.5.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.5.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.5.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.5.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.5.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.5.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.5.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.5.output.dense.weight\n",
      "I will be frozen: encoder.layer.5.output.dense.bias\n",
      "I will be frozen: encoder.layer.5.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.5.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.6.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.6.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.6.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.6.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.6.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.6.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.6.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.6.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.6.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.6.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.6.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.6.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.6.output.dense.weight\n",
      "I will be frozen: encoder.layer.6.output.dense.bias\n",
      "I will be frozen: encoder.layer.6.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.6.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.7.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.7.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.7.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.7.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.7.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.7.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.7.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.7.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.7.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.7.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.7.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.7.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.7.output.dense.weight\n",
      "I will be frozen: encoder.layer.7.output.dense.bias\n",
      "I will be frozen: encoder.layer.7.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.7.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.8.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.8.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.8.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.8.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.8.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.8.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.8.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.8.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.8.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.8.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.8.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.8.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.8.output.dense.weight\n",
      "I will be frozen: encoder.layer.8.output.dense.bias\n",
      "I will be frozen: encoder.layer.8.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.8.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.9.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.9.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.9.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.9.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.9.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.9.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.9.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.9.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.9.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.9.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.9.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.9.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.9.output.dense.weight\n",
      "I will be frozen: encoder.layer.9.output.dense.bias\n",
      "I will be frozen: encoder.layer.9.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.9.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.10.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.10.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.10.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.10.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.10.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.10.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.10.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.10.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.10.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.10.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.10.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.10.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.10.output.dense.weight\n",
      "I will be frozen: encoder.layer.10.output.dense.bias\n",
      "I will be frozen: encoder.layer.10.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.10.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.11.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.11.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.11.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.11.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.11.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.11.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.11.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.11.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.11.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.11.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.11.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.11.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.11.output.dense.weight\n",
      "I will be frozen: encoder.layer.11.output.dense.bias\n",
      "I will be frozen: encoder.layer.11.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.11.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.12.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.12.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.12.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.12.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.12.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.12.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.12.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.12.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.12.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.12.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.12.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.12.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.12.output.dense.weight\n",
      "I will be frozen: encoder.layer.12.output.dense.bias\n",
      "I will be frozen: encoder.layer.12.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.12.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.13.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.13.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.13.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.13.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.13.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.13.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.13.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.13.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.13.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.13.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.13.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.13.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.13.output.dense.weight\n",
      "I will be frozen: encoder.layer.13.output.dense.bias\n",
      "I will be frozen: encoder.layer.13.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.13.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.14.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.14.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.14.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.14.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.14.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.14.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.14.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.14.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.14.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.14.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.14.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.14.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.14.output.dense.weight\n",
      "I will be frozen: encoder.layer.14.output.dense.bias\n",
      "I will be frozen: encoder.layer.14.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.14.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.15.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.15.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.15.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.15.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.15.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.15.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.15.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.15.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.15.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.15.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.15.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.15.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.15.output.dense.weight\n",
      "I will be frozen: encoder.layer.15.output.dense.bias\n",
      "I will be frozen: encoder.layer.15.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.15.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.16.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.16.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.16.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.16.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.16.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.16.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.16.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.16.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.16.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.16.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.16.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.16.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.16.output.dense.weight\n",
      "I will be frozen: encoder.layer.16.output.dense.bias\n",
      "I will be frozen: encoder.layer.16.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.16.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.17.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.17.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.17.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.17.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.17.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.17.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.17.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.17.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.17.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.17.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.17.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.17.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.17.output.dense.weight\n",
      "I will be frozen: encoder.layer.17.output.dense.bias\n",
      "I will be frozen: encoder.layer.17.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.17.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.18.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.18.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.18.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.18.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.18.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.18.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.18.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.18.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.18.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.18.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.18.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.18.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.18.output.dense.weight\n",
      "I will be frozen: encoder.layer.18.output.dense.bias\n",
      "I will be frozen: encoder.layer.18.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.18.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.19.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.19.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.19.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.19.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.19.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.19.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.19.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.19.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.19.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.19.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.19.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.19.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.19.output.dense.weight\n",
      "I will be frozen: encoder.layer.19.output.dense.bias\n",
      "I will be frozen: encoder.layer.19.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.19.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.20.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.20.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.20.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.20.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.20.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.20.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.20.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.20.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.20.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.20.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.20.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.20.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.20.output.dense.weight\n",
      "I will be frozen: encoder.layer.20.output.dense.bias\n",
      "I will be frozen: encoder.layer.20.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.20.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.21.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.21.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.21.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.21.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.21.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.21.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.21.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.21.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.21.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.21.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.21.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.21.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.21.output.dense.weight\n",
      "I will be frozen: encoder.layer.21.output.dense.bias\n",
      "I will be frozen: encoder.layer.21.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.21.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.22.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.22.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.22.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.22.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.22.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.22.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.22.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.22.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.22.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.22.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.22.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.22.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.22.output.dense.weight\n",
      "I will be frozen: encoder.layer.22.output.dense.bias\n",
      "I will be frozen: encoder.layer.22.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.22.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.23.attention.self.query.weight\n",
      "I will be frozen: encoder.layer.23.attention.self.query.bias\n",
      "I will be frozen: encoder.layer.23.attention.self.key.weight\n",
      "I will be frozen: encoder.layer.23.attention.self.key.bias\n",
      "I will be frozen: encoder.layer.23.attention.self.value.weight\n",
      "I will be frozen: encoder.layer.23.attention.self.value.bias\n",
      "I will be frozen: encoder.layer.23.attention.output.dense.weight\n",
      "I will be frozen: encoder.layer.23.attention.output.dense.bias\n",
      "I will be frozen: encoder.layer.23.attention.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.23.attention.output.LayerNorm.bias\n",
      "I will be frozen: encoder.layer.23.intermediate.dense.weight\n",
      "I will be frozen: encoder.layer.23.intermediate.dense.bias\n",
      "I will be frozen: encoder.layer.23.output.dense.weight\n",
      "I will be frozen: encoder.layer.23.output.dense.bias\n",
      "I will be frozen: encoder.layer.23.output.LayerNorm.weight\n",
      "I will be frozen: encoder.layer.23.output.LayerNorm.bias\n",
      "I will be frozen: pooler.dense.weight\n",
      "I will be frozen: pooler.dense.bias\n",
      "ROBERTAClass(\n",
      "  (l1): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50262, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=1024, out_features=264, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (classifier): Linear(in_features=264, out_features=3, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hiperpar√°metros del modelo\n",
    "BATCH_SIZE = 16  # [16,32,64]\n",
    "MAX_SEQUENCE = 256 # [128, 256, 512]\n",
    "LEARNING_RATE = 2e-5 # [1e-4, 5e-5, 1e-5]\n",
    "EPOCH = 10 # [10, 15, 20]\n",
    "\n",
    "# Divisi√≥n de datos en conjuntos de entrenamiento y validaci√≥n\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['full_text_processed'], \n",
    "                                                  df['label'], \n",
    "                                                  test_size=0.2, \n",
    "                                                  random_state=42, \n",
    "                                                  stratify=df['label'])\n",
    "df.loc[X_train.index, 'data_type'] = 'train'\n",
    "df.loc[X_val.index, 'data_type'] = 'val'\n",
    "df.groupby(['label', 'data_type']).count()\n",
    "\n",
    "# Prepara los datos para el entrenamiento y validaci√≥n\n",
    "encoded_data_train = tokenizer.batch_encode_plus( \n",
    "    df[df.data_type=='train']['full_text_processed'].values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    padding=True, \n",
    "    truncation=True,\n",
    "    max_length=MAX_SEQUENCE, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='val']['full_text_processed'].values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    padding=True, \n",
    "    truncation=True,\n",
    "    max_length=MAX_SEQUENCE,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df[df.data_type=='train']['label'].values)\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df[df.data_type=='val']['label'].values)\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "len(dataset_train), len(dataset_val)\n",
    "\n",
    "# Configuraci√≥n del dispositivo (GPU o CPU) y congelaci√≥n de par√°metros del modelo base\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "\n",
    "for name, param in list(model.named_parameters()):#[:-20]:\n",
    "    print('I will be frozen: {}'.format(name))\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "# Definici√≥n de la arquitectura del modelo de clasificaci√≥n\n",
    "class ROBERTAClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ROBERTAClass, self).__init__()\n",
    "        self.l1 = model\n",
    "        self.pre_classifier = torch.nn.Linear(1024, 264) # roberta large 1024, roberta base 768\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(264, n_labels)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Use the pooled output (the second element of the outputs tuple) for classification\n",
    "        # This is typically what you want for a classification task.\n",
    "        pooler_output = outputs[1]\n",
    "\n",
    "        pooler_output = self.pre_classifier(pooler_output)\n",
    "        pooler_output = torch.nn.ReLU()(pooler_output)\n",
    "        pooler_output = self.dropout(pooler_output)\n",
    "        output = self.classifier(pooler_output)\n",
    "        return output\n",
    "\n",
    "# Instanciaci√≥n y configuraci√≥n del modelo\n",
    "model = ROBERTAClass()\n",
    "model.to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Configuraci√≥n de los dataloaders y optimizador\n",
    "batch_size = BATCH_SIZE\n",
    "dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "dataloader_validation = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=batch_size)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)\n",
    "epochs = EPOCH\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train)*epochs)\n",
    "\n",
    "# Definici√≥n de funciones para evaluaci√≥n y m√©tricas\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# C√°lculo de los pesos de clase\n",
    "class_wts = compute_class_weight(class_weight='balanced', classes=np.unique(labels_train), y=labels_train.numpy())\n",
    "weights = torch.tensor(class_wts, dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "\n",
    "# Definici√≥n de la funci√≥n de p√©rdida con pesos de clase\n",
    "def weighted_cross_entropy(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss(weight=weights)(outputs, targets)\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in dataloader_val:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1]\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = weighted_cross_entropy(outputs, batch[2])\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        # Append predictions and true values for later evaluation\n",
    "        predictions.append(outputs.detach().cpu().numpy())\n",
    "        true_vals.append(batch[2].cpu().numpy())\n",
    "\n",
    "    # Calculate average loss over all batches\n",
    "    loss_val_avg = loss_val_total / len(dataloader_val)\n",
    "\n",
    "    # Concatenate all batches\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "\n",
    "    # Convert predictions to labels\n",
    "    preds_flat = np.argmax(predictions, axis=1).flatten()\n",
    "\n",
    "    # Calcular la matriz de confusi√≥n\n",
    "    cm = confusion_matrix(true_vals.flatten(), preds_flat)\n",
    "\n",
    "    # Opcional: Visualizar la matriz de confusi√≥n\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=\"Blues\")\n",
    "    ax.set_xlabel('Predicciones')\n",
    "    ax.set_ylabel('Etiquetas Verdaderas')\n",
    "    ax.set_title('Matriz de Confusi√≥n')\n",
    "    plt.show()\n",
    "\n",
    "    # Retorna la p√©rdida promedio, predicciones, etiquetas verdaderas, y la matriz de confusi√≥n\n",
    "    return loss_val_avg, predictions, true_vals, cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [32:27<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 1.0841427233183023\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 1/10 [34:16<5:08:25, 2056.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 1.0524518002163281\n",
      "F1 Score (Weighted): 0.7520647696747399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 1/10 [1:07:08<5:08:25, 2056.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 1.0697261930415125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 1/10 [1:09:00<10:21:04, 4140.53s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jorge\\Desktop\\UNI\\4-CUARTO\\4-2-TFG\\CODE\\Gender-Bias\\FineTuningNLP.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jorge/Desktop/UNI/4-CUARTO/4-2-TFG/CODE/Gender-Bias/FineTuningNLP.ipynb#X10sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m loss_train_avg \u001b[39m=\u001b[39m loss_train_total \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(dataloader_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jorge/Desktop/UNI/4-CUARTO/4-2-TFG/CODE/Gender-Bias/FineTuningNLP.ipynb#X10sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m tqdm\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining loss: \u001b[39m\u001b[39m{\u001b[39;00mloss_train_avg\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jorge/Desktop/UNI/4-CUARTO/4-2-TFG/CODE/Gender-Bias/FineTuningNLP.ipynb#X10sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m val_loss, predictions, true_vals, confusion_matrix \u001b[39m=\u001b[39m evaluate(dataloader_validation)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jorge/Desktop/UNI/4-CUARTO/4-2-TFG/CODE/Gender-Bias/FineTuningNLP.ipynb#X10sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m val_f1 \u001b[39m=\u001b[39m f1_score_func(predictions, true_vals)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jorge/Desktop/UNI/4-CUARTO/4-2-TFG/CODE/Gender-Bias/FineTuningNLP.ipynb#X10sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m tqdm\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\jorge\\Desktop\\UNI\\4-CUARTO\\4-2-TFG\\CODE\\Gender-Bias\\FineTuningNLP.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jorge/Desktop/UNI/4-CUARTO/4-2-TFG/CODE/Gender-Bias/FineTuningNLP.ipynb#X10sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m preds_flat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(predictions, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jorge/Desktop/UNI/4-CUARTO/4-2-TFG/CODE/Gender-Bias/FineTuningNLP.ipynb#X10sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m \u001b[39m# Calcular la matriz de confusi√≥n\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/jorge/Desktop/UNI/4-CUARTO/4-2-TFG/CODE/Gender-Bias/FineTuningNLP.ipynb#X10sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m cm \u001b[39m=\u001b[39m confusion_matrix(true_vals\u001b[39m.\u001b[39;49mflatten(), preds_flat)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jorge/Desktop/UNI/4-CUARTO/4-2-TFG/CODE/Gender-Bias/FineTuningNLP.ipynb#X10sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m \u001b[39m# Opcional: Visualizar la matriz de confusi√≥n\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jorge/Desktop/UNI/4-CUARTO/4-2-TFG/CODE/Gender-Bias/FineTuningNLP.ipynb#X10sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m,\u001b[39m10\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    model.train()\n",
    "    loss_train_total = 0\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        # Reset the gradients from the previous iteration\n",
    "        model.zero_grad()\n",
    "\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1]\n",
    "        }\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        #logits = outputs.logits\n",
    "        loss = weighted_cross_entropy(outputs, batch[2])\n",
    "        loss_train_total += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item() / len(batch))})\n",
    "\n",
    "    torch.save(model.state_dict(), f'roberta-base-bne/finetuned_roberta-base-bne_epoch_{epoch}.model')\n",
    "\n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    loss_train_avg = loss_train_total / len(dataloader_train)\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    val_loss, predictions, true_vals, confusion_matrix = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluaci√≥n del modelo en el conjunto de validaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluaci√≥n en el conjunto de validaci√≥n\n",
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "\n",
    "# Manipulaci√≥n de predicciones y creaci√≥n del csv\n",
    "tensor_pred = torch.from_numpy(predictions)\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "softmax(tensor_pred)\n",
    "df_predictions = pd.DataFrame(columns = [\"full_text_processed\", \"True Rating\", \"Predicted Rating\", \"Pred Prob 0\", \"Pred Prob 1\", \"Pred Prob 2\"])\n",
    "df_predictions[\"full_text_processed\"] = df[df.data_type=='val']['full_text_processed'].copy().reset_index(drop = True)\n",
    "df_predictions[\"True Rating\"] = df[df.data_type=='val']['label'].reset_index(drop = True).copy()\n",
    "df_predictions[\"Predicted Rating\"] = np.argmax(predictions, axis=1)\n",
    "df_predictions[[\"Pred Prob 0\", \"Pred Prob 1\", \"Pred Prob 2\"]] = list(softmax(tensor_pred).numpy())\n",
    "df_predictions.to_csv('df_predictions.csv', encoding = 'utf8')\n",
    "df_predictions.to_excel('df_prediction.xlsx')\n",
    "val_f1 = f1_score_func(predictions, true_vals)\n",
    "acc = accuracy_per_class(predictions, true_vals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
