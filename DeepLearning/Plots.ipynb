{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy.stats import linregress\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'Comentario Positivo': 0, 'Comentario Negativo': 1}\n",
    "# Define the list of labels and colors\n",
    "labels = ['Comentario Positivo', 'Comentario Negativo']\n",
    "colors = ['darkgreen', 'darkred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file\n",
    "path = \"DL2_Análisis General.xlsx\"\n",
    "df_AG_SC = pd.read_excel(path, sheet_name=\"Sin_contexto\")\n",
    "df_AG_TC = pd.read_excel(path, sheet_name=\"Tweet_context\")\n",
    "df_AG_FC = pd.read_excel(path, sheet_name=\"Full_context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to categorical: {'Comentario Positivo': 0, 'Comentario Negativo': 1}\n",
    "mapping = {'Comentario Positivo': 0, 'Comentario Negativo': 1}\n",
    "\n",
    "# Create a dictionary for renaming\n",
    "rename_dict = {}\n",
    "for key, value in mapping.items():\n",
    "    rename_dict[f'eval_{value}_precision'] = f'eval_{key}_precision'\n",
    "    rename_dict[f'eval_{value}_recall'] = f'eval_{key}_recall'\n",
    "    rename_dict[f'eval_{value}_f1'] = f'eval_{key}_f1-score'\n",
    "    rename_dict[f'eval_{value}_support'] = f'eval_{key}_support'\n",
    "\n",
    "# Rename the columns\n",
    "df_AG_SC.rename(columns=rename_dict, inplace=True)\n",
    "df_AG_TC.rename(columns=rename_dict, inplace=True)\n",
    "df_AG_FC.rename(columns=rename_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir la nueva columna 'Context' a cada DataFrame\n",
    "df_AG_SC['Context'] = 'Without Context'\n",
    "df_AG_TC['Context'] = 'Tweet Context'\n",
    "df_AG_FC['Context'] = 'Full Context'\n",
    "\n",
    "# Concatenar los tres DataFrames\n",
    "df_AG_join = pd.concat([df_AG_SC[['Model_Description', 'Total Time', 'Context', 'eval_weighted_f1', f'eval_{labels[0]}_f1-score', f'eval_{labels[1]}_f1-score', 'eval_accuracy']], \n",
    "                         df_AG_TC[['Model_Description', 'Total Time', 'Context', 'eval_weighted_f1', f'eval_{labels[0]}_f1-score', f'eval_{labels[1]}_f1-score', 'eval_accuracy']], \n",
    "                         df_AG_FC[['Model_Description', 'Total Time', 'Context', 'eval_weighted_f1', f'eval_{labels[0]}_f1-score', f'eval_{labels[1]}_f1-score', 'eval_accuracy']]])\n",
    "\n",
    "\n",
    "\n",
    "df_AG_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to include only numeric columns\n",
    "numeric_cols = ['eval_weighted_f1', f'eval_{labels[0]}_f1-score', f'eval_{labels[1]}_f1-score', 'eval_accuracy', 'Total Time']\n",
    "\n",
    "# Group the data by 'Context' and calculate the mean for numeric columns\n",
    "context_grouped = df_AG_join.groupby('Context')[numeric_cols].max().reset_index()\n",
    "\n",
    "# Reorder the contexts\n",
    "context_grouped['Context'] = pd.Categorical(context_grouped['Context'], categories=['Without Context', 'Tweet Context', 'Full Context'], ordered=True)\n",
    "\n",
    "context_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate the upper limit for the y-axis dynamically\n",
    "y_max = 1.05\n",
    "\n",
    "# Create a figure with subplots using GridSpec\n",
    "fig = plt.figure(figsize=(24, 12))\n",
    "gs = GridSpec(1, 1, figure=fig)\n",
    "\n",
    "# Plot\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "# Plot lines\n",
    "sns.lineplot(ax=ax1, x='Context', y='eval_weighted_f1', data=context_grouped, marker='o', linewidth=14, markersize=26, alpha=0.7, color=\"black\", label=\"Weighted F1-Score\")\n",
    "for i, label in enumerate(labels):\n",
    "    sns.lineplot(ax=ax1, x='Context', y=f'eval_{label}_f1-score', data=context_grouped, marker='o', linewidth=10, markersize=22, alpha=0.7, color=colors[i], label=f\"{label} F1-Score\")\n",
    "\n",
    "ax1.set_title('Maximum Weighted F1-Score for Different Contexts', fontsize=42)\n",
    "ax1.set_xlabel('', fontsize=18)\n",
    "ax1.set_ylabel('F1-Score', fontsize=30)\n",
    "ax1.tick_params(axis='x', labelsize=30)\n",
    "ax1.tick_params(axis='y', labelsize=22)\n",
    "ax1.set_ylim(0.77, 1)\n",
    "\n",
    "# Move legend to the top\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=4, fontsize=22)\n",
    "\n",
    "# Add a grid to improve readability\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Remove spines for a clean look\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['left'].set_visible(False)\n",
    "ax1.spines['bottom'].set_visible(False)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"../../../IMAGES/Análisis General/DL/DL_Experiments in _Análisis General_ weightedF1_max.pdf\", format='pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate the upper limit for the y-axis dynamically\n",
    "y_max = df_AG_join['eval_weighted_f1'].mean() + 3 * df_AG_join['eval_weighted_f1'].std()\n",
    "\n",
    "# Create a figure with subplots using GridSpec\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "gs = GridSpec(1, 1, figure=fig)\n",
    "\n",
    "# Plot for MODELS\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.barplot(ax=ax1, x='Context', y='eval_weighted_f1', hue='Model_Description', data=df_AG_join, palette='crest_r', errorbar=('ci', 95), errwidth=1.5, capsize=0.2, alpha=0.7)\n",
    "ax1.set_title('Weighted F1-Score for Different Context', fontsize=30)\n",
    "ax1.set_xlabel('', fontsize=18)\n",
    "ax1.set_ylabel('F1-Score', fontsize=18)\n",
    "ax1.tick_params(axis='x', labelsize=18)\n",
    "ax1.tick_params(axis='y', labelsize=18)\n",
    "ax1.set_ylim(0, y_max)\n",
    "\n",
    "# Remove grid\n",
    "ax1.grid(False)\n",
    "\n",
    "# Move legend to the top\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=4, fontsize=16)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate the upper limit for the y-axis dynamically\n",
    "y_max = df_AG_join['eval_weighted_f1'].mean() + 3 * df_AG_join['eval_weighted_f1'].std()\n",
    "\n",
    "# Create a figure with subplots using GridSpec\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "gs = GridSpec(1, 1, figure=fig)\n",
    "\n",
    "# Plot for MODELS\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.barplot(ax=ax1, x='Context', y=f'eval_{labels[0]}_f1-score', hue='Model_Description', data=df_AG_join, palette='Greens_d', errorbar=('ci', 95), errwidth=1.5, capsize=0.2, alpha=0.7)\n",
    "ax1.set_title('Comentario Positivo F1-Score for Different Context', fontsize=30)\n",
    "ax1.set_xlabel('', fontsize=18)\n",
    "ax1.set_ylabel('F1-Score', fontsize=18)\n",
    "ax1.tick_params(axis='x', labelsize=18)\n",
    "ax1.tick_params(axis='y', labelsize=18)\n",
    "ax1.set_ylim(0, y_max)\n",
    "\n",
    "# Remove grid\n",
    "ax1.grid(False)\n",
    "\n",
    "# Move legend to the top\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=4, fontsize=16)\n",
    "\n",
    "# Adding grid for better readability\n",
    "ax1.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate the upper limit for the y-axis dynamically\n",
    "y_max = df_AG_join['eval_weighted_f1'].mean() + 3 * df_AG_join['eval_weighted_f1'].std()\n",
    "\n",
    "# Create a figure with subplots using GridSpec\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "gs = GridSpec(1, 1, figure=fig)\n",
    "\n",
    "# Plot for MODELS\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.barplot(ax=ax1, x='Context', y=f'eval_{labels[1]}_f1-score', hue='Model_Description', data=df_AG_join, palette='Reds', errorbar=('ci', 95), errwidth=1.5, capsize=0.2, alpha=0.7)\n",
    "ax1.set_title('Comentario Negativo F1-Score for Different Context', fontsize=30)\n",
    "ax1.set_xlabel('', fontsize=18)\n",
    "ax1.set_ylabel('F1-Score', fontsize=18)\n",
    "ax1.tick_params(axis='x', labelsize=18)\n",
    "ax1.tick_params(axis='y', labelsize=18)\n",
    "ax1.set_ylim(0, y_max)\n",
    "\n",
    "# Remove grid\n",
    "ax1.grid(False)\n",
    "\n",
    "# Move legend to the top\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=4, fontsize=16)\n",
    "\n",
    "# Adding grid for better readability\n",
    "ax1.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance (Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate the upper limit for the y-axis dynamically\n",
    "y_max = df_AG_join['Total Time'].mean() + 4 * df_AG_join['Total Time'].std()\n",
    "\n",
    "# Create a figure with subplots using GridSpec\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "gs = GridSpec(1, 1, figure=fig)\n",
    "\n",
    "# Plot for MODELS\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.barplot(ax=ax1, x='Context', y='Total Time', hue='Model_Description', data=df_AG_join, palette='crest_r', alpha=0.7, ci=None)\n",
    "ax1.set_title('Average Performance Time for Different Contexts', fontsize=30)\n",
    "ax1.set_xlabel('', fontsize=18)\n",
    "ax1.set_ylabel('Average Time (s)', fontsize=18)\n",
    "ax1.tick_params(axis='x', labelsize=18)\n",
    "ax1.tick_params(axis='y', labelsize=18)\n",
    "ax1.set_ylim(0, y_max)\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Move legend to the top\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=4, fontsize=18)\n",
    "\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_AG_join[['eval_weighted_f1', f'eval_{labels[0]}_f1-score', f'eval_{labels[1]}_f1-score', 'eval_accuracy', 'Total Time']].corr()\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a heatmap for the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "heatmap = sns.heatmap(correlation_matrix, annot=True, cmap='crest', fmt='.2f', linewidths=0.5, linecolor='black')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Correlation Matrix of Performance Metrics', fontsize=20)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confussion Matrix - Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# ID del modelo para el que quieres extraer la matriz de confusión\n",
    "model_id = 'dd90bd1e-e30f-4a57-9aa2-270a0eb504fb'\n",
    "\n",
    "# Extraer la matriz de confusión del DataFrame usando el ID\n",
    "conf_matrix_str = df_AG_FC.loc[df_AG_FC['Run_ID'] == model_id, 'eval_confusion_matrix'].values[0]\n",
    "conf_matrix = ast.literal_eval(conf_matrix_str)\n",
    "\n",
    "# Crear la matriz de confusión\n",
    "cm = pd.DataFrame(conf_matrix, index=labels, columns=labels)\n",
    "\n",
    "# Plotear la matriz de confusión\n",
    "plt.figure(figsize=(24, 18))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='crest', xticklabels=labels, \n",
    "            yticklabels=labels, annot_kws={\"size\": 52}, cbar=False)\n",
    "\n",
    "plt.xlabel('Predicted label', fontsize=30)\n",
    "plt.ylabel('True label', fontsize=30)\n",
    "plt.xticks(rotation=0, fontsize=50)\n",
    "plt.yticks(rotation=0, fontsize=50)\n",
    "# plt.title('Confusion Matrix for Best Model of DL Experiments')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"../../../IMAGES/Análisis General/DL/DL_Experiments in _Análisis General_ best_model.pdf\", format='pdf')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido Negativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'Desprestigiar Víctima': 0, 'Desprestigiar Acto': 1, 'Insultos': 2, 'Desprestigiar Deportista Autora': 3}\n",
    "# Define the list of labels and colors\n",
    "labels = ['Desprestigiar Víctima', 'Desprestigiar Acto', 'Insultos', 'Desprestigiar Deportista Autora']\n",
    "colors = ['#478CCF', '#36C2CE', '#77E4C8', '#4535C1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file\n",
    "path = \"DL2_Contenido Negativo.xlsx\"\n",
    "df_CNEG_SC = pd.read_excel(path, sheet_name=\"Sin_contexto\")\n",
    "df_CNEG_TC = pd.read_excel(path, sheet_name=\"Tweet_context\")\n",
    "df_CNEG_FC = pd.read_excel(path, sheet_name=\"Full_context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to categorical: {'Desprestigiar Víctima': 0, 'Desprestigiar Acto': 1, 'Insultos': 2, 'Desprestigiar Deportista Autora': 3}\n",
    "mapping = {'Desprestigiar Víctima': 0, 'Desprestigiar Acto': 1, 'Insultos': 2, 'Desprestigiar Deportista Autora': 3}\n",
    "\n",
    "# Create a dictionary for renaming\n",
    "rename_dict = {}\n",
    "for key, value in mapping.items():\n",
    "    rename_dict[f'eval_{value}_precision'] = f'eval_{key}_precision'\n",
    "    rename_dict[f'eval_{value}_recall'] = f'eval_{key}_recall'\n",
    "    rename_dict[f'eval_{value}_f1'] = f'eval_{key}_f1-score'\n",
    "    rename_dict[f'eval_{value}_support'] = f'eval_{key}_support'\n",
    "\n",
    "# Rename the columns\n",
    "df_CNEG_SC.rename(columns=rename_dict, inplace=True)\n",
    "df_CNEG_TC.rename(columns=rename_dict, inplace=True)\n",
    "df_CNEG_FC.rename(columns=rename_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir la nueva columna 'Context' a cada DataFrame\n",
    "df_CNEG_SC['Context'] = 'Without Context'\n",
    "df_CNEG_TC['Context'] = 'Tweet Context'\n",
    "df_CNEG_FC['Context'] = 'Full Context'\n",
    "\n",
    "# Concatenar los tres DataFrames\n",
    "df_CNEG_join = pd.concat([df_CNEG_SC[['Model_Description', 'Total Time', 'Context', 'eval_weighted_f1', f'eval_{labels[0]}_f1-score', f'eval_{labels[1]}_f1-score', f'eval_{labels[2]}_f1-score', f'eval_{labels[3]}_f1-score', 'eval_accuracy']], \n",
    "                         df_CNEG_TC[['Model_Description', 'Total Time', 'Context', 'eval_weighted_f1', f'eval_{labels[0]}_f1-score', f'eval_{labels[1]}_f1-score', f'eval_{labels[2]}_f1-score', f'eval_{labels[3]}_f1-score', 'eval_accuracy']], \n",
    "                         df_CNEG_FC[['Model_Description', 'Total Time', 'Context', 'eval_weighted_f1', f'eval_{labels[0]}_f1-score', f'eval_{labels[1]}_f1-score', f'eval_{labels[2]}_f1-score', f'eval_{labels[3]}_f1-score', 'eval_accuracy']]])\n",
    "\n",
    "df_CNEG_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to include only numeric columns\n",
    "numeric_cols = ['eval_weighted_f1', f'eval_{labels[0]}_f1-score', f'eval_{labels[1]}_f1-score', f'eval_{labels[2]}_f1-score', f'eval_{labels[3]}_f1-score', 'eval_accuracy', 'Total Time']\n",
    "\n",
    "# Group the data by 'Context' and calculate the mean for numeric columns\n",
    "context_grouped_CNEG = df_CNEG_join.groupby('Context')[numeric_cols].max().reset_index()\n",
    "\n",
    "# Reorder the contexts\n",
    "context_grouped_CNEG['Context'] = pd.Categorical(context_grouped_CNEG['Context'], categories=['Without Context', 'Tweet Context', 'Full Context'], ordered=True)\n",
    "\n",
    "context_grouped_CNEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate the upper limit for the y-axis dynamically\n",
    "y_max = 1.05\n",
    "\n",
    "# Create a figure with subplots using GridSpec\n",
    "fig = plt.figure(figsize=(36, 12))\n",
    "gs = GridSpec(1, 1, figure=fig)\n",
    "\n",
    "# Plot\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "# Plot lines\n",
    "sns.lineplot(ax=ax1, x='Context', y='eval_weighted_f1', data=context_grouped_CNEG, marker='o', linewidth=16, markersize=28, alpha=0.7, color=\"black\", label=\"Weighted F1-Score\")\n",
    "for i,label in enumerate(labels):\n",
    "    sns.lineplot(ax=ax1, x='Context', y=f'eval_{label}_f1-score', data=context_grouped_CNEG, marker='o', linewidth=12, markersize=24, alpha=0.7, color=colors[i], label=f\"{label} F1-Score\")\n",
    "\n",
    "ax1.set_title('Maximum Weighted F1-Score for Different Contexts', fontsize=42)\n",
    "ax1.set_xlabel('', fontsize=24)\n",
    "ax1.set_ylabel('F1-Score', fontsize=30)\n",
    "ax1.tick_params(axis='x', labelsize=32)\n",
    "ax1.tick_params(axis='y', labelsize=26)\n",
    "ax1.set_ylim(0.21, 0.8)\n",
    "\n",
    "# Move legend to the top\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=5, fontsize=24)\n",
    "\n",
    "# Add a grid to improve readability\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Remove spines for a clean look\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['left'].set_visible(False)\n",
    "ax1.spines['bottom'].set_visible(False)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"../../../IMAGES/Contenido Negativo/DL/DL_Experiments in _ContenidoNegativo_ weightedF1_max.pdf\", format='pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate the upper limit for the y-axis dynamically\n",
    "y_max = df_CNEG_join['eval_weighted_f1'].mean() + 3 * df_CNEG_join['eval_weighted_f1'].std()\n",
    "\n",
    "# Create a figure with subplots using GridSpec\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "gs = GridSpec(1, 1, figure=fig)\n",
    "\n",
    "# Plot for MODELS\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.barplot(ax=ax1, x='Context', y='eval_weighted_f1', hue='Model_Description', data=df_CNEG_join, palette='Blues', alpha=0.7, ci=None)\n",
    "ax1.set_title('Weighted F1-Score for Different Contexts', fontsize=30)\n",
    "ax1.set_xlabel('', fontsize=18)\n",
    "ax1.set_ylabel('F1-Score', fontsize=18)\n",
    "ax1.tick_params(axis='x', labelsize=18)\n",
    "ax1.tick_params(axis='y', labelsize=18)\n",
    "ax1.set_ylim(0, y_max)\n",
    "\n",
    "# Remove grid\n",
    "ax1.grid(False)\n",
    "\n",
    "# Move legend to the top\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=5, fontsize=15)\n",
    "\n",
    "# Adding grid for better readability\n",
    "ax1.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance (Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate the upper limit for the y-axis dynamically\n",
    "y_max = df_CNEG_join['Total Time'].mean() + 4 * df_CNEG_join['Total Time'].std()\n",
    "\n",
    "# Create a figure with subplots using GridSpec\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "gs = GridSpec(1, 1, figure=fig)\n",
    "\n",
    "# Plot for MODELS\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.barplot(ax=ax1, x='Context', y='Total Time', hue='Model_Description', data=df_CNEG_join, palette='Blues', alpha=0.7, ci=None)\n",
    "ax1.set_title('Average Performance Time for Different Contexts', fontsize=30)\n",
    "ax1.set_xlabel('', fontsize=18)\n",
    "ax1.set_ylabel('Average Time (s)', fontsize=18)\n",
    "ax1.tick_params(axis='x', labelsize=18)\n",
    "ax1.tick_params(axis='y', labelsize=18)\n",
    "ax1.set_ylim(0, y_max)\n",
    "\n",
    "# Move legend to the top\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=5, fontsize=15)\n",
    "\n",
    "\n",
    "# Adding grid for better readability\n",
    "ax1.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_CNEG_join[['eval_weighted_f1', f'eval_{labels[0]}_f1-score', f'eval_{labels[1]}_f1-score', f'eval_{labels[2]}_f1-score', f'eval_{labels[3]}_f1-score', 'eval_accuracy', 'Total Time']].corr()\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a heatmap for the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "heatmap = sns.heatmap(correlation_matrix, annot=True, cmap='Blues', fmt='.2f', linewidths=0.5, linecolor='black')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Correlation Matrix of Performance Metrics', fontsize=20)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confussion Matrix - Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# ID del modelo para el que quieres extraer la matriz de confusión\n",
    "model_id = '04518fe6-86e2-4ee7-b91a-275041a3ba64'\n",
    "\n",
    "# Extraer la matriz de confusión del DataFrame usando el ID\n",
    "conf_matrix_str = df_CNEG_TC.loc[df_CNEG_TC['Run_ID'] == model_id, 'eval_confusion_matrix'].values[0]\n",
    "conf_matrix = ast.literal_eval(conf_matrix_str)\n",
    "\n",
    "# Crear la matriz de confusión\n",
    "cm = pd.DataFrame(conf_matrix, index=labels, columns=labels)\n",
    "\n",
    "# Plotear la matriz de confusión\n",
    "plt.figure(figsize=(32, 30))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, \n",
    "            yticklabels=labels, annot_kws={\"size\": 52}, cbar=False)\n",
    "\n",
    "plt.xlabel('Predicted label', fontsize=30)\n",
    "plt.ylabel('True label', fontsize=30)\n",
    "plt.xticks(rotation=45, fontsize=46)\n",
    "plt.yticks(rotation=0, fontsize=46)\n",
    "# plt.title('Confusion Matrix for Best Model of ML Experiments')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"../../../IMAGES/Contenido Negativo/DL/DL_Experiments in _ContenidoNegativo_ best_model.pdf\", format='pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insultos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'Sexistas/misóginos': 0, 'Genéricos': 1, 'Deseo de Dañar': 2}\n",
    "# Define the list of labels and colors\n",
    "labels = ['Sexistas/misóginos', 'Genéricos', 'Deseo de Dañar']\n",
    "colors = ['#FFAF45', '#FB6D48', '#D74B76']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file\n",
    "path = \"DL2_Insultos.xlsx\"\n",
    "df_INS_SC = pd.read_excel(path, sheet_name=\"Sin_contexto\")\n",
    "df_INS_TC = pd.read_excel(path, sheet_name=\"Tweet_context\")\n",
    "df_INS_FC = pd.read_excel(path, sheet_name=\"Full_context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to categorical: {'Sexistas/misóginos': 0, 'Genéricos': 1, 'Deseo de Dañar': 2}\n",
    "mapping = {'Sexistas/misóginos': 0, 'Genéricos': 1, 'Deseo de Dañar': 2}\n",
    "\n",
    "# Create a dictionary for renaming\n",
    "rename_dict = {}\n",
    "for key, value in mapping.items():\n",
    "    rename_dict[f'eval_{value}_precision'] = f'eval_{key}_precision'\n",
    "    rename_dict[f'eval_{value}_recall'] = f'eval_{key}_recall'\n",
    "    rename_dict[f'eval_{value}_f1'] = f'eval_{key}_f1-score'\n",
    "    rename_dict[f'eval_{value}_support'] = f'eval_{key}_support'\n",
    "\n",
    "# Rename the columns\n",
    "df_INS_SC.rename(columns=rename_dict, inplace=True)\n",
    "df_INS_TC.rename(columns=rename_dict, inplace=True)\n",
    "df_INS_FC.rename(columns=rename_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir la nueva columna 'Context' a cada DataFrame\n",
    "df_INS_SC['Context'] = 'Without Context'\n",
    "df_INS_TC['Context'] = 'Tweet Context'\n",
    "df_INS_FC['Context'] = 'Full Context'\n",
    "\n",
    "# Concatenar los tres DataFrames\n",
    "df_INS_join = pd.concat([df_INS_SC[['Model_Description', 'Total Time', 'Context', 'eval_weighted_f1', f'eval_{labels[0]}_f1-score', f'eval_{labels[1]}_f1-score', f'eval_{labels[2]}_f1-score', 'eval_accuracy']], \n",
    "                         df_INS_TC[['Model_Description', 'Total Time', 'Context', 'eval_weighted_f1', f'eval_{labels[0]}_f1-score', f'eval_{labels[1]}_f1-score', f'eval_{labels[2]}_f1-score', 'eval_accuracy']], \n",
    "                         df_INS_FC[['Model_Description', 'Total Time', 'Context', 'eval_weighted_f1', f'eval_{labels[0]}_f1-score', f'eval_{labels[1]}_f1-score', f'eval_{labels[2]}_f1-score', 'eval_accuracy']]])\n",
    "\n",
    "df_INS_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to include only numeric columns\n",
    "numeric_cols = ['eval_weighted_f1', f'eval_{labels[0]}_f1-score', f'eval_{labels[1]}_f1-score', f'eval_{labels[2]}_f1-score', 'eval_accuracy', 'Total Time']\n",
    "\n",
    "# Group the data by 'Context' and calculate the mean for numeric columns\n",
    "context_grouped_INS = df_INS_join.groupby('Context')[numeric_cols].max().reset_index()\n",
    "\n",
    "# Reorder the contexts\n",
    "context_grouped_INS['Context'] = pd.Categorical(context_grouped_INS['Context'], categories=['Without Context', 'Tweet Context', 'Full Context'], ordered=True)\n",
    "\n",
    "context_grouped_INS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate the upper limit for the y-axis dynamically\n",
    "y_max = 1.05\n",
    "\n",
    "# Create a figure with subplots using GridSpec\n",
    "fig = plt.figure(figsize=(24, 12))\n",
    "gs = GridSpec(1, 1, figure=fig)\n",
    "\n",
    "# Plot\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "# Plot lines\n",
    "sns.lineplot(ax=ax1, x='Context', y='eval_weighted_f1', data=context_grouped_INS, marker='o', linewidth=14, markersize=26, alpha=0.7, color=\"black\", label=\"Weighted F1-Score\")\n",
    "for i, label in enumerate(labels):\n",
    "    sns.lineplot(ax=ax1, x='Context', y=f'eval_{label}_f1-score', data=context_grouped_INS, marker='o', linewidth=10, markersize=24, alpha=0.7, color=colors[i], label=f\"{label} F1-Score\")\n",
    "\n",
    "ax1.set_title('Maximum Weighted F1-Score for Different Contexts', fontsize=42)\n",
    "ax1.set_xlabel('', fontsize=18)\n",
    "ax1.set_ylabel('F1-Score', fontsize=30)\n",
    "ax1.tick_params(axis='x', labelsize=30)\n",
    "ax1.tick_params(axis='y', labelsize=22)\n",
    "ax1.set_ylim(0.01, 0.8)\n",
    "\n",
    "# Move legend to the top\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=4, fontsize=22)\n",
    "\n",
    "# Add a grid to improve readability\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Remove spines for a clean look\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['left'].set_visible(False)\n",
    "ax1.spines['bottom'].set_visible(False)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"../../../IMAGES/Insultos/DL/DL_Experiments in _Insultos_ weightedF1_max.pdf\", format='pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate the upper limit for the y-axis dynamically\n",
    "y_max = df_INS_join['eval_weighted_f1'].mean() + 4 * df_INS_join['eval_weighted_f1'].std()\n",
    "\n",
    "# Create a figure with subplots using GridSpec\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "gs = GridSpec(1, 1, figure=fig)\n",
    "\n",
    "# Plot for MODELS\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.barplot(ax=ax1, x='Context', y='eval_weighted_f1', hue='Model_Description', data=df_INS_join, palette='flare', alpha=0.7, ci=None)\n",
    "ax1.set_title('Weighted F1-Score for Different Contexts', fontsize=30)\n",
    "ax1.set_xlabel('', fontsize=18)\n",
    "ax1.set_ylabel('F1-Score', fontsize=18)\n",
    "ax1.tick_params(axis='x', labelsize=18)\n",
    "ax1.tick_params(axis='y', labelsize=18)\n",
    "ax1.set_ylim(0, y_max)\n",
    "\n",
    "# Remove grid\n",
    "ax1.grid(False)\n",
    "\n",
    "# Move legend to the top\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=5, fontsize=15)\n",
    "\n",
    "# Adding grid for better readability\n",
    "ax1.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance (Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate the upper limit for the y-axis dynamically\n",
    "y_max = df_INS_join['Total Time'].mean() + 4 * df_INS_join['Total Time'].std()\n",
    "\n",
    "# Create a figure with subplots using GridSpec\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "gs = GridSpec(1, 1, figure=fig)\n",
    "\n",
    "# Plot for MODELS\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.barplot(ax=ax1, x='Context', y='Total Time', hue='Model_Description', data=df_INS_join, palette='flare', alpha=0.7, ci=None)\n",
    "ax1.set_title('Average Performance Time for Different Contexts', fontsize=30)\n",
    "ax1.set_xlabel('', fontsize=18)\n",
    "ax1.set_ylabel('Average Time (s)', fontsize=18)\n",
    "ax1.tick_params(axis='x', labelsize=18)\n",
    "ax1.tick_params(axis='y', labelsize=18)\n",
    "ax1.set_ylim(0, y_max)\n",
    "\n",
    "# Move legend to the top\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=5, fontsize=15)\n",
    "\n",
    "\n",
    "# Adding grid for better readability\n",
    "ax1.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_INS_join[['eval_weighted_f1', f'eval_{labels[0]}_f1-score', f'eval_{labels[1]}_f1-score', f'eval_{labels[2]}_f1-score', 'eval_accuracy', 'Total Time']].corr()\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a heatmap for the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "heatmap = sns.heatmap(correlation_matrix, annot=True, cmap='flare', fmt='.2f', linewidths=0.5, linecolor='black')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Correlation Matrix of Performance Metrics', fontsize=20)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confussion Matrix - Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# ID del modelo para el que quieres extraer la matriz de confusión\n",
    "model_id = '04f6baca-f95e-47d0-83f3-8c737b1f1c46'\n",
    "\n",
    "# Extraer la matriz de confusión del DataFrame usando el ID\n",
    "conf_matrix_str = df_INS_SC.loc[df_INS_SC['Run_ID'] == model_id, 'eval_confusion_matrix'].values[0]\n",
    "conf_matrix = ast.literal_eval(conf_matrix_str)\n",
    "\n",
    "# Crear la matriz de confusión\n",
    "cm = pd.DataFrame(conf_matrix, index=labels, columns=labels)\n",
    "\n",
    "# Plotear la matriz de confusión\n",
    "plt.figure(figsize=(24, 22))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='flare', xticklabels=labels, \n",
    "            yticklabels=labels, annot_kws={\"size\": 52}, cbar=False)\n",
    "\n",
    "plt.xlabel('Predicted label', fontsize=30)\n",
    "plt.ylabel('True label', fontsize=30)\n",
    "plt.xticks(rotation=45, fontsize=46)\n",
    "plt.yticks(rotation=0, fontsize=46)\n",
    "# plt.title('Confusion Matrix for Best Model of ML Experiments')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"../../../IMAGES/Insultos/DL/DL_Experiments in _Insultos_ best_model.pdf\", format='pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'Sexistas/misóginos': 0, 'Genéricos': 1, 'Deseo de Dañar': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar df_AG_join, df_CNEG_join y df_INS_join\n",
    "df_all = pd.concat([df_AG_join.assign(Task='Análisis General'), \n",
    "                    df_CNEG_join.assign(Task='Contenido Negativo'), \n",
    "                    df_INS_join.assign(Task='Insultos')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary for task names\n",
    "task_name_mapping = {\n",
    "    \"analisis_general\": \"Análisis General\", \n",
    "    \"contenido_negativo\": \"Contenido Negativo\", \n",
    "    \"insultos\": \"Insultos\"\n",
    "}\n",
    "\n",
    "# Agrupar el DataFrame `df_all` por 'Type' y calcular la suma, promedio y número de experimentos del 'Time (s)'\n",
    "df_all_grouped_sum = df_all.groupby(['Task'])['Total Time'].sum().reset_index()\n",
    "df_all_grouped_mean = df_all.groupby(['Task'])['Total Time'].mean().reset_index()\n",
    "df_all_grouped_count = df_all.groupby(['Task'])['Total Time'].count().reset_index()\n",
    "\n",
    "# Fusionar las agrupaciones en un solo DataFrame\n",
    "df_all_grouped = df_all_grouped_sum.merge(df_all_grouped_mean, on='Task', suffixes=('_Total', '_Mean'))\n",
    "df_all_grouped = df_all_grouped.merge(df_all_grouped_count, on='Task')\n",
    "\n",
    "# Renombrar las columnas para mayor claridad\n",
    "df_all_grouped.columns = ['Task', 'Total Time (s)', 'Mean Time (s)', 'Number of Experiments']\n",
    "\n",
    "# Convert seconds to hours and round to 3 decimal places\n",
    "df_all_grouped['Total Time (h)'] = (df_all_grouped['Total Time (s)'] / 3600).round(3)\n",
    "df_all_grouped['Mean Time (h)'] = (df_all_grouped['Mean Time (s)'] / 3600).round(3)\n",
    "\n",
    "# Apply the mapping to change the task names\n",
    "df_all_grouped['Task'] = df_all_grouped['Task'].replace(task_name_mapping)\n",
    "\n",
    "# Drop the original time in seconds columns if not needed\n",
    "df_all_grouped = df_all_grouped[['Task', 'Total Time (h)', 'Mean Time (h)', 'Number of Experiments']]\n",
    "\n",
    "df_all_grouped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Maximum per limit for the y-axis between ML, DL and GenAI\n",
    "y_max = 750\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, ax = plt.subplots(figsize=(24, 8))\n",
    "\n",
    "# Plot for Total Time usando los colores personalizados\n",
    "sns.barplot(ax=ax, x='Task', y='Number of Experiments', data=df_all_grouped, palette='bone', alpha=0.9)\n",
    "\n",
    "# Set plot title and labels\n",
    "ax.set_title('Total Number of Experiments per Classification Task in DL Experiments', fontsize=30)\n",
    "ax.set_xlabel('', fontsize=18)\n",
    "ax.set_ylabel('Time (h)', fontsize=18)\n",
    "ax.tick_params(axis='x', labelsize=18)\n",
    "ax.tick_params(axis='y', labelsize=18)\n",
    "ax.set_ylim(0, y_max)\n",
    "\n",
    "\n",
    "# Adding grid for better readability\n",
    "ax.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Maximum pper limit for the y-axis between ML, DL and GenAI\n",
    "y_max = 75\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, ax = plt.subplots(figsize=(24, 8))\n",
    "\n",
    "# Plot for Total Time usando los colores personalizados\n",
    "sns.barplot(ax=ax, x='Task', y='Total Time (h)', data=df_all_grouped, palette='bone', alpha=0.9)\n",
    "\n",
    "# Set plot title and labels\n",
    "ax.set_title('Total Performance Time per Classification Task in DL Experiments', fontsize=30)\n",
    "ax.set_xlabel('', fontsize=18)\n",
    "ax.set_ylabel('Time (h)', fontsize=18)\n",
    "ax.tick_params(axis='x', labelsize=18)\n",
    "ax.tick_params(axis='y', labelsize=18)\n",
    "ax.set_ylim(0, y_max)\n",
    "\n",
    "\n",
    "# Adding grid for better readability\n",
    "ax.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Maximum per limit for the y-axis between ML, DL and GenAI\n",
    "y_max = 75\n",
    "\n",
    "# Define colors for specific tasks\n",
    "task_colors = {\n",
    "    'Análisis General': '#8E809E',\n",
    "    'Contenido Negativo': '#6AA6D4',\n",
    "    'Insultos': '#E38A83'\n",
    "}\n",
    "\n",
    "# Create a list of colors for the bars based on the task\n",
    "bar_colors = [task_colors.get(task, '#CCCCCC') for task in df_all_grouped['Task']]  # Default color is light grey if not specified\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, ax = plt.subplots(figsize=(24, 8))\n",
    "\n",
    "# Plot for Total Time using the customized colors\n",
    "sns.barplot(ax=ax, x='Task', y='Total Time (h)', data=df_all_grouped, palette=bar_colors, alpha=0.9)\n",
    "\n",
    "# Set plot title and labels\n",
    "# ax.set_title('Total Performance Time per Classification Task in ML Experiments', fontsize=40)\n",
    "ax.set_xlabel('', fontsize=20)\n",
    "ax.set_ylabel('Time (h)', fontsize=30)\n",
    "ax.tick_params(axis='x', labelsize=34)\n",
    "ax.tick_params(axis='y', labelsize=34)\n",
    "ax.set_ylim(0, y_max)\n",
    "\n",
    "# Add value labels on top of the bars in the same color as the bars\n",
    "for bar, color in zip(ax.patches, bar_colors):\n",
    "    yval = bar.get_height()\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2, yval + 1, f'{yval:.3f}',\n",
    "        ha='center', va='bottom', fontsize=34, fontweight='bold',\n",
    "        color=color  # Match text color with the bar color\n",
    "    )\n",
    "\n",
    "# Add a grid to improve readability\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Remove spines for a clean look\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "# Adjust layout to avoid clipping of labels and titles\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(\"../../../IMAGES/DL_total_performance_time.pdf\", format='pdf')\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig(\"../../../IMAGES/DL_total_performance_time.png\", format='png', transparent=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Maximum pper limit for the y-axis between ML, DL and GenAI\n",
    "y_max = 6\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, ax = plt.subplots(figsize=(24, 8))\n",
    "\n",
    "# Plot for Total Time usando los colores personalizados\n",
    "sns.barplot(ax=ax, x='Task', y='Mean Time (h)', data=df_all_grouped, palette='bone', alpha=0.9)\n",
    "\n",
    "# Set plot title and labels\n",
    "ax.set_title('Mean Performance Time per Classification Task in DL Experiments', fontsize=30)\n",
    "ax.set_xlabel('', fontsize=18)\n",
    "ax.set_ylabel('Time (s)', fontsize=18)\n",
    "ax.tick_params(axis='x', labelsize=18)\n",
    "ax.tick_params(axis='y', labelsize=18)\n",
    "ax.set_ylim(0, y_max)\n",
    "\n",
    "\n",
    "# Adding grid for better readability\n",
    "ax.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
