{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id_EXIST lang                                              tweet  \\\n",
      "0      100001   es  @TheChiflis Ignora al otro, es un capullo.El p...   \n",
      "1      100002   es  @ultimonomada_ Si comicsgate se parece en algo...   \n",
      "2      100003   es  @Steven2897 Lee sobre Gamergate, y como eso ha...   \n",
      "3      100004   es  @Lunariita7 Un retraso social bastante lamenta...   \n",
      "4      100005   es  @novadragon21 @icep4ck @TvDannyZ Entonces como...   \n",
      "...       ...  ...                                                ...   \n",
      "6915   203256   en  idk why y’all bitches think having half your a...   \n",
      "6916   203257   en  This has been a part of an experiment with @Wo...   \n",
      "6917   203258   en  \"Take me already\" \"Not yet. You gotta be ready...   \n",
      "6918   203259   en  @clintneedcoffee why do you look like a whore?...   \n",
      "6919   203260   en  ik when mandy says “you look like a whore” i l...   \n",
      "\n",
      "      number_annotators                                         annotators  \\\n",
      "0                     6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
      "1                     6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
      "2                     6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
      "3                     6  [Annotator_13, Annotator_14, Annotator_15, Ann...   \n",
      "4                     6  [Annotator_19, Annotator_20, Annotator_21, Ann...   \n",
      "...                 ...                                                ...   \n",
      "6915                  6  [Annotator_478, Annotator_479, Annotator_480, ...   \n",
      "6916                  6  [Annotator_668, Annotator_669, Annotator_670, ...   \n",
      "6917                  6  [Annotator_467, Annotator_468, Annotator_469, ...   \n",
      "6918                  6  [Annotator_674, Annotator_675, Annotator_676, ...   \n",
      "6919                  6  [Annotator_473, Annotator_474, Annotator_475, ...   \n",
      "\n",
      "       gender_annotators                          age_annotators  \\\n",
      "0     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
      "1     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
      "2     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
      "3     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
      "4     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
      "...                  ...                                     ...   \n",
      "6915  [F, F, M, M, M, F]  [18-22, 23-45, 18-22, 23-45, 46+, 46+]   \n",
      "6916  [F, F, M, M, M, F]  [18-22, 23-45, 18-22, 23-45, 46+, 46+]   \n",
      "6917  [F, F, M, M, M, F]  [18-22, 23-45, 18-22, 23-45, 46+, 46+]   \n",
      "6918  [F, F, M, M, M, F]  [18-22, 23-45, 18-22, 23-45, 46+, 46+]   \n",
      "6919  [F, F, M, M, M, F]  [18-22, 23-45, 18-22, 23-45, 46+, 46+]   \n",
      "\n",
      "                                 ethnicities_annotators  \\\n",
      "0     [White or Caucasian, Hispano or Latino, White ...   \n",
      "1     [Black or African American, Hispano or Latino,...   \n",
      "2     [Black or African American, Hispano or Latino,...   \n",
      "3     [Hispano or Latino, Hispano or Latino, White o...   \n",
      "4     [Hispano or Latino, Hispano or Latino, White o...   \n",
      "...                                                 ...   \n",
      "6915  [White or Caucasian, Black or African American...   \n",
      "6916  [Hispano or Latino, other, White or Caucasian,...   \n",
      "6917  [White or Caucasian, White or Caucasian, White...   \n",
      "6918  [Black or African American, Black or African A...   \n",
      "6919  [Hispano or Latino, White or Caucasian, White ...   \n",
      "\n",
      "                                study_levels_annotators  \\\n",
      "0     [Bachelor’s degree, Bachelor’s degree, High sc...   \n",
      "1     [High school degree or equivalent, Bachelor’s ...   \n",
      "2     [High school degree or equivalent, Bachelor’s ...   \n",
      "3     [High school degree or equivalent, Bachelor’s ...   \n",
      "4     [Bachelor’s degree, Bachelor’s degree, Master’...   \n",
      "...                                                 ...   \n",
      "6915  [High school degree or equivalent, Bachelor’s ...   \n",
      "6916  [High school degree or equivalent, Master’s de...   \n",
      "6917  [High school degree or equivalent, Bachelor’s ...   \n",
      "6918  [High school degree or equivalent, Bachelor’s ...   \n",
      "6919  [High school degree or equivalent, Bachelor’s ...   \n",
      "\n",
      "                                   countries_annotators  \\\n",
      "0     [Italy, Mexico, United States, Spain, Spain, C...   \n",
      "1     [United Kingdom, Mexico, United States, Portug...   \n",
      "2     [United Kingdom, Mexico, United States, Portug...   \n",
      "3        [Mexico, Chile, Spain, Spain, Portugal, Spain]   \n",
      "4     [Mexico, Afghanistan, United States, Italy, Po...   \n",
      "...                                                 ...   \n",
      "6915  [Hungary, South Africa, Chile, Portugal, Unite...   \n",
      "6916  [Mexico, Algeria, Portugal, Spain, United King...   \n",
      "6917  [Poland, Poland, Portugal, Canada, United King...   \n",
      "6918  [South Africa, South Africa, Portugal, Portuga...   \n",
      "6919  [Mexico, Portugal, Poland, Hungary, United Kin...   \n",
      "\n",
      "                        labels_task1  \\\n",
      "0      [YES, YES, NO, YES, YES, YES]   \n",
      "1          [NO, NO, NO, NO, YES, NO]   \n",
      "2           [NO, NO, NO, NO, NO, NO]   \n",
      "3        [NO, NO, YES, NO, YES, YES]   \n",
      "4       [YES, NO, YES, NO, YES, YES]   \n",
      "...                              ...   \n",
      "6915  [YES, YES, YES, YES, YES, YES]   \n",
      "6916  [YES, YES, YES, YES, YES, YES]   \n",
      "6917    [NO, YES, NO, YES, YES, YES]   \n",
      "6918  [YES, YES, YES, YES, YES, YES]   \n",
      "6919   [YES, YES, YES, NO, YES, YES]   \n",
      "\n",
      "                                           labels_task2  \\\n",
      "0     [REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...   \n",
      "1                               [-, -, -, -, DIRECT, -]   \n",
      "2                                    [-, -, -, -, -, -]   \n",
      "3                 [-, -, DIRECT, -, REPORTED, REPORTED]   \n",
      "4     [REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...   \n",
      "...                                                 ...   \n",
      "6915  [JUDGEMENTAL, DIRECT, DIRECT, DIRECT, JUDGEMEN...   \n",
      "6916  [JUDGEMENTAL, REPORTED, JUDGEMENTAL, DIRECT, J...   \n",
      "6917        [-, DIRECT, -, DIRECT, DIRECT, JUDGEMENTAL]   \n",
      "6918  [DIRECT, DIRECT, DIRECT, DIRECT, JUDGEMENTAL, ...   \n",
      "6919  [DIRECT, DIRECT, REPORTED, -, JUDGEMENTAL, REP...   \n",
      "\n",
      "                                           labels_task3     split  \n",
      "0     [[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...  TRAIN_ES  \n",
      "1          [[-], [-], [-], [-], [OBJECTIFICATION], [-]]  TRAIN_ES  \n",
      "2                        [[-], [-], [-], [-], [-], [-]]  TRAIN_ES  \n",
      "3     [[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...  TRAIN_ES  \n",
      "4     [[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...  TRAIN_ES  \n",
      "...                                                 ...       ...  \n",
      "6915  [[OBJECTIFICATION], [STEREOTYPING-DOMINANCE, S...  TRAIN_EN  \n",
      "6916  [[OBJECTIFICATION], [OBJECTIFICATION], [OBJECT...  TRAIN_EN  \n",
      "6917  [[-], [OBJECTIFICATION], [-], [SEXUAL-VIOLENCE...  TRAIN_EN  \n",
      "6918  [[OBJECTIFICATION, SEXUAL-VIOLENCE, MISOGYNY-N...  TRAIN_EN  \n",
      "6919  [[STEREOTYPING-DOMINANCE], [OBJECTIFICATION], ...  TRAIN_EN  \n",
      "\n",
      "[6920 rows x 14 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>number_annotators</th>\n",
       "      <th>annotators</th>\n",
       "      <th>gender_annotators</th>\n",
       "      <th>age_annotators</th>\n",
       "      <th>ethnicities_annotators</th>\n",
       "      <th>study_levels_annotators</th>\n",
       "      <th>countries_annotators</th>\n",
       "      <th>labels_task1</th>\n",
       "      <th>labels_task2</th>\n",
       "      <th>labels_task3</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>es</td>\n",
       "      <td>@TheChiflis Ignora al otro, es un capullo.El p...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_1, Annotator_2, Annotator_3, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[White or Caucasian, Hispano or Latino, White ...</td>\n",
       "      <td>[Bachelor’s degree, Bachelor’s degree, High sc...</td>\n",
       "      <td>[Italy, Mexico, United States, Spain, Spain, C...</td>\n",
       "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
       "      <td>[REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...</td>\n",
       "      <td>[[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100002</td>\n",
       "      <td>es</td>\n",
       "      <td>@ultimonomada_ Si comicsgate se parece en algo...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[Black or African American, Hispano or Latino,...</td>\n",
       "      <td>[High school degree or equivalent, Bachelor’s ...</td>\n",
       "      <td>[United Kingdom, Mexico, United States, Portug...</td>\n",
       "      <td>[NO, NO, NO, NO, YES, NO]</td>\n",
       "      <td>[-, -, -, -, DIRECT, -]</td>\n",
       "      <td>[[-], [-], [-], [-], [OBJECTIFICATION], [-]]</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100003</td>\n",
       "      <td>es</td>\n",
       "      <td>@Steven2897 Lee sobre Gamergate, y como eso ha...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[Black or African American, Hispano or Latino,...</td>\n",
       "      <td>[High school degree or equivalent, Bachelor’s ...</td>\n",
       "      <td>[United Kingdom, Mexico, United States, Portug...</td>\n",
       "      <td>[NO, NO, NO, NO, NO, NO]</td>\n",
       "      <td>[-, -, -, -, -, -]</td>\n",
       "      <td>[[-], [-], [-], [-], [-], [-]]</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100004</td>\n",
       "      <td>es</td>\n",
       "      <td>@Lunariita7 Un retraso social bastante lamenta...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_13, Annotator_14, Annotator_15, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[Hispano or Latino, Hispano or Latino, White o...</td>\n",
       "      <td>[High school degree or equivalent, Bachelor’s ...</td>\n",
       "      <td>[Mexico, Chile, Spain, Spain, Portugal, Spain]</td>\n",
       "      <td>[NO, NO, YES, NO, YES, YES]</td>\n",
       "      <td>[-, -, DIRECT, -, REPORTED, REPORTED]</td>\n",
       "      <td>[[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100005</td>\n",
       "      <td>es</td>\n",
       "      <td>@novadragon21 @icep4ck @TvDannyZ Entonces como...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_19, Annotator_20, Annotator_21, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[Hispano or Latino, Hispano or Latino, White o...</td>\n",
       "      <td>[Bachelor’s degree, Bachelor’s degree, Master’...</td>\n",
       "      <td>[Mexico, Afghanistan, United States, Italy, Po...</td>\n",
       "      <td>[YES, NO, YES, NO, YES, YES]</td>\n",
       "      <td>[REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...</td>\n",
       "      <td>[[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>203256</td>\n",
       "      <td>en</td>\n",
       "      <td>idk why y’all bitches think having half your a...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_478, Annotator_479, Annotator_480, ...</td>\n",
       "      <td>[F, F, M, M, M, F]</td>\n",
       "      <td>[18-22, 23-45, 18-22, 23-45, 46+, 46+]</td>\n",
       "      <td>[White or Caucasian, Black or African American...</td>\n",
       "      <td>[High school degree or equivalent, Bachelor’s ...</td>\n",
       "      <td>[Hungary, South Africa, Chile, Portugal, Unite...</td>\n",
       "      <td>[YES, YES, YES, YES, YES, YES]</td>\n",
       "      <td>[JUDGEMENTAL, DIRECT, DIRECT, DIRECT, JUDGEMEN...</td>\n",
       "      <td>[[OBJECTIFICATION], [STEREOTYPING-DOMINANCE, S...</td>\n",
       "      <td>TRAIN_EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>203257</td>\n",
       "      <td>en</td>\n",
       "      <td>This has been a part of an experiment with @Wo...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_668, Annotator_669, Annotator_670, ...</td>\n",
       "      <td>[F, F, M, M, M, F]</td>\n",
       "      <td>[18-22, 23-45, 18-22, 23-45, 46+, 46+]</td>\n",
       "      <td>[Hispano or Latino, other, White or Caucasian,...</td>\n",
       "      <td>[High school degree or equivalent, Master’s de...</td>\n",
       "      <td>[Mexico, Algeria, Portugal, Spain, United King...</td>\n",
       "      <td>[YES, YES, YES, YES, YES, YES]</td>\n",
       "      <td>[JUDGEMENTAL, REPORTED, JUDGEMENTAL, DIRECT, J...</td>\n",
       "      <td>[[OBJECTIFICATION], [OBJECTIFICATION], [OBJECT...</td>\n",
       "      <td>TRAIN_EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>203258</td>\n",
       "      <td>en</td>\n",
       "      <td>\"Take me already\" \"Not yet. You gotta be ready...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_467, Annotator_468, Annotator_469, ...</td>\n",
       "      <td>[F, F, M, M, M, F]</td>\n",
       "      <td>[18-22, 23-45, 18-22, 23-45, 46+, 46+]</td>\n",
       "      <td>[White or Caucasian, White or Caucasian, White...</td>\n",
       "      <td>[High school degree or equivalent, Bachelor’s ...</td>\n",
       "      <td>[Poland, Poland, Portugal, Canada, United King...</td>\n",
       "      <td>[NO, YES, NO, YES, YES, YES]</td>\n",
       "      <td>[-, DIRECT, -, DIRECT, DIRECT, JUDGEMENTAL]</td>\n",
       "      <td>[[-], [OBJECTIFICATION], [-], [SEXUAL-VIOLENCE...</td>\n",
       "      <td>TRAIN_EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>203259</td>\n",
       "      <td>en</td>\n",
       "      <td>@clintneedcoffee why do you look like a whore?...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_674, Annotator_675, Annotator_676, ...</td>\n",
       "      <td>[F, F, M, M, M, F]</td>\n",
       "      <td>[18-22, 23-45, 18-22, 23-45, 46+, 46+]</td>\n",
       "      <td>[Black or African American, Black or African A...</td>\n",
       "      <td>[High school degree or equivalent, Bachelor’s ...</td>\n",
       "      <td>[South Africa, South Africa, Portugal, Portuga...</td>\n",
       "      <td>[YES, YES, YES, YES, YES, YES]</td>\n",
       "      <td>[DIRECT, DIRECT, DIRECT, DIRECT, JUDGEMENTAL, ...</td>\n",
       "      <td>[[OBJECTIFICATION, SEXUAL-VIOLENCE, MISOGYNY-N...</td>\n",
       "      <td>TRAIN_EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>203260</td>\n",
       "      <td>en</td>\n",
       "      <td>ik when mandy says “you look like a whore” i l...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_473, Annotator_474, Annotator_475, ...</td>\n",
       "      <td>[F, F, M, M, M, F]</td>\n",
       "      <td>[18-22, 23-45, 18-22, 23-45, 46+, 46+]</td>\n",
       "      <td>[Hispano or Latino, White or Caucasian, White ...</td>\n",
       "      <td>[High school degree or equivalent, Bachelor’s ...</td>\n",
       "      <td>[Mexico, Portugal, Poland, Hungary, United Kin...</td>\n",
       "      <td>[YES, YES, YES, NO, YES, YES]</td>\n",
       "      <td>[DIRECT, DIRECT, REPORTED, -, JUDGEMENTAL, REP...</td>\n",
       "      <td>[[STEREOTYPING-DOMINANCE], [OBJECTIFICATION], ...</td>\n",
       "      <td>TRAIN_EN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6920 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_EXIST lang                                              tweet  \\\n",
       "0      100001   es  @TheChiflis Ignora al otro, es un capullo.El p...   \n",
       "1      100002   es  @ultimonomada_ Si comicsgate se parece en algo...   \n",
       "2      100003   es  @Steven2897 Lee sobre Gamergate, y como eso ha...   \n",
       "3      100004   es  @Lunariita7 Un retraso social bastante lamenta...   \n",
       "4      100005   es  @novadragon21 @icep4ck @TvDannyZ Entonces como...   \n",
       "...       ...  ...                                                ...   \n",
       "6915   203256   en  idk why y’all bitches think having half your a...   \n",
       "6916   203257   en  This has been a part of an experiment with @Wo...   \n",
       "6917   203258   en  \"Take me already\" \"Not yet. You gotta be ready...   \n",
       "6918   203259   en  @clintneedcoffee why do you look like a whore?...   \n",
       "6919   203260   en  ik when mandy says “you look like a whore” i l...   \n",
       "\n",
       "      number_annotators                                         annotators  \\\n",
       "0                     6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
       "1                     6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
       "2                     6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
       "3                     6  [Annotator_13, Annotator_14, Annotator_15, Ann...   \n",
       "4                     6  [Annotator_19, Annotator_20, Annotator_21, Ann...   \n",
       "...                 ...                                                ...   \n",
       "6915                  6  [Annotator_478, Annotator_479, Annotator_480, ...   \n",
       "6916                  6  [Annotator_668, Annotator_669, Annotator_670, ...   \n",
       "6917                  6  [Annotator_467, Annotator_468, Annotator_469, ...   \n",
       "6918                  6  [Annotator_674, Annotator_675, Annotator_676, ...   \n",
       "6919                  6  [Annotator_473, Annotator_474, Annotator_475, ...   \n",
       "\n",
       "       gender_annotators                          age_annotators  \\\n",
       "0     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "1     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "2     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "3     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "4     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "...                  ...                                     ...   \n",
       "6915  [F, F, M, M, M, F]  [18-22, 23-45, 18-22, 23-45, 46+, 46+]   \n",
       "6916  [F, F, M, M, M, F]  [18-22, 23-45, 18-22, 23-45, 46+, 46+]   \n",
       "6917  [F, F, M, M, M, F]  [18-22, 23-45, 18-22, 23-45, 46+, 46+]   \n",
       "6918  [F, F, M, M, M, F]  [18-22, 23-45, 18-22, 23-45, 46+, 46+]   \n",
       "6919  [F, F, M, M, M, F]  [18-22, 23-45, 18-22, 23-45, 46+, 46+]   \n",
       "\n",
       "                                 ethnicities_annotators  \\\n",
       "0     [White or Caucasian, Hispano or Latino, White ...   \n",
       "1     [Black or African American, Hispano or Latino,...   \n",
       "2     [Black or African American, Hispano or Latino,...   \n",
       "3     [Hispano or Latino, Hispano or Latino, White o...   \n",
       "4     [Hispano or Latino, Hispano or Latino, White o...   \n",
       "...                                                 ...   \n",
       "6915  [White or Caucasian, Black or African American...   \n",
       "6916  [Hispano or Latino, other, White or Caucasian,...   \n",
       "6917  [White or Caucasian, White or Caucasian, White...   \n",
       "6918  [Black or African American, Black or African A...   \n",
       "6919  [Hispano or Latino, White or Caucasian, White ...   \n",
       "\n",
       "                                study_levels_annotators  \\\n",
       "0     [Bachelor’s degree, Bachelor’s degree, High sc...   \n",
       "1     [High school degree or equivalent, Bachelor’s ...   \n",
       "2     [High school degree or equivalent, Bachelor’s ...   \n",
       "3     [High school degree or equivalent, Bachelor’s ...   \n",
       "4     [Bachelor’s degree, Bachelor’s degree, Master’...   \n",
       "...                                                 ...   \n",
       "6915  [High school degree or equivalent, Bachelor’s ...   \n",
       "6916  [High school degree or equivalent, Master’s de...   \n",
       "6917  [High school degree or equivalent, Bachelor’s ...   \n",
       "6918  [High school degree or equivalent, Bachelor’s ...   \n",
       "6919  [High school degree or equivalent, Bachelor’s ...   \n",
       "\n",
       "                                   countries_annotators  \\\n",
       "0     [Italy, Mexico, United States, Spain, Spain, C...   \n",
       "1     [United Kingdom, Mexico, United States, Portug...   \n",
       "2     [United Kingdom, Mexico, United States, Portug...   \n",
       "3        [Mexico, Chile, Spain, Spain, Portugal, Spain]   \n",
       "4     [Mexico, Afghanistan, United States, Italy, Po...   \n",
       "...                                                 ...   \n",
       "6915  [Hungary, South Africa, Chile, Portugal, Unite...   \n",
       "6916  [Mexico, Algeria, Portugal, Spain, United King...   \n",
       "6917  [Poland, Poland, Portugal, Canada, United King...   \n",
       "6918  [South Africa, South Africa, Portugal, Portuga...   \n",
       "6919  [Mexico, Portugal, Poland, Hungary, United Kin...   \n",
       "\n",
       "                        labels_task1  \\\n",
       "0      [YES, YES, NO, YES, YES, YES]   \n",
       "1          [NO, NO, NO, NO, YES, NO]   \n",
       "2           [NO, NO, NO, NO, NO, NO]   \n",
       "3        [NO, NO, YES, NO, YES, YES]   \n",
       "4       [YES, NO, YES, NO, YES, YES]   \n",
       "...                              ...   \n",
       "6915  [YES, YES, YES, YES, YES, YES]   \n",
       "6916  [YES, YES, YES, YES, YES, YES]   \n",
       "6917    [NO, YES, NO, YES, YES, YES]   \n",
       "6918  [YES, YES, YES, YES, YES, YES]   \n",
       "6919   [YES, YES, YES, NO, YES, YES]   \n",
       "\n",
       "                                           labels_task2  \\\n",
       "0     [REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...   \n",
       "1                               [-, -, -, -, DIRECT, -]   \n",
       "2                                    [-, -, -, -, -, -]   \n",
       "3                 [-, -, DIRECT, -, REPORTED, REPORTED]   \n",
       "4     [REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...   \n",
       "...                                                 ...   \n",
       "6915  [JUDGEMENTAL, DIRECT, DIRECT, DIRECT, JUDGEMEN...   \n",
       "6916  [JUDGEMENTAL, REPORTED, JUDGEMENTAL, DIRECT, J...   \n",
       "6917        [-, DIRECT, -, DIRECT, DIRECT, JUDGEMENTAL]   \n",
       "6918  [DIRECT, DIRECT, DIRECT, DIRECT, JUDGEMENTAL, ...   \n",
       "6919  [DIRECT, DIRECT, REPORTED, -, JUDGEMENTAL, REP...   \n",
       "\n",
       "                                           labels_task3     split  \n",
       "0     [[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...  TRAIN_ES  \n",
       "1          [[-], [-], [-], [-], [OBJECTIFICATION], [-]]  TRAIN_ES  \n",
       "2                        [[-], [-], [-], [-], [-], [-]]  TRAIN_ES  \n",
       "3     [[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...  TRAIN_ES  \n",
       "4     [[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...  TRAIN_ES  \n",
       "...                                                 ...       ...  \n",
       "6915  [[OBJECTIFICATION], [STEREOTYPING-DOMINANCE, S...  TRAIN_EN  \n",
       "6916  [[OBJECTIFICATION], [OBJECTIFICATION], [OBJECT...  TRAIN_EN  \n",
       "6917  [[-], [OBJECTIFICATION], [-], [SEXUAL-VIOLENCE...  TRAIN_EN  \n",
       "6918  [[OBJECTIFICATION, SEXUAL-VIOLENCE, MISOGYNY-N...  TRAIN_EN  \n",
       "6919  [[STEREOTYPING-DOMINANCE], [OBJECTIFICATION], ...  TRAIN_EN  \n",
       "\n",
       "[6920 rows x 14 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#### ARCHIVO SOLO PARA TESTEAR LA CONVERSIÓN DE JSON A CSV\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def load_data(data):\n",
    "    # Carga los datos desde un archivo JSON\n",
    "    with open(data, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Preparar una lista para almacenar los datos aplanados\n",
    "    flat_data = []\n",
    "\n",
    "    # Iterar sobre cada tweet anidado\n",
    "    for tweet_id, tweet_info in data.items():\n",
    "        # Aquí 'tweet_info' contiene todos los datos del tweet individual\n",
    "        flat_data.append(tweet_info)\n",
    "\n",
    "    # Convertir la lista de datos aplanados a un DataFrame\n",
    "    df = pd.DataFrame(flat_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "data = \"EXIST2024_Tweets_Dataset/training/EXIST2024_training.json\" \n",
    "load_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(data):\n",
    "    # Carga los datos desde un archivo JSON\n",
    "    with open(data, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Preparar una lista para almacenar los datos aplanados\n",
    "    flat_data = []\n",
    "\n",
    "    # Iterar sobre cada tweet anidado\n",
    "    for tweet_id, tweet_info in data.items():\n",
    "        # Aquí 'tweet_info' contiene todos los datos del tweet individual\n",
    "        flat_data.append(tweet_info)\n",
    "\n",
    "    # Convertir la lista de datos aplanados a un DataFrame\n",
    "    df = pd.DataFrame(flat_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_target(df, type_id, type_dataset, label_mapping):\n",
    "\n",
    "    def most_frequent_label(labels_list):\n",
    "        if labels_list:\n",
    "            # Obtener las etiquetas más comunes y sus conteos\n",
    "            common_labels = collections.Counter(labels_list).most_common()\n",
    "            if len(common_labels) > 1 and common_labels[0][1] == common_labels[1][1]:\n",
    "                # Hay un empate entre al menos las dos etiquetas más frecuentes\n",
    "                return \"Tie\"\n",
    "            else:\n",
    "                # No hay empate, devolver la etiqueta más frecuente\n",
    "                return common_labels[0][0]\n",
    "        else:\n",
    "            # Manejar listas vacías según sea necesario\n",
    "            return None\n",
    "\n",
    "    #[\"task_1\", \"task_2\", \"task_3\"]\n",
    "    if type_id == \"task_1\":\n",
    "        # Remove NAs\n",
    "        df = df.dropna(subset=['labels_task1'])\n",
    "\n",
    "\n",
    "        if type_dataset == \"train\":\n",
    "\n",
    "            # Aplicar la función a cada fila y crear una nueva columna con la etiqueta más frecuente\n",
    "            df['label'] = df['labels_task1'].apply(most_frequent_label)\n",
    "\n",
    "            # Factorizar la columna de etiquetas más frecuentes\n",
    "            labels, labels_names = pd.factorize(df['label'])\n",
    "            df['label'] = labels\n",
    "\n",
    "            # If you want to keep a record of the mapping from the original labels to the numeric labels\n",
    "            label_mapping = dict(zip(labels_names, range(len(labels_names))))\n",
    "\n",
    "            # Contar el soporte de cada etiqueta\n",
    "            soporte_etiquetas = df['label'].value_counts()\n",
    "\n",
    "            # Imprimir el soporte para cada etiqueta\n",
    "            print(\"\\nSoporte de etiquetas con nombres originales:\")\n",
    "            for nombre_etiqueta, codigo in label_mapping.items():\n",
    "                print(f\"{nombre_etiqueta}: {soporte_etiquetas[codigo]}\")\n",
    "\n",
    "\n",
    "\n",
    "            return df, labels_names, label_mapping\n",
    "\n",
    "        if type_dataset == \"dev\":\n",
    "            \n",
    "            # Aplicar la función a cada fila y crear una nueva columna con la etiqueta más frecuente\n",
    "            df['label'] = df['labels_task1'].apply(most_frequent_label)\n",
    "            df['label'] = df['label'].map(label_mapping)\n",
    "\n",
    "            # Contar el soporte de cada etiqueta\n",
    "            soporte_etiquetas = df['label'].value_counts()\n",
    "\n",
    "            # Imprimir el soporte para cada etiqueta\n",
    "            print(\"\\nSoporte de etiquetas con nombres originales:\")\n",
    "            for nombre_etiqueta, codigo in label_mapping.items():\n",
    "                print(f\"{nombre_etiqueta}: {soporte_etiquetas[codigo]}\")\n",
    "\n",
    "            return df\n",
    "\n",
    "\n",
    "    \n",
    "def process_data(df, type_id):\n",
    "\n",
    "\n",
    "    # Tweets in 'es':\n",
    "    df = df.loc[df['lang'] == 'es']\n",
    "\n",
    "    # Length Tweet\n",
    "    df['tweet_length'] = df['tweet'].str.len()\n",
    "\n",
    "    # Num Adjetives\n",
    "    def count_adjectives(text):\n",
    "        words = word_tokenize(text)\n",
    "        pos_tags = pos_tag(words)\n",
    "        return sum(1 for word, tag in pos_tags if tag.startswith('JJ'))\n",
    "    df['num_adjectives'] = df['tweet'].apply(count_adjectives)\n",
    "\n",
    "\n",
    "    if type_id == \"task_2\": # no modificado\n",
    "\n",
    "        # Filtrar el DataFrame para seleccionar solo los \"Comentario Negativo\"\n",
    "        df = df.loc[df['Análisis General'] == 'Comentario Negativo']\n",
    "\n",
    "        # Define the specific labels to keep\n",
    "        etiquetas = [\"Desprestigiar Víctima\", \"Desprestigiar Acto\", \"Insultos\", \"Desprestigiar Deportista Autora\"]\n",
    "        df['Contenido Negativo'] = df['Contenido Negativo'].where(df['Contenido Negativo'].isin(etiquetas))\n",
    "\n",
    "        # Remove NAs\n",
    "        df = df.dropna(subset=['Contenido Negativo'])\n",
    "        \n",
    "\n",
    "        # Factorize the 'Análisis General' column\n",
    "        labels, labels_names = pd.factorize(df['Contenido Negativo'])\n",
    "\n",
    "        # 'labels' now contains the numeric representation of your original labels\n",
    "        # 'label_names' contains the unique values from your original column in the order they were encoded\n",
    "\n",
    "        # Replace the original column with the numeric labels\n",
    "        df['label'] = labels\n",
    "\n",
    "        # If you want to keep a record of the mapping from the original labels to the numeric labels\n",
    "        label_mapping = dict(zip(labels_names, range(len(labels_names))))\n",
    "        #print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "\n",
    "    if type_id == \"task_3\": # no modificado\n",
    "\n",
    "        # Filtrar el DataFrame para seleccionar solo los \"Comentario Negativo\"\n",
    "        df = df.loc[df['Análisis General'] == 'Comentario Negativo']\n",
    "\n",
    "        # Define the specific labels to keep\n",
    "        etiquetas = [\"Deseo de Dañar\", \"Genéricos\", \"Sexistas/misóginos\", \"\"]\n",
    "\n",
    "        # Replace labels that are not in the list with \"Genéricos\"\n",
    "        df['Insultos'] = df['Insultos'].where(df['Insultos'].isin(etiquetas), other=\"Genéricos\")\n",
    "\n",
    "        # Remove NAs\n",
    "        df = df.dropna(subset=['Insultos'])\n",
    "        \n",
    "\n",
    "        # Factorize the 'Insultos' column\n",
    "        labels, labels_names = pd.factorize(df['Insultos'])\n",
    "\n",
    "        # 'labels' now contains the numeric representation of your original labels\n",
    "        # 'label_names' contains the unique values from your original column in the order they were encoded\n",
    "\n",
    "        # Replace the original column with the numeric labels\n",
    "        df['label'] = labels\n",
    "\n",
    "        # If you want to keep a record of the mapping from the original labels to the numeric labels\n",
    "        label_mapping = dict(zip(labels_names, range(len(labels_names))))\n",
    "        #print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Initialize stemmer\n",
    "    ##stemmer = SnowballStemmer('spanish')\n",
    "    \n",
    "    # Define function to remove stopwords, punctuation, and apply stemming\n",
    "    def remove_spanish_stopwords(text):\n",
    "\n",
    "        # Eliminar menciones a usuarios (palabras que comienzan con @)\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "        # Eliminar enlaces (todo lo que comienza con http o https)\n",
    "        #text = re.sub(r'http\\S+|https\\S+', '', text)\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = ''.join([char for char in text if char not in string.punctuation])\n",
    "        \n",
    "        # Remove stopwords\n",
    "        spanish_stopwords = set(stopwords.words('spanish'))\n",
    "        spanish_stopwords.remove(\"no\")  # Retain \"no\" as it provides negative context\n",
    "        words = word_tokenize(text)\n",
    "        filtered_words = [word.lower() for word in words if word.lower() not in spanish_stopwords]\n",
    "        \n",
    "        # Apply stemming (MIRAR)\n",
    "        ##stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "        \n",
    "        ##return ' '.join(stemmed_words)  \n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    df['tweet_processed'] = df['tweet'].apply(remove_spanish_stopwords)\n",
    "    # Eliminar filas donde 'tweet_processed' es una cadena vacía\n",
    "    df = df[df['tweet_processed'] != \"\"]\n",
    "\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Soporte de etiquetas con nombres originales:\n",
      "YES: 2697\n",
      "NO: 3367\n",
      "Tie: 856\n",
      "\n",
      "Soporte de etiquetas con nombres originales:\n",
      "YES: 455\n",
      "NO: 479\n",
      "Tie: 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorge\\AppData\\Local\\Temp\\ipykernel_9468\\349672268.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tweet_length'] = df['tweet'].str.len()\n",
      "C:\\Users\\jorge\\AppData\\Local\\Temp\\ipykernel_9468\\349672268.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['num_adjectives'] = df['tweet'].apply(count_adjectives)\n",
      "C:\\Users\\jorge\\AppData\\Local\\Temp\\ipykernel_9468\\349672268.py:195: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tweet_processed'] = df['tweet'].apply(remove_spanish_stopwords)\n",
      "C:\\Users\\jorge\\AppData\\Local\\Temp\\ipykernel_9468\\349672268.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tweet_length'] = df['tweet'].str.len()\n",
      "C:\\Users\\jorge\\AppData\\Local\\Temp\\ipykernel_9468\\349672268.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['num_adjectives'] = df['tweet'].apply(count_adjectives)\n",
      "C:\\Users\\jorge\\AppData\\Local\\Temp\\ipykernel_9468\\349672268.py:195: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tweet_processed'] = df['tweet'].apply(remove_spanish_stopwords)\n"
     ]
    }
   ],
   "source": [
    "data_train = \"EXIST2024_Tweets_Dataset/training/EXIST2024_training.json\" \n",
    "data_dev = \"EXIST2024_Tweets_Dataset/dev/EXIST2024_dev.json\"\n",
    "type_id = \"task_1\"\n",
    "\n",
    "# Cargamos datos\n",
    "df_train = load_data(data_train)\n",
    "df_dev = load_data(data_dev)\n",
    "\n",
    "# Process target\n",
    "df_train, labels_names, label_mapping = process_target(df_train, type_id, \"train\", None)\n",
    "df_dev = process_target(df_dev, type_id, \"dev\", label_mapping)\n",
    "\n",
    "# Preprocesado de datos\n",
    "df_train = process_data(df_train, type_id)\n",
    "df_dev = process_data(df_dev, type_id)\n",
    "\n",
    "#print(f\"{labels_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>number_annotators</th>\n",
       "      <th>annotators</th>\n",
       "      <th>gender_annotators</th>\n",
       "      <th>age_annotators</th>\n",
       "      <th>ethnicities_annotators</th>\n",
       "      <th>study_levels_annotators</th>\n",
       "      <th>countries_annotators</th>\n",
       "      <th>labels_task1</th>\n",
       "      <th>labels_task2</th>\n",
       "      <th>labels_task3</th>\n",
       "      <th>split</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>num_adjectives</th>\n",
       "      <th>tweet_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>es</td>\n",
       "      <td>@TheChiflis Ignora al otro, es un capullo.El p...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_1, Annotator_2, Annotator_3, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[White or Caucasian, Hispano or Latino, White ...</td>\n",
       "      <td>[Bachelor’s degree, Bachelor’s degree, High sc...</td>\n",
       "      <td>[Italy, Mexico, United States, Spain, Spain, C...</td>\n",
       "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
       "      <td>[REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...</td>\n",
       "      <td>[[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>0</td>\n",
       "      <td>281</td>\n",
       "      <td>3</td>\n",
       "      <td>ignora capulloel problema youtuber denuncia ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100002</td>\n",
       "      <td>es</td>\n",
       "      <td>@ultimonomada_ Si comicsgate se parece en algo...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[Black or African American, Hispano or Latino,...</td>\n",
       "      <td>[High school degree or equivalent, Bachelor’s ...</td>\n",
       "      <td>[United Kingdom, Mexico, United States, Portug...</td>\n",
       "      <td>[NO, NO, NO, NO, YES, NO]</td>\n",
       "      <td>[-, -, -, -, DIRECT, -]</td>\n",
       "      <td>[[-], [-], [-], [-], [OBJECTIFICATION], [-]]</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>1</td>\n",
       "      <td>226</td>\n",
       "      <td>8</td>\n",
       "      <td>si comicsgate parece gamergate pues bien acoso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100003</td>\n",
       "      <td>es</td>\n",
       "      <td>@Steven2897 Lee sobre Gamergate, y como eso ha...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[Black or African American, Hispano or Latino,...</td>\n",
       "      <td>[High school degree or equivalent, Bachelor’s ...</td>\n",
       "      <td>[United Kingdom, Mexico, United States, Portug...</td>\n",
       "      <td>[NO, NO, NO, NO, NO, NO]</td>\n",
       "      <td>[-, -, -, -, -, -]</td>\n",
       "      <td>[[-], [-], [-], [-], [-], [-]]</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>1</td>\n",
       "      <td>233</td>\n",
       "      <td>3</td>\n",
       "      <td>lee gamergate cambiado manera comunicamos inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100004</td>\n",
       "      <td>es</td>\n",
       "      <td>@Lunariita7 Un retraso social bastante lamenta...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_13, Annotator_14, Annotator_15, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[Hispano or Latino, Hispano or Latino, White o...</td>\n",
       "      <td>[High school degree or equivalent, Bachelor’s ...</td>\n",
       "      <td>[Mexico, Chile, Spain, Spain, Portugal, Spain]</td>\n",
       "      <td>[NO, NO, YES, NO, YES, YES]</td>\n",
       "      <td>[-, -, DIRECT, -, REPORTED, REPORTED]</td>\n",
       "      <td>[[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>retraso social bastante lamentable gamergate t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100005</td>\n",
       "      <td>es</td>\n",
       "      <td>@novadragon21 @icep4ck @TvDannyZ Entonces como...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_19, Annotator_20, Annotator_21, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[Hispano or Latino, Hispano or Latino, White o...</td>\n",
       "      <td>[Bachelor’s degree, Bachelor’s degree, Master’...</td>\n",
       "      <td>[Mexico, Afghanistan, United States, Italy, Po...</td>\n",
       "      <td>[YES, NO, YES, NO, YES, YES]</td>\n",
       "      <td>[REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...</td>\n",
       "      <td>[[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>0</td>\n",
       "      <td>305</td>\n",
       "      <td>6</td>\n",
       "      <td>entonces así mercado mejor no hacer cambiarlo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3655</th>\n",
       "      <td>103656</td>\n",
       "      <td>es</td>\n",
       "      <td>Los hombres sieeeempre dicen que si fueran muj...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_265, Annotator_266, Annotator_267, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[White or Caucasian, White or Caucasian, Hispa...</td>\n",
       "      <td>[High school degree or equivalent, Bachelor’s ...</td>\n",
       "      <td>[Spain, Spain, Mexico, Italy, Spain, Portugal]</td>\n",
       "      <td>[YES, YES, YES, YES, NO, YES]</td>\n",
       "      <td>[JUDGEMENTAL, JUDGEMENTAL, DIRECT, REPORTED, -...</td>\n",
       "      <td>[[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [S...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>0</td>\n",
       "      <td>209</td>\n",
       "      <td>4</td>\n",
       "      <td>hombres sieeeempre dicen si mujeres bien zorra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3656</th>\n",
       "      <td>103657</td>\n",
       "      <td>es</td>\n",
       "      <td>@dat_mauro Estas zorras qls siempre preocupada...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_223, Annotator_224, Annotator_225, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[White or Caucasian, Hispano or Latino, Hispan...</td>\n",
       "      <td>[High school degree or equivalent, High school...</td>\n",
       "      <td>[Spain, Chile, Chile, Netherlands, Portugal, P...</td>\n",
       "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
       "      <td>[DIRECT, DIRECT, -, DIRECT, JUDGEMENTAL, DIRECT]</td>\n",
       "      <td>[[MISOGYNY-NON-SEXUAL-VIOLENCE], [IDEOLOGICAL-...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>2</td>\n",
       "      <td>zorras qls siempre preocupadas u tiran peo des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3657</th>\n",
       "      <td>103658</td>\n",
       "      <td>es</td>\n",
       "      <td>A pesar de que convenga que las Zorras jueguen...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_1, Annotator_2, Annotator_3, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[White or Caucasian, Hispano or Latino, White ...</td>\n",
       "      <td>[Bachelor’s degree, Bachelor’s degree, High sc...</td>\n",
       "      <td>[Italy, Mexico, United States, Spain, Spain, C...</td>\n",
       "      <td>[NO, YES, YES, YES, YES, YES]</td>\n",
       "      <td>[-, DIRECT, DIRECT, DIRECT, DIRECT, DIRECT]</td>\n",
       "      <td>[[-], [OBJECTIFICATION, SEXUAL-VIOLENCE], [STE...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>3</td>\n",
       "      <td>pesar convenga zorras jueguen juveniles admiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3658</th>\n",
       "      <td>103659</td>\n",
       "      <td>es</td>\n",
       "      <td>@patofigueroam Las 2 estrellas que se bordó SW...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_139, Annotator_140, Annotator_141, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[White or Caucasian, Hispano or Latino, Multir...</td>\n",
       "      <td>[Bachelor’s degree, Bachelor’s degree, Bachelo...</td>\n",
       "      <td>[Portugal, Mexico, Italy, Italy, Portugal, Mex...</td>\n",
       "      <td>[NO, NO, YES, NO, YES, YES]</td>\n",
       "      <td>[-, -, DIRECT, -, JUDGEMENTAL, DIRECT]</td>\n",
       "      <td>[[-], [-], [OBJECTIFICATION, SEXUAL-VIOLENCE, ...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>2</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>2 estrellas bordó sw ganó secretaria deportivo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3659</th>\n",
       "      <td>103660</td>\n",
       "      <td>es</td>\n",
       "      <td>@soyunamedusilla Aaaaaaa ¿quién te manda socia...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_13, Annotator_14, Annotator_15, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[Hispano or Latino, Hispano or Latino, White o...</td>\n",
       "      <td>[High school degree or equivalent, Bachelor’s ...</td>\n",
       "      <td>[Mexico, Chile, Spain, Spain, Portugal, Spain]</td>\n",
       "      <td>[YES, NO, YES, YES, YES, NO]</td>\n",
       "      <td>[DIRECT, -, DIRECT, REPORTED, REPORTED, -]</td>\n",
       "      <td>[[MISOGYNY-NON-SEXUAL-VIOLENCE], [-], [OBJECTI...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "      <td>aaaaaaa ¿quién manda socializar dos unas buena...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3660 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_EXIST lang                                              tweet  \\\n",
       "0      100001   es  @TheChiflis Ignora al otro, es un capullo.El p...   \n",
       "1      100002   es  @ultimonomada_ Si comicsgate se parece en algo...   \n",
       "2      100003   es  @Steven2897 Lee sobre Gamergate, y como eso ha...   \n",
       "3      100004   es  @Lunariita7 Un retraso social bastante lamenta...   \n",
       "4      100005   es  @novadragon21 @icep4ck @TvDannyZ Entonces como...   \n",
       "...       ...  ...                                                ...   \n",
       "3655   103656   es  Los hombres sieeeempre dicen que si fueran muj...   \n",
       "3656   103657   es  @dat_mauro Estas zorras qls siempre preocupada...   \n",
       "3657   103658   es  A pesar de que convenga que las Zorras jueguen...   \n",
       "3658   103659   es  @patofigueroam Las 2 estrellas que se bordó SW...   \n",
       "3659   103660   es  @soyunamedusilla Aaaaaaa ¿quién te manda socia...   \n",
       "\n",
       "      number_annotators                                         annotators  \\\n",
       "0                     6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
       "1                     6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
       "2                     6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
       "3                     6  [Annotator_13, Annotator_14, Annotator_15, Ann...   \n",
       "4                     6  [Annotator_19, Annotator_20, Annotator_21, Ann...   \n",
       "...                 ...                                                ...   \n",
       "3655                  6  [Annotator_265, Annotator_266, Annotator_267, ...   \n",
       "3656                  6  [Annotator_223, Annotator_224, Annotator_225, ...   \n",
       "3657                  6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
       "3658                  6  [Annotator_139, Annotator_140, Annotator_141, ...   \n",
       "3659                  6  [Annotator_13, Annotator_14, Annotator_15, Ann...   \n",
       "\n",
       "       gender_annotators                          age_annotators  \\\n",
       "0     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "1     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "2     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "3     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "4     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "...                  ...                                     ...   \n",
       "3655  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "3656  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "3657  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "3658  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "3659  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "\n",
       "                                 ethnicities_annotators  \\\n",
       "0     [White or Caucasian, Hispano or Latino, White ...   \n",
       "1     [Black or African American, Hispano or Latino,...   \n",
       "2     [Black or African American, Hispano or Latino,...   \n",
       "3     [Hispano or Latino, Hispano or Latino, White o...   \n",
       "4     [Hispano or Latino, Hispano or Latino, White o...   \n",
       "...                                                 ...   \n",
       "3655  [White or Caucasian, White or Caucasian, Hispa...   \n",
       "3656  [White or Caucasian, Hispano or Latino, Hispan...   \n",
       "3657  [White or Caucasian, Hispano or Latino, White ...   \n",
       "3658  [White or Caucasian, Hispano or Latino, Multir...   \n",
       "3659  [Hispano or Latino, Hispano or Latino, White o...   \n",
       "\n",
       "                                study_levels_annotators  \\\n",
       "0     [Bachelor’s degree, Bachelor’s degree, High sc...   \n",
       "1     [High school degree or equivalent, Bachelor’s ...   \n",
       "2     [High school degree or equivalent, Bachelor’s ...   \n",
       "3     [High school degree or equivalent, Bachelor’s ...   \n",
       "4     [Bachelor’s degree, Bachelor’s degree, Master’...   \n",
       "...                                                 ...   \n",
       "3655  [High school degree or equivalent, Bachelor’s ...   \n",
       "3656  [High school degree or equivalent, High school...   \n",
       "3657  [Bachelor’s degree, Bachelor’s degree, High sc...   \n",
       "3658  [Bachelor’s degree, Bachelor’s degree, Bachelo...   \n",
       "3659  [High school degree or equivalent, Bachelor’s ...   \n",
       "\n",
       "                                   countries_annotators  \\\n",
       "0     [Italy, Mexico, United States, Spain, Spain, C...   \n",
       "1     [United Kingdom, Mexico, United States, Portug...   \n",
       "2     [United Kingdom, Mexico, United States, Portug...   \n",
       "3        [Mexico, Chile, Spain, Spain, Portugal, Spain]   \n",
       "4     [Mexico, Afghanistan, United States, Italy, Po...   \n",
       "...                                                 ...   \n",
       "3655     [Spain, Spain, Mexico, Italy, Spain, Portugal]   \n",
       "3656  [Spain, Chile, Chile, Netherlands, Portugal, P...   \n",
       "3657  [Italy, Mexico, United States, Spain, Spain, C...   \n",
       "3658  [Portugal, Mexico, Italy, Italy, Portugal, Mex...   \n",
       "3659     [Mexico, Chile, Spain, Spain, Portugal, Spain]   \n",
       "\n",
       "                       labels_task1  \\\n",
       "0     [YES, YES, NO, YES, YES, YES]   \n",
       "1         [NO, NO, NO, NO, YES, NO]   \n",
       "2          [NO, NO, NO, NO, NO, NO]   \n",
       "3       [NO, NO, YES, NO, YES, YES]   \n",
       "4      [YES, NO, YES, NO, YES, YES]   \n",
       "...                             ...   \n",
       "3655  [YES, YES, YES, YES, NO, YES]   \n",
       "3656  [YES, YES, NO, YES, YES, YES]   \n",
       "3657  [NO, YES, YES, YES, YES, YES]   \n",
       "3658    [NO, NO, YES, NO, YES, YES]   \n",
       "3659   [YES, NO, YES, YES, YES, NO]   \n",
       "\n",
       "                                           labels_task2  \\\n",
       "0     [REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...   \n",
       "1                               [-, -, -, -, DIRECT, -]   \n",
       "2                                    [-, -, -, -, -, -]   \n",
       "3                 [-, -, DIRECT, -, REPORTED, REPORTED]   \n",
       "4     [REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...   \n",
       "...                                                 ...   \n",
       "3655  [JUDGEMENTAL, JUDGEMENTAL, DIRECT, REPORTED, -...   \n",
       "3656   [DIRECT, DIRECT, -, DIRECT, JUDGEMENTAL, DIRECT]   \n",
       "3657        [-, DIRECT, DIRECT, DIRECT, DIRECT, DIRECT]   \n",
       "3658             [-, -, DIRECT, -, JUDGEMENTAL, DIRECT]   \n",
       "3659         [DIRECT, -, DIRECT, REPORTED, REPORTED, -]   \n",
       "\n",
       "                                           labels_task3     split  label  \\\n",
       "0     [[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...  TRAIN_ES      0   \n",
       "1          [[-], [-], [-], [-], [OBJECTIFICATION], [-]]  TRAIN_ES      1   \n",
       "2                        [[-], [-], [-], [-], [-], [-]]  TRAIN_ES      1   \n",
       "3     [[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...  TRAIN_ES      2   \n",
       "4     [[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...  TRAIN_ES      0   \n",
       "...                                                 ...       ...    ...   \n",
       "3655  [[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [S...  TRAIN_ES      0   \n",
       "3656  [[MISOGYNY-NON-SEXUAL-VIOLENCE], [IDEOLOGICAL-...  TRAIN_ES      0   \n",
       "3657  [[-], [OBJECTIFICATION, SEXUAL-VIOLENCE], [STE...  TRAIN_ES      0   \n",
       "3658  [[-], [-], [OBJECTIFICATION, SEXUAL-VIOLENCE, ...  TRAIN_ES      2   \n",
       "3659  [[MISOGYNY-NON-SEXUAL-VIOLENCE], [-], [OBJECTI...  TRAIN_ES      0   \n",
       "\n",
       "      tweet_length  num_adjectives  \\\n",
       "0              281               3   \n",
       "1              226               8   \n",
       "2              233               3   \n",
       "3              108               5   \n",
       "4              305               6   \n",
       "...            ...             ...   \n",
       "3655           209               4   \n",
       "3656           158               2   \n",
       "3657           146               3   \n",
       "3658            97               0   \n",
       "3659            98               2   \n",
       "\n",
       "                                        tweet_processed  \n",
       "0     ignora capulloel problema youtuber denuncia ac...  \n",
       "1     si comicsgate parece gamergate pues bien acoso...  \n",
       "2     lee gamergate cambiado manera comunicamos inte...  \n",
       "3     retraso social bastante lamentable gamergate t...  \n",
       "4     entonces así mercado mejor no hacer cambiarlo ...  \n",
       "...                                                 ...  \n",
       "3655  hombres sieeeempre dicen si mujeres bien zorra...  \n",
       "3656  zorras qls siempre preocupadas u tiran peo des...  \n",
       "3657  pesar convenga zorras jueguen juveniles admiti...  \n",
       "3658  2 estrellas bordó sw ganó secretaria deportivo...  \n",
       "3659  aaaaaaa ¿quién manda socializar dos unas buena...  \n",
       "\n",
       "[3660 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>number_annotators</th>\n",
       "      <th>annotators</th>\n",
       "      <th>gender_annotators</th>\n",
       "      <th>age_annotators</th>\n",
       "      <th>ethnicities_annotators</th>\n",
       "      <th>study_levels_annotators</th>\n",
       "      <th>countries_annotators</th>\n",
       "      <th>labels_task1</th>\n",
       "      <th>labels_task2</th>\n",
       "      <th>labels_task3</th>\n",
       "      <th>split</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>num_adjectives</th>\n",
       "      <th>tweet_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300001</td>\n",
       "      <td>es</td>\n",
       "      <td>@Fichinescu La comunidad gamer es un antro de ...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_726, Annotator_727, Annotator_357, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[Hispano or Latino, Hispano or Latino, White o...</td>\n",
       "      <td>[High school degree or equivalent, Bachelor’s ...</td>\n",
       "      <td>[Mexico, Chile, Serbia, Portugal, Mexico, Spain]</td>\n",
       "      <td>[NO, YES, YES, NO, YES, NO]</td>\n",
       "      <td>[-, JUDGEMENTAL, JUDGEMENTAL, -, REPORTED, -]</td>\n",
       "      <td>[[-], [MISOGYNY-NON-SEXUAL-VIOLENCE], [MISOGYN...</td>\n",
       "      <td>DEV_ES</td>\n",
       "      <td>2</td>\n",
       "      <td>190</td>\n",
       "      <td>5</td>\n",
       "      <td>comunidad gamer antro misóginos supremacistas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300002</td>\n",
       "      <td>es</td>\n",
       "      <td>@anacaotica88 @MordorLivin No me acuerdo de lo...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_731, Annotator_732, Annotator_315, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[White or Caucasian, Hispano or Latino, White ...</td>\n",
       "      <td>[High school degree or equivalent, Bachelor’s ...</td>\n",
       "      <td>[Spain, Chile, United Kingdom, Chile, Chile, S...</td>\n",
       "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
       "      <td>[JUDGEMENTAL, REPORTED, -, JUDGEMENTAL, JUDGEM...</td>\n",
       "      <td>[[IDEOLOGICAL-INEQUALITY, STEREOTYPING-DOMINAN...</td>\n",
       "      <td>DEV_ES</td>\n",
       "      <td>0</td>\n",
       "      <td>285</td>\n",
       "      <td>6</td>\n",
       "      <td>no acuerdo detalles gamergate ojo huracán reci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300003</td>\n",
       "      <td>es</td>\n",
       "      <td>@cosmicJunkBot lo digo cada pocos dias y lo re...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_735, Annotator_736, Annotator_345, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[White or Caucasian, White or Caucasian, White...</td>\n",
       "      <td>[Bachelor’s degree, Master’s degree, Master’s ...</td>\n",
       "      <td>[Italy, Spain, Germany, Portugal, Spain, Spain]</td>\n",
       "      <td>[NO, NO, NO, NO, NO, NO]</td>\n",
       "      <td>[-, -, -, -, -, -]</td>\n",
       "      <td>[[-], [-], [-], [-], [-], [-]]</td>\n",
       "      <td>DEV_ES</td>\n",
       "      <td>1</td>\n",
       "      <td>119</td>\n",
       "      <td>1</td>\n",
       "      <td>digo cada pocos dias repito pudo evitar si par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300004</td>\n",
       "      <td>es</td>\n",
       "      <td>Also mientras les decia eso la señalaba y deci...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_259, Annotator_739, Annotator_291, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[Hispano or Latino, Hispano or Latino, White o...</td>\n",
       "      <td>[Bachelor’s degree, Bachelor’s degree, High sc...</td>\n",
       "      <td>[Mexico, Mexico, Portugal, Mexico, Mexico, Spain]</td>\n",
       "      <td>[NO, YES, YES, YES, YES, YES]</td>\n",
       "      <td>[-, REPORTED, REPORTED, REPORTED, JUDGEMENTAL,...</td>\n",
       "      <td>[[-], [SEXUAL-VIOLENCE], [SEXUAL-VIOLENCE], [S...</td>\n",
       "      <td>DEV_ES</td>\n",
       "      <td>0</td>\n",
       "      <td>280</td>\n",
       "      <td>6</td>\n",
       "      <td>also mientras decia señalaba decia staff iban ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300005</td>\n",
       "      <td>es</td>\n",
       "      <td>And all people killed,  attacked, harassed by ...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_731, Annotator_732, Annotator_315, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[White or Caucasian, Hispano or Latino, White ...</td>\n",
       "      <td>[High school degree or equivalent, Bachelor’s ...</td>\n",
       "      <td>[Spain, Chile, United Kingdom, Chile, Chile, S...</td>\n",
       "      <td>[NO, YES, NO, NO, NO, NO]</td>\n",
       "      <td>[-, DIRECT, -, -, -, -]</td>\n",
       "      <td>[[-], [STEREOTYPING-DOMINANCE], [-], [-], [-],...</td>\n",
       "      <td>DEV_ES</td>\n",
       "      <td>1</td>\n",
       "      <td>245</td>\n",
       "      <td>3</td>\n",
       "      <td>and all people killed attacked harassed by cam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>300545</td>\n",
       "      <td>es</td>\n",
       "      <td>#ChiringuitoHaaland Dice la zorra de estar ver...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_756, Annotator_757, Annotator_758, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[White or Caucasian, Hispano or Latino, White ...</td>\n",
       "      <td>[Bachelor’s degree, Bachelor’s degree, Master’...</td>\n",
       "      <td>[Portugal, Mexico, Italy, Mexico, Mexico, Mexico]</td>\n",
       "      <td>[YES, YES, YES, YES, YES, YES]</td>\n",
       "      <td>[DIRECT, DIRECT, DIRECT, DIRECT, DIRECT, JUDGE...</td>\n",
       "      <td>[[IDEOLOGICAL-INEQUALITY], [STEREOTYPING-DOMIN...</td>\n",
       "      <td>DEV_ES</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>chiringuitohaaland dice zorra verdes uvas no p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>300546</td>\n",
       "      <td>es</td>\n",
       "      <td>soy una zorra por el kayn el mayor jodido smas...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_259, Annotator_739, Annotator_291, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[Hispano or Latino, Hispano or Latino, White o...</td>\n",
       "      <td>[Bachelor’s degree, Bachelor’s degree, High sc...</td>\n",
       "      <td>[Mexico, Mexico, Portugal, Mexico, Mexico, Spain]</td>\n",
       "      <td>[YES, NO, YES, YES, NO, NO]</td>\n",
       "      <td>[DIRECT, -, REPORTED, DIRECT, -, -]</td>\n",
       "      <td>[[SEXUAL-VIOLENCE], [-], [OBJECTIFICATION], [S...</td>\n",
       "      <td>DEV_ES</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>zorra kayn mayor jodido smash juego httpstcoc8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>300547</td>\n",
       "      <td>es</td>\n",
       "      <td>Poniéndoseme en contra solo por las sombras, h...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_761, Annotator_762, Annotator_207, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[Hispano or Latino, Hispano or Latino, Hispano...</td>\n",
       "      <td>[Bachelor’s degree, Bachelor’s degree, Bachelo...</td>\n",
       "      <td>[Mexico, Mexico, Spain, Mexico, Mexico, Canada]</td>\n",
       "      <td>[YES, YES, YES, YES, YES, YES]</td>\n",
       "      <td>[DIRECT, JUDGEMENTAL, DIRECT, DIRECT, JUDGEMEN...</td>\n",
       "      <td>[[IDEOLOGICAL-INEQUALITY, MISOGYNY-NON-SEXUAL-...</td>\n",
       "      <td>DEV_ES</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>poniéndoseme solo sombras hablado espalda hace...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>300548</td>\n",
       "      <td>es</td>\n",
       "      <td>@srthouston Me lo estoy perdiendo y no estoy n...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_744, Annotator_745, Annotator_746, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[Hispano or Latino, Hispano or Latino, Hispano...</td>\n",
       "      <td>[High school degree or equivalent, Bachelor’s ...</td>\n",
       "      <td>[Mexico, Mexico, Venezuela, Mexico, Chile, Mex...</td>\n",
       "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
       "      <td>[DIRECT, DIRECT, -, DIRECT, DIRECT, JUDGEMENTAL]</td>\n",
       "      <td>[[MISOGYNY-NON-SEXUAL-VIOLENCE], [STEREOTYPING...</td>\n",
       "      <td>DEV_ES</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>perdiendo no bien ganadors zorras 💖🏆🦊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>300549</td>\n",
       "      <td>es</td>\n",
       "      <td>@LillyTellez Le pagamos bien a la senadora cha...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_259, Annotator_739, Annotator_291, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[Hispano or Latino, Hispano or Latino, White o...</td>\n",
       "      <td>[Bachelor’s degree, Bachelor’s degree, High sc...</td>\n",
       "      <td>[Mexico, Mexico, Portugal, Mexico, Mexico, Spain]</td>\n",
       "      <td>[YES, YES, YES, YES, YES, YES]</td>\n",
       "      <td>[DIRECT, DIRECT, DIRECT, DIRECT, DIRECT, DIRECT]</td>\n",
       "      <td>[[STEREOTYPING-DOMINANCE, MISOGYNY-NON-SEXUAL-...</td>\n",
       "      <td>DEV_ES</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>4</td>\n",
       "      <td>pagamos bien senadora chapulines único hace la...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id_EXIST lang                                              tweet  \\\n",
       "0     300001   es  @Fichinescu La comunidad gamer es un antro de ...   \n",
       "1     300002   es  @anacaotica88 @MordorLivin No me acuerdo de lo...   \n",
       "2     300003   es  @cosmicJunkBot lo digo cada pocos dias y lo re...   \n",
       "3     300004   es  Also mientras les decia eso la señalaba y deci...   \n",
       "4     300005   es  And all people killed,  attacked, harassed by ...   \n",
       "..       ...  ...                                                ...   \n",
       "544   300545   es  #ChiringuitoHaaland Dice la zorra de estar ver...   \n",
       "545   300546   es  soy una zorra por el kayn el mayor jodido smas...   \n",
       "546   300547   es  Poniéndoseme en contra solo por las sombras, h...   \n",
       "547   300548   es  @srthouston Me lo estoy perdiendo y no estoy n...   \n",
       "548   300549   es  @LillyTellez Le pagamos bien a la senadora cha...   \n",
       "\n",
       "     number_annotators                                         annotators  \\\n",
       "0                    6  [Annotator_726, Annotator_727, Annotator_357, ...   \n",
       "1                    6  [Annotator_731, Annotator_732, Annotator_315, ...   \n",
       "2                    6  [Annotator_735, Annotator_736, Annotator_345, ...   \n",
       "3                    6  [Annotator_259, Annotator_739, Annotator_291, ...   \n",
       "4                    6  [Annotator_731, Annotator_732, Annotator_315, ...   \n",
       "..                 ...                                                ...   \n",
       "544                  6  [Annotator_756, Annotator_757, Annotator_758, ...   \n",
       "545                  6  [Annotator_259, Annotator_739, Annotator_291, ...   \n",
       "546                  6  [Annotator_761, Annotator_762, Annotator_207, ...   \n",
       "547                  6  [Annotator_744, Annotator_745, Annotator_746, ...   \n",
       "548                  6  [Annotator_259, Annotator_739, Annotator_291, ...   \n",
       "\n",
       "      gender_annotators                          age_annotators  \\\n",
       "0    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "1    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "2    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "3    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "4    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "..                  ...                                     ...   \n",
       "544  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "545  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "546  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "547  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "548  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "\n",
       "                                ethnicities_annotators  \\\n",
       "0    [Hispano or Latino, Hispano or Latino, White o...   \n",
       "1    [White or Caucasian, Hispano or Latino, White ...   \n",
       "2    [White or Caucasian, White or Caucasian, White...   \n",
       "3    [Hispano or Latino, Hispano or Latino, White o...   \n",
       "4    [White or Caucasian, Hispano or Latino, White ...   \n",
       "..                                                 ...   \n",
       "544  [White or Caucasian, Hispano or Latino, White ...   \n",
       "545  [Hispano or Latino, Hispano or Latino, White o...   \n",
       "546  [Hispano or Latino, Hispano or Latino, Hispano...   \n",
       "547  [Hispano or Latino, Hispano or Latino, Hispano...   \n",
       "548  [Hispano or Latino, Hispano or Latino, White o...   \n",
       "\n",
       "                               study_levels_annotators  \\\n",
       "0    [High school degree or equivalent, Bachelor’s ...   \n",
       "1    [High school degree or equivalent, Bachelor’s ...   \n",
       "2    [Bachelor’s degree, Master’s degree, Master’s ...   \n",
       "3    [Bachelor’s degree, Bachelor’s degree, High sc...   \n",
       "4    [High school degree or equivalent, Bachelor’s ...   \n",
       "..                                                 ...   \n",
       "544  [Bachelor’s degree, Bachelor’s degree, Master’...   \n",
       "545  [Bachelor’s degree, Bachelor’s degree, High sc...   \n",
       "546  [Bachelor’s degree, Bachelor’s degree, Bachelo...   \n",
       "547  [High school degree or equivalent, Bachelor’s ...   \n",
       "548  [Bachelor’s degree, Bachelor’s degree, High sc...   \n",
       "\n",
       "                                  countries_annotators  \\\n",
       "0     [Mexico, Chile, Serbia, Portugal, Mexico, Spain]   \n",
       "1    [Spain, Chile, United Kingdom, Chile, Chile, S...   \n",
       "2      [Italy, Spain, Germany, Portugal, Spain, Spain]   \n",
       "3    [Mexico, Mexico, Portugal, Mexico, Mexico, Spain]   \n",
       "4    [Spain, Chile, United Kingdom, Chile, Chile, S...   \n",
       "..                                                 ...   \n",
       "544  [Portugal, Mexico, Italy, Mexico, Mexico, Mexico]   \n",
       "545  [Mexico, Mexico, Portugal, Mexico, Mexico, Spain]   \n",
       "546    [Mexico, Mexico, Spain, Mexico, Mexico, Canada]   \n",
       "547  [Mexico, Mexico, Venezuela, Mexico, Chile, Mex...   \n",
       "548  [Mexico, Mexico, Portugal, Mexico, Mexico, Spain]   \n",
       "\n",
       "                       labels_task1  \\\n",
       "0       [NO, YES, YES, NO, YES, NO]   \n",
       "1     [YES, YES, NO, YES, YES, YES]   \n",
       "2          [NO, NO, NO, NO, NO, NO]   \n",
       "3     [NO, YES, YES, YES, YES, YES]   \n",
       "4         [NO, YES, NO, NO, NO, NO]   \n",
       "..                              ...   \n",
       "544  [YES, YES, YES, YES, YES, YES]   \n",
       "545     [YES, NO, YES, YES, NO, NO]   \n",
       "546  [YES, YES, YES, YES, YES, YES]   \n",
       "547   [YES, YES, NO, YES, YES, YES]   \n",
       "548  [YES, YES, YES, YES, YES, YES]   \n",
       "\n",
       "                                          labels_task2  \\\n",
       "0        [-, JUDGEMENTAL, JUDGEMENTAL, -, REPORTED, -]   \n",
       "1    [JUDGEMENTAL, REPORTED, -, JUDGEMENTAL, JUDGEM...   \n",
       "2                                   [-, -, -, -, -, -]   \n",
       "3    [-, REPORTED, REPORTED, REPORTED, JUDGEMENTAL,...   \n",
       "4                              [-, DIRECT, -, -, -, -]   \n",
       "..                                                 ...   \n",
       "544  [DIRECT, DIRECT, DIRECT, DIRECT, DIRECT, JUDGE...   \n",
       "545                [DIRECT, -, REPORTED, DIRECT, -, -]   \n",
       "546  [DIRECT, JUDGEMENTAL, DIRECT, DIRECT, JUDGEMEN...   \n",
       "547   [DIRECT, DIRECT, -, DIRECT, DIRECT, JUDGEMENTAL]   \n",
       "548   [DIRECT, DIRECT, DIRECT, DIRECT, DIRECT, DIRECT]   \n",
       "\n",
       "                                          labels_task3   split  label  \\\n",
       "0    [[-], [MISOGYNY-NON-SEXUAL-VIOLENCE], [MISOGYN...  DEV_ES      2   \n",
       "1    [[IDEOLOGICAL-INEQUALITY, STEREOTYPING-DOMINAN...  DEV_ES      0   \n",
       "2                       [[-], [-], [-], [-], [-], [-]]  DEV_ES      1   \n",
       "3    [[-], [SEXUAL-VIOLENCE], [SEXUAL-VIOLENCE], [S...  DEV_ES      0   \n",
       "4    [[-], [STEREOTYPING-DOMINANCE], [-], [-], [-],...  DEV_ES      1   \n",
       "..                                                 ...     ...    ...   \n",
       "544  [[IDEOLOGICAL-INEQUALITY], [STEREOTYPING-DOMIN...  DEV_ES      0   \n",
       "545  [[SEXUAL-VIOLENCE], [-], [OBJECTIFICATION], [S...  DEV_ES      2   \n",
       "546  [[IDEOLOGICAL-INEQUALITY, MISOGYNY-NON-SEXUAL-...  DEV_ES      0   \n",
       "547  [[MISOGYNY-NON-SEXUAL-VIOLENCE], [STEREOTYPING...  DEV_ES      0   \n",
       "548  [[STEREOTYPING-DOMINANCE, MISOGYNY-NON-SEXUAL-...  DEV_ES      0   \n",
       "\n",
       "     tweet_length  num_adjectives  \\\n",
       "0             190               5   \n",
       "1             285               6   \n",
       "2             119               1   \n",
       "3             280               6   \n",
       "4             245               3   \n",
       "..            ...             ...   \n",
       "544            90               1   \n",
       "545            90               2   \n",
       "546            92               1   \n",
       "547           118               2   \n",
       "548           156               4   \n",
       "\n",
       "                                       tweet_processed  \n",
       "0    comunidad gamer antro misóginos supremacistas ...  \n",
       "1    no acuerdo detalles gamergate ojo huracán reci...  \n",
       "2    digo cada pocos dias repito pudo evitar si par...  \n",
       "3    also mientras decia señalaba decia staff iban ...  \n",
       "4    and all people killed attacked harassed by cam...  \n",
       "..                                                 ...  \n",
       "544  chiringuitohaaland dice zorra verdes uvas no p...  \n",
       "545  zorra kayn mayor jodido smash juego httpstcoc8...  \n",
       "546  poniéndoseme solo sombras hablado espalda hace...  \n",
       "547              perdiendo no bien ganadors zorras 💖🏆🦊  \n",
       "548  pagamos bien senadora chapulines único hace la...  \n",
       "\n",
       "[549 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': '@Jennihermoso Al menos podrías haber escrito el comunicado sin faltas de ortografía.', 'labels': ['Comentario Neutro', 'Comentario Positivo acerca de las mujeres', 'Comentario Negativo acerca de las mujeres'], 'scores': [0.3884323239326477, 0.3174141049385071, 0.2941535711288452]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "\n",
    "text = \"@Jennihermoso Al menos podrías haber escrito el comunicado sin faltas de ortografía.\"\n",
    "\n",
    "candidate_labels = [\"Comentario Positivo acerca de las mujeres\", \"Comentario Neutro\", \"Comentario Negativo acerca de las mujeres\"]\n",
    "\n",
    "\n",
    "result = classifier(text, candidate_labels)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, DistilBertTokenizer, BertModel, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, RobertaTokenizer, RobertaModel, XLMRobertaModel, AutoTokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import string\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roberta_embeddings(texts, tokenizer, model, device='cpu'):\n",
    "\n",
    "    # Mover el modelo al dispositivo adecuado\n",
    "    model.to(device)\n",
    "    \n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        # Tokenizar el texto y agregar los tokens especiales\n",
    "        encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "        \n",
    "        # Obtener los embeddings del modelo\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded_input)\n",
    "        \n",
    "        # Usar los embeddings del último estado oculto\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        \n",
    "        # Promediar los embeddings del token a lo largo de la dimensión de la secuencia para obtener un único vector de embedding por texto\n",
    "        mean_embedding = torch.mean(last_hidden_state, dim=1)\n",
    "        embeddings.append(mean_embedding.cpu().numpy())\n",
    "        \n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data):\n",
    "    # Leer el archivo de datos\n",
    "    df = pd.read_csv(data)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df, type_id, balance):\n",
    "\n",
    "    # Normalize \"view_count\"\n",
    "    scaler = StandardScaler()\n",
    "    df['view_count_scaled'] = scaler.fit_transform(df[['view_count']])\n",
    "\n",
    "    # User Mentions\n",
    "    def count_user_mentions(mentions):\n",
    "        if pd.isna(mentions) or mentions == \"\":\n",
    "            return 0\n",
    "        else:\n",
    "            return len(mentions.split(';'))\n",
    "        \n",
    "    df['mention_count'] = df['user_mentions'].apply(count_user_mentions)\n",
    "\n",
    "    # Length Tweet\n",
    "    df['tweet_length'] = df['full_text'].str.len()\n",
    "\n",
    "    # Num Adjetives\n",
    "    def count_adjectives(text):\n",
    "        words = word_tokenize(text)\n",
    "        pos_tags = pos_tag(words)\n",
    "        return sum(1 for word, tag in pos_tags if tag.startswith('JJ'))\n",
    "    df['num_adjectives'] = df['full_text'].apply(count_adjectives)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    #[\"analisis_general\", \"contenido_negativo\", \"insultos\"]\n",
    "    if type_id == \"analisis_general\":\n",
    "        # Define the specific labels to keep\n",
    "        #etiquetas = [\"Comentario Positivo\", \"Comentario Negativo\", \"Comentario Neutro\"]\n",
    "        etiquetas = [\"Comentario Positivo\", \"Comentario Negativo\"]\n",
    "        \n",
    "        df['Análisis General'] = df['Análisis General'].where(df['Análisis General'].isin(etiquetas))\n",
    "\n",
    "\n",
    "        # Remove NAs\n",
    "        df = df.dropna(subset=['Análisis General'])\n",
    "        \n",
    "\n",
    "        # Factorize the 'Análisis General' column\n",
    "        labels, labels_names = pd.factorize(df['Análisis General'])\n",
    "\n",
    "        # 'labels' now contains the numeric representation of your original labels\n",
    "        # 'label_names' contains the unique values from your original column in the order they were encoded\n",
    "\n",
    "        # Replace the original column with the numeric labels\n",
    "        df['label'] = labels\n",
    "\n",
    "        # If you want to keep a record of the mapping from the original labels to the numeric labels\n",
    "        label_mapping = dict(zip(labels_names, range(len(labels_names))))\n",
    "        #print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "    if type_id == \"contenido_negativo\":\n",
    "\n",
    "        # Filtrar el DataFrame para seleccionar solo los \"Comentario Negativo\"\n",
    "        df = df.loc[df['Análisis General'] == 'Comentario Negativo']\n",
    "\n",
    "        # Define the specific labels to keep\n",
    "        etiquetas = [\"Desprestigiar Víctima\", \"Desprestigiar Acto\", \"Insultos\", \"Desprestigiar Deportista Autora\"]\n",
    "        df['Contenido Negativo'] = df['Contenido Negativo'].where(df['Contenido Negativo'].isin(etiquetas))\n",
    "\n",
    "        # Remove NAs\n",
    "        df = df.dropna(subset=['Contenido Negativo'])\n",
    "        \n",
    "\n",
    "        # Factorize the 'Análisis General' column\n",
    "        labels, labels_names = pd.factorize(df['Contenido Negativo'])\n",
    "\n",
    "        # 'labels' now contains the numeric representation of your original labels\n",
    "        # 'label_names' contains the unique values from your original column in the order they were encoded\n",
    "\n",
    "        # Replace the original column with the numeric labels\n",
    "        df['label'] = labels\n",
    "\n",
    "        # If you want to keep a record of the mapping from the original labels to the numeric labels\n",
    "        label_mapping = dict(zip(labels_names, range(len(labels_names))))\n",
    "        #print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "\n",
    "    if type_id == \"insultos\":\n",
    "\n",
    "        # Filtrar el DataFrame para seleccionar solo los \"Comentario Negativo\"\n",
    "        df = df.loc[df['Análisis General'] == 'Comentario Negativo']\n",
    "\n",
    "        # Define the specific labels to keep\n",
    "        etiquetas = [\"Deseo de Dañar\", \"Genéricos\", \"Sexistas/misóginos\", \"\"]\n",
    "\n",
    "        # Replace labels that are not in the list with \"Genéricos\"\n",
    "        df['Insultos'] = df['Insultos'].where(df['Insultos'].isin(etiquetas), other=\"Genéricos\")\n",
    "\n",
    "        # Remove NAs\n",
    "        df = df.dropna(subset=['Insultos'])\n",
    "        \n",
    "\n",
    "        # Factorize the 'Insultos' column\n",
    "        labels, labels_names = pd.factorize(df['Insultos'])\n",
    "\n",
    "        # 'labels' now contains the numeric representation of your original labels\n",
    "        # 'label_names' contains the unique values from your original column in the order they were encoded\n",
    "\n",
    "        # Replace the original column with the numeric labels\n",
    "        df['label'] = labels\n",
    "\n",
    "        # If you want to keep a record of the mapping from the original labels to the numeric labels\n",
    "        label_mapping = dict(zip(labels_names, range(len(labels_names))))\n",
    "        #print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "\n",
    "    # Contar el soporte de cada etiqueta\n",
    "    soporte_etiquetas = df['label'].value_counts()\n",
    "\n",
    "    # Imprimir el soporte para cada etiqueta\n",
    "    print(\"\\nSoporte de etiquetas con nombres originales:\")\n",
    "    for nombre_etiqueta, codigo in label_mapping.items():\n",
    "        print(f\"{nombre_etiqueta}: {soporte_etiquetas[codigo]}\")\n",
    "\n",
    "    # Initialize stemmer\n",
    "    ##stemmer = SnowballStemmer('spanish')\n",
    "    \n",
    "    # Define function to remove stopwords, punctuation, and apply stemming\n",
    "    def remove_spanish_stopwords(text):\n",
    "\n",
    "        # Eliminar menciones a usuarios (palabras que comienzan con @)\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "        # Eliminar enlaces (todo lo que comienza con http o https)\n",
    "        #text = re.sub(r'http\\S+|https\\S+', '', text)\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = ''.join([char for char in text if char not in string.punctuation])\n",
    "        \n",
    "        # Remove stopwords\n",
    "        spanish_stopwords = set(stopwords.words('spanish'))\n",
    "        spanish_stopwords.remove(\"no\")  # Retain \"no\" as it provides negative context\n",
    "        words = word_tokenize(text)\n",
    "        filtered_words = [word.lower() for word in words if word.lower() not in spanish_stopwords]\n",
    "        \n",
    "        # Apply stemming (MIRAR)\n",
    "        ##stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "        \n",
    "        ##return ' '.join(stemmed_words)  \n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    df['full_text_processed'] = df['full_text'].apply(remove_spanish_stopwords)\n",
    "    # Eliminar filas donde 'full_text_processed' es una cadena vacía\n",
    "    df = df[df['full_text_processed'] != \"\"]\n",
    "\n",
    "\n",
    "    return df, labels_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_data(df, embedding_name, embedding_size):\n",
    "\n",
    "    if embedding_name == \"fasttext\":\n",
    "        # Create a FastText model\n",
    "        sentences = df['full_text_processed'].str.split().tolist()\n",
    "        #print(f\"{sentences=}\")\n",
    "        embedding_size = embedding_size  # you can adjust this value as needed\n",
    "        model = FastText(sentences, vector_size=embedding_size, window=15, min_count=1, workers=8, sg=1, epochs=10)\n",
    "\n",
    "        # Convert text data into FastText embeddings\n",
    "        def text_to_vector(text):\n",
    "            #print(f\"{text=}\")\n",
    "            words = text.split()\n",
    "            #print(f\"{words=}\")\n",
    "            vector = np.mean([model.wv[word] for word in words if word in model.wv.index_to_key], axis=0)\n",
    "            if len(vector) == 0:\n",
    "                return np.zeros(embedding_size)\n",
    "            else:\n",
    "                return vector\n",
    "\n",
    "        df['full_text_filtered'] = df['full_text_processed'].apply(text_to_vector)\n",
    "\n",
    "        X = np.stack(df['full_text_filtered'].values)\n",
    "        y = df['label'].values\n",
    "\n",
    "    elif embedding_name == \"word2vec\":\n",
    "        # Create a FastText model\n",
    "        sentences = df['full_text_processed'].str.split().tolist()\n",
    "        embedding_size = embedding_size  # you can adjust this value as needed\n",
    "        model = Word2Vec(sentences, vector_size=embedding_size, window=15, min_count=1, workers=8, sg=1, epochs=10)\n",
    "\n",
    "        # Convert text data into FastText embeddings\n",
    "        def text_to_vector(text):\n",
    "            words = text.split()\n",
    "            vector = np.mean([model.wv[word] for word in words if word in model.wv.index_to_key], axis=0)\n",
    "            if len(vector) == 0:\n",
    "                return np.zeros(embedding_size)\n",
    "            else:\n",
    "                return vector\n",
    "\n",
    "\n",
    "        df['full_text_filtered'] = df['full_text_processed'].apply(text_to_vector)\n",
    "\n",
    "        X = np.stack(df['full_text_filtered'].values)\n",
    "        y = df['label'].values\n",
    "\n",
    "\n",
    "    elif embedding_name == \"bow\":\n",
    "        # Creating the bag of Word Model\n",
    "        model = CountVectorizer(max_features = 5000, ngram_range=(1, 5))\n",
    "        X = model.fit_transform(df['full_text_processed']).toarray()\n",
    "        y = df['label'].values\n",
    "\n",
    "    elif embedding_name == \"tfidf\":\n",
    "        # Creating the TF-IDF model\n",
    "        model = TfidfVectorizer(max_features=5000, ngram_range=(1, 5))\n",
    "        X = model.fit_transform(df['full_text_processed']).toarray()\n",
    "        y = df['label'].values\n",
    "        \n",
    "    elif embedding_name == \"custom\":\n",
    "        wordvectors_file = 'embeddings-l-model.bin'\n",
    "        model = FastText.load_fasttext_format(wordvectors_file)\n",
    "        # Function to convert text to vector using the pre-trained model\n",
    "        def text_to_vector(text):\n",
    "            words = text.split()\n",
    "            vector = np.mean([model.wv[word] for word in words if word in model.wv], axis=0)\n",
    "            if len(vector) == 0:\n",
    "                return np.zeros(model.vector_size)\n",
    "            else:\n",
    "                return vector\n",
    "            \n",
    "        df['full_text_filtered'] = df['full_text_processed'].apply(text_to_vector)\n",
    "\n",
    "        X = np.stack(df['full_text_filtered'].values)\n",
    "        y = df['label'].values\n",
    "\n",
    "    elif embedding_name == \"roberta\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('PlanTL-GOB-ES/roberta-large-bne') \n",
    "        model = RobertaModel.from_pretrained(\"PlanTL-GOB-ES/roberta-large-bne\") \n",
    "\n",
    "        embeddings = get_roberta_embeddings(df['full_text_processed'].tolist(), tokenizer, model)\n",
    "        \n",
    "        X = embeddings\n",
    "        y = df['label'].values\n",
    "\n",
    "    elif embedding_name == \"beto\":\n",
    "        tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased') #beto\n",
    "        model = BertModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\") #beto\n",
    "\n",
    "        embeddings = get_roberta_embeddings(df['full_text_processed'].tolist(), tokenizer, model)\n",
    "        \n",
    "        X = embeddings\n",
    "        y = df['label'].values\n",
    "\n",
    "    elif embedding_name == \"bert-multi\":\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "        model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "        embeddings = get_roberta_embeddings(df['full_text_processed'].tolist(), tokenizer, model)\n",
    "        \n",
    "        X = embeddings\n",
    "        y = df['label'].values\n",
    "\n",
    "    elif embedding_name == \"xlm-roberta-base\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "        model = XLMRobertaModel.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "\n",
    "        embeddings = get_roberta_embeddings(df['full_text_processed'].tolist(), tokenizer, model)\n",
    "        \n",
    "        X = embeddings\n",
    "        y = df['label'].values\n",
    "\n",
    "\n",
    "    # Add the additional features to your embeddings\n",
    "    additional_features = df[['mention_count', 'view_count_scaled', 'tweet_length', 'num_adjectives', \"full_text_processed\"]].values\n",
    "\n",
    "    # Assuming X is your text embeddings\n",
    "    X = np.hstack((X, additional_features))\n",
    "\n",
    "    ## SAVE MODEL when running best model -> future (https://radimrehurek.com/gensim/models/fasttext.html)\n",
    "    #dump(model, f\"embeddings/embedding_{run_id}.pkl\")\n",
    "\n",
    "\n",
    "    return df, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split_data(X, y, balance):\n",
    "\n",
    "    # Splitting the dataset into the Training set and Test set with stratify=y so training and test sets have a similar distribution of classes as original dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, stratify=y)\n",
    "\n",
    "    # Combinar X_train y y_train para upsampling\n",
    "    # upsample after split so train examples are not repeated in test\n",
    "\n",
    "    if balance == \"upsampling\":\n",
    "        print(f\"{balance=}\")\n",
    "        # Convertir X_train y y_train a DataFrame para facilitar el manejo\n",
    "        df_train = pd.DataFrame(X_train)\n",
    "        df_train['label'] = y_train\n",
    "\n",
    "        # Aplicar upsampling\n",
    "        df_train_upsampled = upsample(df_train)\n",
    "\n",
    "        # Separar las características y las etiquetas después del upsampling\n",
    "        y_train_upsampled = df_train_upsampled['label'].values\n",
    "        X_train_upsampled = df_train_upsampled.drop('label', axis=1).values\n",
    "\n",
    "        # Imprimir el soporte de las clases después del upsampling\n",
    "        print(df_train_upsampled['label'].value_counts())\n",
    "\n",
    "        # Asegurar que X_train y y_train estén actualizados\n",
    "        X_train, y_train = X_train_upsampled, y_train_upsampled\n",
    "\n",
    "    # Aplicar SMOTE solo si se especifica\n",
    "    if balance == \"smote\":\n",
    "        sm = SMOTE(random_state=42)\n",
    "        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "        print(\"Después de aplicar SMOTE:\")\n",
    "        print(pd.Series(y_train).value_counts())\n",
    "\n",
    "    # Aplicar ADASYN solo si se especifica\n",
    "    if balance == \"adasyn\":\n",
    "        ada = ADASYN(random_state=42)\n",
    "        X_train, y_train = ada.fit_resample(X_train, y_train)\n",
    "        print(\"Después de aplicar ADASYN:\")\n",
    "        print(pd.Series(y_train).value_counts())\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorge\\AppData\\Local\\Temp\\ipykernel_9468\\911966701.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['label'] = labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Soporte de etiquetas con nombres originales:\n",
      "Comentario Positivo: 2117\n",
      "Comentario Negativo: 435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorge\\AppData\\Local\\Temp\\ipykernel_9468\\911966701.py:146: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['full_text_processed'] = df['full_text'].apply(remove_spanish_stopwords)\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '👏🏻👏🏻'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Entrenar un clasificador\u001b[39;00m\n\u001b[0;32m     25\u001b[0m clf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:348\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 348\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32mc:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1144\u001b[0m     )\n\u001b[1;32m-> 1146\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1162\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    913\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 915\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    918\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    919\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '👏🏻👏🏻'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = \"../data/BBDD_SeAcabo.csv\" # \"data/BBDD_SeAcabo.csv\" \"AMI_IBEREVAL2018/es_AMI_TrainingSet_NEW.csv\"\n",
    "type_id = \"analisis_general\" # [\"analisis_general\", \"contenido_negativo\", \"insultos\"]\n",
    "balance = \"None\" # [\"downsampling\", \"upsampling\", \"smote\", \"adasyn\", \"None\"] # falta \"smote\", \"adasyn\" para analisis_general y contenido_negativo\n",
    "embedding_name = \"beto\" #[\"fasttext\", \"word2vec\", \"bow\", \"tfidf\", \"custom\", \"roberta\", \"beto\", \"bert-multi\", \"xlm-roberta-base\"] // falta custom para [\"analisis_general\", \"contenido_negativo\", \"insultos\"]\n",
    "embedding_size = 500 #[100, 500] \n",
    "\n",
    "# Cargamos datos\n",
    "df = load_data(data)\n",
    "\n",
    "# Preprocesado de datos\n",
    "df, labels_names = process_data(df, type_id, balance)\n",
    "\n",
    "# Embeddings\n",
    "df, X, y = embedding_data(df, embedding_name, embedding_size)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = split_data(X, y, balance)\n",
    "X_train = np.delete(X_train, 5, axis=1)\n",
    "\n",
    "\n",
    "# Entrenar un clasificador\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomForestClassifier' object has no attribute 'feature_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lime_tabular\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Crear un explainer para datos tabulares\u001b[39;00m\n\u001b[0;32m      5\u001b[0m explainer \u001b[38;5;241m=\u001b[39m lime_tabular\u001b[38;5;241m.\u001b[39mLimeTabularExplainer(\n\u001b[0;32m      6\u001b[0m     training_data\u001b[38;5;241m=\u001b[39mX_train,\n\u001b[0;32m      7\u001b[0m     class_names\u001b[38;5;241m=\u001b[39mlabels_names,\n\u001b[1;32m----> 8\u001b[0m     feature_names\u001b[38;5;241m=\u001b[39m\u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_name\u001b[49m(),\n\u001b[0;32m      9\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Elegir una instancia del conjunto de prueba para explicar\u001b[39;00m\n\u001b[0;32m     13\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Cambia este índice para explorar otras instancias\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomForestClassifier' object has no attribute 'feature_name'"
     ]
    }
   ],
   "source": [
    "#%pip install lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Suponiendo que X_test_text contiene el texto original correspondiente a cada instancia en X_test\n",
    "# Deberías preparar X_test_text de manera que coincida con los índices de X_test\n",
    "\n",
    "# Crear el explainer de texto\n",
    "explainer = LimeTextExplainer(class_names=labels_names)\n",
    "\n",
    "# Crear un pipeline con tu clasificador si es necesario\n",
    "# Esto es útil si necesitas preprocesar el texto antes de pasarlo al clasificador\n",
    "# En tu caso, si el clasificador espera embeddings directamente, necesitarías un paso de preprocesamiento que convierta texto a embeddings\n",
    "# Si eso no es práctico, considera cómo estás utilizando el clasificador y ajusta según sea necesario\n",
    "\n",
    "# Elegir una instancia de texto del conjunto de prueba para explicar\n",
    "idx = 1  # Asegúrate de que este índice sea válido en X_test_text\n",
    "text_instance = X_test.iloc[idx][\"full_text_processed\"]\n",
    "\n",
    "# Generar explicación para una instancia específica\n",
    "exp = explainer.explain_instance(text_instance, clf.predict_proba, num_features=4)\n",
    "\n",
    "# Mostrar la explicación\n",
    "exp.show_in_notebook(text=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
